{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MIT Office of Research Computing and Data Hands On Help Pages","text":"<p>The MIT Office or Research Computing and Data (ORCD) provides access and support for the compute and data needs of a wide range of research activities. These pages provide  help material for hands-on working with ORCD supported services. </p> <p>Help working with ORCD services is also available through email to orcd-help@mit.edu, please  feel free to contact us with questions and suggestions.</p>"},{"location":"data-security/","title":"Data Security and Privacy","text":""},{"location":"data-security/#what-data-can-be-stored","title":"What data can be stored?","text":"<p>All ORCD current systems are only suitable for storing data with low-level security requirements. This means that they are not to be used to store sensitive data, such as personal information, financial information, or intellectual property. Additionally, they are not to be used to store data that is subject to use agreements that require security controls or audit tracking.</p> <p>The following data categories are appropriate for storing and analyzing on current ORCD systems:</p> <p>Open public data Low-sensitivity private research data Low-sensitivity research data are items such as drafts of papers and analyses derived from public data.</p> <p>The following data types are suitable for ORCD systems:</p> <ul> <li>Anything in the low-risk category described at the MIT IS&amp;T information security pages</li> <li>Drafts of unpublished research papers and results that are based on low-risk data</li> </ul> <p>Anything else in the medium-risk or higher risk levels of the MIT IS&amp;T information security pages should not be stored or analyzed on current ORCD systems.</p> <p>If you have any questions about whether your data is appropriate for storing on ORCD systems please feel free to reach out to us at orcd-help@mit.edu. </p>"},{"location":"data-security/#where-can-more-sensitive-data-be-processed-and-stored","title":"Where can more sensitive data be processed and stored?","text":"<p>The ORCD team is currently developing a system for more sensitive data. If you have sensitive data and would like to learn about our plans please feel free to get in touch at orcd-help@mit.edu.</p>"},{"location":"orcd-systems/","title":"ORCD Systems","text":"<p>ORCD operates and provides support and training for a number of cluster computer systems available to all researchers. These systems all run with a Slurm scheduler and have a web portal for interactive computing. These are Engaging, SuperCloud, and Satori.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-cluster","title":"Engaging Cluster","text":"<p>The engaging cluster is a mixed CPU and GPU computng cluster that is openly available to all  research projects at MIT. It has around 80,000 x86 CPU cores and 300  GPU cards ranging from K80 generation to recent Voltas. Hardware access is through the Slurm  resource scheduler that supports batch and interactive workloads and allows dedicated reservations. The cluster has a large shared file system for working datasets. Additional compute and storage  resources can be purchased by PIs. A wide range of standard software is available and the Docker  compatible Singularity container tool is supported. User-level tools like Anaconda for Python,  R libraries and Julia packages are all supported. A range of PI group maintained custom software  stacks are also available through the widely adopted environment modules toolkit. A standard,  open-source, web-based portal supporting Jupyter notebooks, R studio, Mathematica and X graphics  is available at https://engaging-ood.mit.edu. Further information and support is available from orcd-help-engaging@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-engaging","title":"How to Get an Account on Engaging","text":"<p>Accounts on the engaging cluster are connected to your main MIT institutional kerberos id.  Connecting to the cluster for the first time through its web portal automatically activates an account with basic access to resources. See this page for instructions on how to log in.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-quick-links","title":"Engaging Quick Links","text":"<ul> <li>Documentation: https://engaging-web.mit.edu/eofe-wiki/</li> <li>OnDemand web portal: https://engaging-ood.mit.edu</li> <li>Help: Send email to orcd-help-engaging@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori","title":"Satori","text":"<p>Satori is an IBM Power 9 large memory node system. It is open to everyone on campus and has  optimized software stacks for machine learning and for image stack post-processing for  MIT.nano Cryo-EM facilities. The system has 256 NVidia Volta GPU cards attached in groups of  four to 1TB memory nodes and a total of 2560 Power 9 CPU cores. Hardware access is through the  Slurm resource scheduler that supports batch and interactive workload and allows dedicated  reservations. A wide range of standard software is available and the Docker compatible  Singularity container tool is supported. A standard web based portal  https://satori-portal.mit.edu with Jupyter notebook support is available. Additional compute  and storage resources can be purchased by PIs and integrated into the system. Further  information and support is available at orcd-help-satori@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-satori","title":"How to Get an Account on Satori","text":"<p>You can get an account by logging into https://satori-portal.mit.edu with your MIT credentials.  See this page for instructions: https://mit-satori.github.io/satori-basics.html#how-can-i-get-an-account.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori-quick-links","title":"Satori Quick Links","text":"<ul> <li>Documentation: (https://mit-satori.github.io/</li> <li>OnDemand web portal: https://satori-portal.mit.edu</li> <li>Help: Send email to orcd-help-satori@mit.edu</li> <li>Slack: https://mit-satori.slack.com/</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud","title":"SuperCloud","text":"<p>The SuperCloud system is a collaboration with MIT Lincoln Laboratory on a shared facility that  is optimized for streamlining open research collaborations with Lincoln Laboratory. The facility  is open to everyone on campus. The latest SuperCloud system has more than 16,000 x86 CPU cores  and more than 850 NVidia Volta GPUs in total. Hardware access is through the Slurm resource  scheduler that supports batch and interactive workload and allows dedicated reservations. A wide  range of standard software is available and the Docker compatible Singularity container tool is  supported. A custom, web-based portal supporting Jupyter notebooks is available at https://txe1-portal.mit.edu/. Further information and support is available at  supercloud@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-supercloud","title":"How to Get an Account on SuperCloud","text":"<p>To request a SuperCloud account follow the instructions on this page:  https://supercloud.mit.edu/requesting-account.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud-quick-links","title":"SuperCloud Quick Links","text":"<ul> <li>Documentation: https://supercloud.mit.edu/</li> <li>Online Course: https://learn.llx.edly.io/course/practical-hpc/</li> <li>Web portal: https://txe1-portal.mit.edu/</li> <li>Help: Send email to supercloud@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#openmind","title":"OpenMind","text":"<p>The OpenMind system is a collaboration with Department of Brain and Cognitive Sciences (BCS) and McGovern Institute. OpenMind is mainly a GPU computing cluster optimized for artificial intelligence (AI) research and data science. Totally there are around 70 compute nodes, 3500 CPU cores, 48 TB of RAM, and 340 GPUs, including 142 A100-80GB GPUs. It also provides around 2 PB of flash storage supporting fast read/write data speed. Hardware access is through the Slurm resource scheduler that supports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and Docker compatible Apptainer/Singularity container tool is supported. User-level tools like Anaconda for Python, R libraries and Julia packages are all supported. Further information and support is available at orcd-help-openmind@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-openmind","title":"How to Get an Account on OpenMind","text":"<p>Accounts will be available for MIT users in 2024.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#openmind-quick-links","title":"OpenMind Quick Links","text":"<ul> <li>Documentation: https://github.mit.edu/MGHPCC/OpenMind/wiki</li> <li>Home Page and Online Course: https://openmind.mit.edu/</li> <li>Help: Send email to orcd-help-openmind@mit.edu</li> <li>Slack: https://openmind-46.slack.com</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#engaging","title":"Engaging","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>Example of building custom LAMMPS configuration using spack</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gpu","title":"GPU","text":"<ul> <li>GROMACS</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gromacs","title":"GROMACS","text":"<ul> <li>GROMACS</li> </ul>"},{"location":"tags/#h100","title":"H100","text":"<ul> <li>Getting started on 8-way H100 nodes on Satori</li> </ul>"},{"location":"tags/#howto-recipes","title":"Howto Recipes","text":"<ul> <li>GROMACS</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>Example of building custom LAMMPS configuration using spack</li> <li>MPI for Python</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#lammps","title":"LAMMPS","text":"<ul> <li>Example of building custom LAMMPS configuration using spack</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>GROMACS</li> <li>MPI for Python</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#mujoco","title":"MuJoCo","text":"<ul> <li>MuJoCo</li> </ul>"},{"location":"tags/#openmind","title":"OpenMind","text":"<ul> <li>MPI for Python</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>MPI for Python</li> </ul>"},{"location":"tags/#rocky-linux","title":"Rocky Linux","text":"<ul> <li>Example of building custom LAMMPS configuration using spack</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#satori","title":"Satori","text":"<ul> <li>ORCD Systems</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#supercloud","title":"SuperCloud","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MuJoCo</li> </ul>"},{"location":"tags/#cuda","title":"cuda","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#cuda-aware-mpi","title":"cuda aware mpi","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#nvhpc","title":"nvhpc","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#spack","title":"spack","text":"<ul> <li>Example of building custom LAMMPS configuration using spack</li> </ul>"},{"location":"filesystems-file-transfer/filesystems/","title":"General Use Filesystems","text":"<p>Large HPC systems often have different filesystems for different purposes. ORCD systems are no different, and each have their own approach. This page documents these.</p>"},{"location":"filesystems-file-transfer/filesystems/#supercloud","title":"SuperCloud","text":"<p>SuperCloud uses Lustre for all central/shared storage (accessible to all nodes in the system). This storage is not backed up. See the SuperCloud Best Practices and Performance Tips page for best practices using the Lustre filesystem. Quotas or limits are set on the storage as guardrails. Additional storage may be granted on a case by case basis. Local disk spaces will be faster than the Lustre shared filesystem, but all are temporary and can only be accessed on the node where they are created.</p> Storage Type Path Access Backed up Limits Home Directory  Lustre <code>/home/gridsan/&lt;username&gt;</code> User only Not backed up See User Profile Page Group Directories  Lustre <code>/home/gridsan/groups/&lt;groupname&gt;</code> Files shared within a group Not backed up See User Profile Page Job-specific Temporary Storage  Local Disk Access using the <code>$TMPDIR</code> environment variable User or Group Not backed up   Temporary directory created at the start of a job and cleaned up at the end of the job NA Local Disk Space Create the directory <code>/state/partition1/user/$USER</code> as needed User or Group Not backed up  Cleaned up monthly during downtimes NA"},{"location":"filesystems-file-transfer/filesystems/#engaging","title":"Engaging","text":"<p>Users each get a small home directory that is backed up and meant for important files. Larger scratch space is not backed up. Additional storage can be purchased. The Lustre scratch space will be faster than NFS for the majority of workloads, however having large numbers of small files will make it slower than NFS and can slow down the filesystem overall, so it is important to follow the Lustre Best Practices. See the Engaging Documentation Page on Storage for more information.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory  NFS <code>/home/&lt;username&gt;</code> 100 GB Backed up Use for important files Lustre <code>/nobackup1/&lt;username&gt;</code> 1 TB Not backed up Scratch space  Faster than NFS NFS <code>/pool001/&lt;username&gt;</code> 1 TB Not backed up Scratch space"},{"location":"filesystems-file-transfer/filesystems/#satori","title":"Satori","text":"Storage Type Path Quota Backed up Purpose/Notes Home Directory <code>/home/&lt;username&gt;</code> 25GB Backed up Use for important files. Quota increase request to 100GB. GPFS <code>/nobackup/users/&lt;username&gt;</code> 500GB Not backed up Scratch space. Quota increase request to 2TB."},{"location":"filesystems-file-transfer/filesystems/#openmind","title":"OpenMind","text":"<p>OpenMind provides a number of different storage options. See the OpenMind Documentation page on Storage for more information, best practices, and recommendations.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory <code>/home/&lt;username&gt;</code> 20 GB Backed up Use for very important files. Physically located on Vast. Weka <code>/om/user/&lt;username&gt;</code> (individual users) and <code>/om/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Weka Scratch <code>/om/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 3 weeks after creation Scratch space Vast <code>/om2/user/&lt;username&gt;</code> (individual users) and <code>/om2/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Vast Scratch <code>/om2/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 2 weeks after creation Scratch space Lustre Scratch <code>/nobackup/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 3 weeks after creation Scratch space   See Lustre Best Practices page NFS <code>/om3</code>, <code>/om4</code>, <code>/om5</code> Per group Backed up Slow internal long-term storage NESE <code>/nese</code> Per group Backed up Slow internal long-term storage"},{"location":"filesystems-file-transfer/project-filesystems/","title":"Project Specific Filesystems","text":""},{"location":"filesystems-file-transfer/project-filesystems/#purchasing-storage","title":"Purchasing Storage","text":"<p>Additional project and lab storage can be purchased on ORCD shared clusters by individual PI groups. This storage is mounted on the cluster and access to the storage is managed  by the group through MIT Web Moira, https://groups.mit.edu/webmoira/ (see below for details).</p> <p>Current pricing for storage is</p> Storage Type Pricing Duration Backup NESE encrypted at rest   disk. $2.50/month  50TB minimum,    10TB increments. 12 month  minimum. No automated backup. <p>The NESE encrypted at rest disk uses a large centrally managed storage cloud at the MGHPCC facility. Any shared ORCD cluster at the MGHPCC can access this storage. Data on NESE disk is transparently encrypted at rest.</p> <p>To purchase storage please send an email to orcd-help@mit.edu.</p>"},{"location":"filesystems-file-transfer/project-filesystems/#managing-access-using-mit-web-moira","title":"Managing access using MIT Web Moira","text":"<p>Individual group storage is configured so that access is limited to a set of accounts belonging to a web moira list that is defined for the group store. The owner and administrators of group storage can manage access themselves, by modifying the membership of an associated moira list under https://groups.mit.edu/webmoira/list/. The name of the list corresponds to the UNIX group name associated with the ORCD shared  cluster storage.</p>"},{"location":"filesystems-file-transfer/project-filesystems/#moira-web-interface-example","title":"Moira Web Interface Example","text":"<p>The figure below shows a screenshot of the web moira management page at https://groups.mit.edu/webmoira/list/cnh_research_computing for a hypothetical storage group named <code>cnh_research_computing</code>. The interface provides a  self-service mechanism for controlling access to any storage belonging to this group. MIT account ids can be added and  removed as needed from this list by the storage access administrators.</p> <p></p>"},{"location":"images/","title":"Index","text":""},{"location":"images/#directory-of-static-images","title":"Directory of static images","text":""},{"location":"recipes/gromacs/","title":"Installing and Using GROMACS","text":"<p>GROMACS is a free and open-source software suite for high-performance molecular dynamics and output analysis.</p> <p>You can learn about GROMACS here: https://www.gromacs.org/.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#gromacs-on-engaging","title":"GROMACS on Engaging","text":"","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#install-gromacs-with-mpi","title":"Install GROMACS with MPI","text":"<p>Select a version on the GROMACS website, then dowload and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget --no-check-certificate http://ftp.gromacs.org/pub/gromacs/gromacs-2019.6.tar.gz\ntar xvfz gromacs-2019.6.tar.gz\n</code></pre></p> <p>Load MPI and Cmake modules, <pre><code>module load engaging/openmpi/2.0.3 cmake/3.17.3\n</code></pre></p> <p>Create build and isntall directories, <pre><code>mkdir -p 2019.6/build\nmkdir 2019.6/install\ncd 2019.6/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2019.6 -DGMX_MPI=ON -DCMAKE_INSTALL_PREFIX=~/gromacs/2019.6/install\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage enviroenment, <pre><code>source ~/gromacs/2019.6/install/bin/GMXRC\n</code></pre></p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#gromacs-on-supercloud","title":"GROMACS on SuperCloud","text":"","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#install-gromacs-with-mpi-and-cuda","title":"Install GROMACS with MPI and CUDA","text":"<p>Select a version on the GROMACS website, then dowload and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget http://ftp.gromacs.org/pub/gromacs/gromacs-2023.2.tar.gz\ntar xvfz gromacs-2023.2.tar.gz\n</code></pre></p> <p>Load CUDA, Anaconda and MPI modules, <pre><code>module load cuda/11.8 anaconda/2023a\nmodule load mpi/openmpi-4.1.5\n</code></pre></p> <p>Create build and isntall directories, <pre><code>mkdir -p 2023.2/build\nmkdir 2023.2/install\ncd 2023.2/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2023.2 -DGMX_MPI=ON -DGMX_GPU=CUDA -DCMAKE_INSTALL_PREFIX=~/gromacs/2023.2-gpu\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage enviroenment before running GROMACS programs, <pre><code>source ~/gromacs/2023.2/install/bin/GMXRC\n</code></pre></p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#run-gromacs","title":"Run GROMACS","text":"<p>Firstly, prepare for an input file. Refer to file formats. Here shows an example with an input file named <code>benchPEP-h.tpr</code> dowloaded from this page.</p> <p>Secondly, edit a batch job script, for example, named <code>job.sh</code>, requesting 2 node with 4 CPU cores and 2 GPUs per node. <pre><code>#!/bin/bash\n#SBATCH --nodes=2              # 2 nodes\n#SBATCH --ntasks-per-node=2    # 2 MPI tasks per node\n#SBATCH --cpus-per-task=2      # 2 CPU cores per task\n#SBATCH --gres=gpu:volta:2     # 2 GPUs per node\n#SBATCH --time=01:00:00        # 1 hour\n\n\n# Load required modules\nmodule load cuda/11.8 mpi/openmpi-4.1.5\n\n# Allow GROMACS to see GPUs\nexport CUDA_VISIBLE_DEVICES=0,1\n\n# Enable direct GPU to GPU communications\nexport GMX_ENABLE_DIRECT_GPU_COMM=true\n\n# Activate user install of GROMACS\nsource ~/gromacs/2023.2-gpu/bin/GMXRC\n\n# Check MPI, GPU and GROMACS\nmpirun hostname\nnvidia-smi\nwhich gmx_mpi\n\n# Run GROMACS\nmpirun gmx_mpi mdrun -s ~/gromacs/bench/benchPEP-h.tpr -ntomp ${SLURM_CPUS_PER_TASK} -pme gpu -update gpu -bonded gpu -npme 1\n</code></pre></p> <p>Finally, submit the job, <pre><code>sbatch job.sh\n</code></pre></p> <p>Refer to GROMACS user guide for more info.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/h100_getting_started/","title":"Getting started on 8-way H100 nodes on Satori","text":"<p>This page provides information on how to run a first job on the IBM Watson AI Lab H100 GPU nodes on Satori. The page describes how to request an access to the Slurm partition associated  with the H100 nodes and how to run a first example pytorch script on the systems. </p> <p>A first set of H100 GPU systems has been added to Satori. These are for priority use by IBM Watson AI Lab research collaborators. They are also available for general opportunistic use when they are idle.</p> <p>Currently ( 2023-06-19 ) there are 4 H100 systems installed.  Each system has 8 H100 GPU cards, two Intel 8468 CPUs each with 48 physical cores and 1TiB of main memory.</p> <p>Below are some instructions for getting started with these systems. </p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#access-to-the-nodes","title":"Access to the nodes","text":"<p>To access the nodes in the priority group you need your satori login id to be listed in the Webmoira  group https://groups.mit.edu/webmoira/list/sched_oliva.  Either Alex Andonian and Vincent Sitzmann are able to add accounts to the <code>sched_oliva</code> moira list.</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#interactive-access-through-slurm","title":"Interactive access through Slurm","text":"<p>To access an entire node through Slurm, the command below can be used from the satori login node</p> <pre><code>srun -p sched_oliva --gres=gpu:8 -N 1 --mem=0 -c 192 --time 1:00:00 --pty /bin/bash\n</code></pre> <p>this command will launch an interactive shell on one of the nodes (when a full node becomes available).  From this shell the NVidia status command  <pre><code>nvidia-smi\n</code></pre> should list 8 H100 GPUs as available. </p> <p>Single node, multi-gpu training examples (for example https://github.com/artidoro/qlora ) should run  on all 8 GPUs. </p> <p>To use a single GPU interactively the following command can be used <pre><code>srun -p sched_oliva --gres=gpu:1 --mem=128 -c 24 --time 1:00:00 --pty /bin/bash\n</code></pre></p> <p>this will request a single GPU. This request will allow other Slurm sessions to run on other GPUs  simultaneously with this session.</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-nightly-build-pytorch-example-with-a-freash-miniconda-and-pytorch","title":"Running a nightly build pytorch example with a freash miniconda and pytorch","text":"<p>A miniconda environment can be used to run the latest nightly build pytorch code on these  systems. To do this, first create a software install directory and install the needed pytorch software</p> <pre><code>mkdir -p /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre> <p>and then switch your shell to that directory. <pre><code>cd /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre></p> <p>now install miniconda and create an environment with the needed software <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \nchmod +x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh -b -p minic\n. ./minic/bin/activate \nconda create -y -n pytorch_test python=3.10\nconda activate pytorch_test                          \nconda install -y -c conda-forge cupy\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n</code></pre></p> <p>Once the software is installed, the following script can be used to test the installation. <pre><code>cat &gt; test.py &lt;&lt;'EOF'\nimport torch\ndevice_id = torch.cuda.current_device()\ngpu_properties = torch.cuda.get_device_properties(device_id)\nprint(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n          \"%.1fGb total memory.\\n\" % \n          (torch.cuda.device_count(),\n          device_id,\n          gpu_properties.name,\n          gpu_properties.major,\n          gpu_properties.minor,\n          gpu_properties.total_memory / 1e9))\nEOF\n\npython test.py\n</code></pre></p> <p>To exit the Slurm srun session enter the command <pre><code>exit\n</code></pre></p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-simple-batch-script-using-an-installed-miniconda-environment","title":"Running a simple batch script using an installed miniconda environment","text":"<p>To run a batch script on one of the H100 nodes in partition sched_oliva first paste the content in the  box below into a slurm script file called, for example, <code>test_script.slurm</code> ( change the RUNDIR setting to assign the  path to a directory where you have already installed a conda environment in a sub-directory called <code>minic</code> ).</p> <pre><code>#!/bin/bash\n#\n#SBATCH --gres=gpu:8\n#SBATCH --partition=sched_oliva\n#SBATCH --time=1:00:00\n#SBATCH --mem=0\n#\n\nnvidia-smi\n\nRUNDIR=/nobackup/users/${USER}/h100-testing/minic\ncd ${RUNDIR}\n\n. ./minic/bin/activate\n\nconda activate pytorch_test\n\ncat &gt; mytest.py &lt;&lt;'EOF'\nimport torch\ndevice_id = torch.cuda.current_device()\ngpu_properties = torch.cuda.get_device_properties(device_id)\nprint(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n          \"%.1fGb total memory.\\n\" %\n          (torch.cuda.device_count(),\n          device_id,\n          gpu_properties.name,\n          gpu_properties.major,\n          gpu_properties.minor,\n          gpu_properties.total_memory / 1e9))\nEOF\n\npython mytest.py\n</code></pre> <p>This script can then be submitted to Slurm to run in a background batch node using the command.</p> <pre><code>sbatch &lt; test_script.slurm\n</code></pre>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#getting-help","title":"Getting help","text":"<p>As always, please feel welcome to email orcd-help@mit.edu with questions, comments or suggestions. We would be happy to hear from you!</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/lammps-with-spack/","title":"Example of building custom LAMMPS configuration using spack","text":"","tags":["Engaging","Howto Recipes","spack","LAMMPS","Rocky Linux"]},{"location":"recipes/lammps-with-spack/#about-lammps","title":"About LAMMPS","text":"<p>LAMMPS is a fully open-source molecular dynamics simulator.  Its name is an acronym from Large-scale Atomic/Molecular Massively Parallel Simulator. It is widely used in materials research. LAMMPS is actively developed here by a team of collaborators led by researchers from Sandia National Laboratory and  Temple University. LAMMPS has many different compilation options that can be used to activate different technical and scientific features.</p>","tags":["Engaging","Howto Recipes","spack","LAMMPS","Rocky Linux"]},{"location":"recipes/lammps-with-spack/#compiling-and-running-a-custom-lammps-using-spack","title":"Compiling and running a custom LAMMPS using Spack","text":"<p>In this recipe we look at setting up a custom configuration of LAMMPS using the Spack system. The recipe shows how to compile LAMMPS in a way that uses an existing Spack install of core tools like MPI and GPU CUDA libraries and tools. The recipe The detailed steps, that can be executed in an interactive Slurm session, are explained  below. </p> <p>Prerequisites</p> <p>This example assumes you have access to a Slurm partition and are working with a Rocky Linux environment.</p>","tags":["Engaging","Howto Recipes","spack","LAMMPS","Rocky Linux"]},{"location":"recipes/lammps-with-spack/#1-configure-an-instance-of-spack-in-a-directory-under-your-account","title":"1. Configure an instance of Spack in a directory under your account","text":"<p>Spack is a tool for compiling programs in a uniform away. It is designed for use by regular accounts on a computer. It does not need any administrative privileges. Here it will be used to compile the LAMMPS software. </p> <p>A first step to using Spack is to download the software from its Github repository using the following command.</p> <pre><code>git clone https://github.com/spack/spack.git\n</code></pre> <p>Next we configure Spack, setting the it ot use standard tools that have already been built from a pre-exisiting location.</p> Note <p>The compilation example shows using Spack in a way that uses pre-existing upstream Spack built software  from another location. This can be useful on a cluster computer where a central team may have already installed and configured some standard foundation software tools,  such as a compiler and high-performance tools for using GPUs and/or for parallel communication. Configuring these foundation software tools can involve seaprate testing and performance settings, so using a pre-installed foundation is generally useful. To make a Spack upstream work reliably we need to use the same tag of  Spack as used in the upstream and provide a path name to the upstream isntallation.</p> <p>To configure Spack use the following sequence of commands.</p> <pre><code># Switch to build location\nmkdir -p /nobackup1/users/${USER}/lammps-testing\ncd /nobackup1/users/${USER}/lammps-testing\n\n# Set any .spack files to be local to this test\nexport SPACK_USER_CONFIG_PATH=`pwd`/user_config\n\n# Download spack and set version to match upstream Spack\ngit clone https://github.com/spack/spack.git\n(\n cd spack\n git checkout -b v0.19.1 v0.19.1\n)\n\n# Set upstream and cp reference config files for upstream\nmkdir -p `pwd`/user_config\ncp /software/spack/etc/spack/*yaml user_config\ncat  &gt; user_config/upstreams.yaml &lt;&lt; EOF\nupstreams:\n  orcd-rcf-2023:\n   install_tree: /software/spack-20230328/opt/spack\nEOF\n\nsource spack/share/spack/setup-env.sh\n</code></pre>","tags":["Engaging","Howto Recipes","spack","LAMMPS","Rocky Linux"]},{"location":"recipes/lammps-with-spack/#2-check-what-extra-software-spack-will-build-for-our-lammps-install","title":"2. Check what extra software Spack will build for our LAMMPS install","text":"<p>Spack will download and compile extra software that it needs to compile LAMMPS.  It is good to check what software Spack selects to build to make sure that  the upstream libraries are being used as epxected.</p> <p>To check the software Spack will build use the commad.</p> <pre><code>spack spec -I -L lammps%gcc@12.2.0 fftw_precision=single +intel ~kim +asphere +class2 +kspace +manybody +molecule +opt +replica +rigid +granular +openmp-package +openmp ^openmpi\n</code></pre> <p>this command produces a large amount of output that is described below</p> Output from spack spec -I -L query <p>The<code>spack spec -I -L</code> command produces a series of lines that show the versions of all the software packages that are needed for some software. The first column in these lines show symbols that denote whether an existing  version will be used ( [^], or [e] ) or a new version needs to be downloaded and installed ( [+] ). If there are many packages marked with the [+] it can be useful to check what options you have selected for the software you are trying to build. Changing these options can affect how Spack decides which packages needs to be built frm scratch. <pre><code>Input spec\n--------------------------------\n -   lammps%gcc@12.2.0+asphere+class2+granular+intel~kim+kspace+manybody+molecule+openmp+openmp-package+opt+replica+rigid fftw_precision=single\n -       ^openmpi\n\nConcretized\n--------------------------------\n[+]  murlzo54sqte5xacqcusa6pdmuv7lbju  lammps@20220623%gcc@12.2.0~adios+asphere~atc~awpmd~bocs~body~bpm~brownian~cg-sdk+class2~colloid~colvars~compress~coreshell~cuda~cuda_mps~dielectric~diffraction~dipole~dpd-basic~dpd-meso~dpd-react~dpd-smooth~drude~eff~electrode~exceptions~extra-compute~extra-dump~extra-fix~extra-molecule~extra-pair~fep+ffmpeg+granular~h5md+intel~interlayer~ipo+jpeg~kim~kokkos+kspace~latboltz~latte+lib~machdyn~manifold+manybody~mc~meam~mesont~mgpt~misc~ml-iap~ml-snap~mliap~mofff+molecule~molfile+mpi~mpiio~netcdf~opencl+openmp+openmp-package+opt~orient~peri~phonon~plugin~plumed+png~poems~ptm~python~qeq~qtb~reaction~reaxff+replica+rigid~shock~smtbq~snap~sph~spin~srd~tally~uef~user-adios~user-atc~user-awpmd~user-bocs~user-brownian~user-cgsdk~user-colvars~user-diffraction~user-dpd~user-drude~user-eff~user-fep~user-h5md~user-intel~user-lb~user-manifold~user-meamc~user-mesodpd~user-mesont~user-mgpt~user-misc~user-mofff~user-molfile~user-netcdf~user-omp~user-phonon~user-plumed~user-ptm~user-qtb~user-reaction~user-reaxc~user-sdpd~user-smd~user-smtbq~user-sph~user-tally~user-uef~user-yaff~voronoi~yaff build_system=cmake build_type=RelWithDebInfo fftw_precision=single lammps_sizes=smallbig arch=linux-rocky8-x86_64\n[^]  7kayesehfqsqbz3anbeuesrhg7jivrh7      ^cmake@3.24.3%gcc@12.2.0~doc+ncurses+ownlibs~qt build_system=generic build_type=Release arch=linux-rocky8-x86_64\n[^]  c5ckfq5br4hzxtjpinax3wmblpxcwccq          ^ncurses@6.3%gcc@12.2.0~symlinks+termlib abi=none build_system=autotools arch=linux-rocky8-x86_64\n[^]  teeyrkysydy6st2gjgmlilsqhdvhytxg          ^openssl@1.1.1s%gcc@12.2.0~docs~shared build_system=generic certs=mozilla arch=linux-rocky8-x86_64\n[^]  acak7eo66b264d5tnlrgdsqquriqzikw              ^ca-certificates-mozilla@2022-10-11%gcc@12.2.0 build_system=generic arch=linux-rocky8-x86_64\n[+]  o2bzn3dn2oxv3z2gxmjdbqdrg2cujdub      ^ffmpeg@4.4.1%gcc@12.2.0~X~avresample+bzlib~drawtext+gpl~libaom~libmp3lame~libopenjpeg~libopus~libsnappy~libspeex~libssh~libvorbis~libvpx~libwebp~libx264~libzmq~lzma~nonfree~openssl~sdl2+shared+version3 build_system=autotools arch=linux-rocky8-x86_64\n[+]  da4dhnudfslxlganozpe6kgegttb5wqt          ^alsa-lib@1.2.3.2%gcc@12.2.0~python build_system=autotools arch=linux-rocky8-x86_64\n[^]  y2uodljjpotqgdgf4ync654ow6zq3yui          ^bzip2@1.0.8%gcc@12.2.0~debug~pic+shared build_system=generic arch=linux-rocky8-x86_64\n[^]  lmmbcwxrpcrrdzfv46acihazdbchects              ^diffutils@3.6%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  y7hkdyocmeei7gipuzq6dauwoscds65d          ^libiconv@1.16%gcc@12.2.0 build_system=autotools libs=shared,static arch=linux-rocky8-x86_64\n[+]  up3oys25bdxvv5n2cdvhyaodk4pjm46t          ^yasm@1.3.0%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  xzya3i6ni4zkbycrk2bnbwba3dtfjpag          ^zlib@1.2.13%gcc@12.2.0+optimize+pic+shared build_system=makefile arch=linux-rocky8-x86_64\n[^]  qiaruimvw6zu2h4f5eolqom7tixem6vk      ^fftw@3.3.10%gcc@12.2.0+mpi+openmp~pfft_patches build_system=autotools precision=double,float arch=linux-rocky8-x86_64\n[^]  ig3drj7aya7pibjynlbjdki4wj26nvq3      ^libjpeg-turbo@2.1.3%gcc@12.2.0 build_system=generic arch=linux-rocky8-x86_64\n[^]  t4dj6jrogzp26ylmy7meqdm5uerw2vou          ^nasm@2.15.05%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  bpm4irmoa3ly7mn2v2eezr4nvoxt57uz      ^libpng@1.6.37%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  3r4zaihkaqj2gmfvtzk4adiu3qxlzgj5      ^openmpi@4.1.4%gcc@12.2.0~atomics+cuda~cxx~cxx_exceptions~gpfs~internal-hwloc~java+legacylaunchers~lustre~memchecker+pmi+romio+rsh~singularity+static+vt+wrapper-rpath build_system=autotools cuda_arch=none fabrics=ucx schedulers=slurm arch=linux-rocky8-x86_64\n[^]  loulnd3xxa433rvdvtzu67nb4muiyxqt          ^cuda@12.1.0%gcc@12.2.0~allow-unsupported-compilers~dev build_system=generic arch=linux-rocky8-x86_64\n[^]  avncq4uc2k673jnoxdeqijalhwxfu452              ^libxml2@2.10.1%gcc@12.2.0~python build_system=autotools arch=linux-rocky8-x86_64\n[^]  a7ikzndwlj3et447m7ycfy3rjnllhr6c                  ^xz@5.2.7%gcc@12.2.0~pic build_system=autotools libs=shared,static arch=linux-rocky8-x86_64\n[^]  a56oj35bkhqi7rpsxyrzv2cvjhk6f4nl          ^hwloc@2.8.0%gcc@12.2.0~cairo+cuda~gl~libudev+libxml2~netloc+nvml~oneapi-level-zero~opencl+pci~rocm build_system=autotools cuda_arch=none libs=shared,static arch=linux-rocky8-x86_64\n[^]  id32hkaz34tj6rm436wmiaoes7jtjomj              ^libpciaccess@0.16%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  74pwk3n734nymhilw7fvcjhkdzr22xa5                  ^util-macros@1.19.3%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  n3atoewxkrcnzrv35ggcghde7uknwnc2          ^numactl@2.0.14%gcc@12.2.0 build_system=autotools patches=4e1d78c,62fc8a8,ff37630 arch=linux-rocky8-x86_64\n[^]  llmf6eoq46fjuega6mzjc6kjpeta2abx              ^autoconf@2.69%gcc@12.2.0 build_system=autotools patches=35c4492,7793209,a49dd5b arch=linux-rocky8-x86_64\n[^]  t3rctjlwqpmn5x433eeeusphmtypv6g7              ^automake@1.16.5%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  lmcpaypsio2xylqqkhyis36sem4q2uqx              ^libtool@2.4.7%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  fqwqfqqkhvsw7oklwjgsgfz5ksfantur              ^m4@1.4.18%gcc@12.2.0+sigsegv build_system=autotools patches=3877ab5,fc9b616 arch=linux-rocky8-x86_64\n[^]  o3v3w2aysw3bxl2ioig7bu4nl54xb6ln          ^openssh@8.0p1%gcc@12.2.0+gssapi build_system=autotools arch=linux-rocky8-x86_64\n[^]  d345fqycp52qf5jf35in4tkz3bg7en2t          ^perl@5.36.0%gcc@12.2.0+cpanm+shared+threads build_system=generic arch=linux-rocky8-x86_64\n[^]  ehlkmyphsdbfkgvt6wtznpvs6gpelo4a              ^berkeley-db@18.1.40%gcc@12.2.0+cxx~docs+stl build_system=autotools patches=26090f4,b231fcc arch=linux-rocky8-x86_64\n[^]  medv6udj2ovi3p7sjpffxzfl4t4yb6i2              ^gdbm@1.23%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  5zhrz242nlulabfgalogdv6vgxfnigae                  ^readline@8.1.2%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  wxuqfjdqv4bjudl2aixkqcowfz35q62u          ^pkgconf@1.8.0%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  d7f7fwzoomvmc6hwotduhlzmdoc6oz7o          ^pmix@4.1.2%gcc@12.2.0~docs+pmi_backwards_compatibility~restful build_system=autotools arch=linux-rocky8-x86_64\n[^]  dnasb7atyzwlagnyyrplzk5if6efrfbe              ^libevent@2.1.12%gcc@12.2.0+openssl build_system=autotools arch=linux-rocky8-x86_64\n[^]  w4bduhiz53fpkwuucvzayhpj7dquy6wa          ^slurm@22.05.6%gcc@12.2.0~gtk~hdf5~hwloc~mariadb+pmix+readline~restd build_system=autotools sysconfdir=PREFIX/etc arch=linux-rocky8-x86_64\n[^]  xxapazpv4rciyeuajxom5vfll4djhakq          ^ucx@1.13.1%gcc@12.2.0~assertions~backtrace_detail+cma+cuda+dc~debug+dm+examples+gdrcopy+ib_hw_tm~java+knem~logging+mlx5_dv+openmp+optimizations~parameter_checking+pic+rc+rdmacm~rocm+thread_multiple~ucg+ud+verbs~vfs+xpmem build_system=autotools cuda_arch=none libs=shared,static opt=3 patches=32fce32 simd=auto arch=linux-rocky8-x86_64\n[^]  6mhshpnmk5eluz3l7kkiiwaijetaugaz              ^gdrcopy@2.3%gcc@12.2.0 build_system=makefile patches=c5efec1 arch=linux-rocky8-x86_64\n[^]  lf6nroe2ungmj4jfktllyn3lb634phai              ^knem@1.1.4%gcc@12.2.0+hwloc build_system=autotools patches=78885a0 arch=linux-rocky8-x86_64\n[^]  y7we5jx3cbxrdetx4czervl3x5u6sw4d              ^rdma-core@41.0%gcc@12.2.0~ipo build_system=cmake build_type=RelWithDebInfo arch=linux-rocky8-x86_64\n[^]  tf5fu6kqufhtdnnt4v3xyzpyomqlod3x                  ^libnl@3.3.0%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  c4tgrq23hiatbdxkxdsvuzure2uu3igf                      ^bison@3.8.2%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  n4ex34zvvefzgb3kujwzymjcjx7bqy72                      ^flex@2.6.3%gcc@12.2.0+lex~nls build_system=autotools arch=linux-rocky8-x86_64\n[^]  jofcfnbajncj3a3ooylnkls4zrg2gd7u                          ^findutils@4.6.0%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  imomhs5czwwjpjwka3pchqoqmvhkybta                  ^py-docutils@0.19%gcc@12.2.0 build_system=python_pip arch=linux-rocky8-x86_64\n[^]  ru3kxolfjtu3dna4ou6ebnd34xxqdgug                      ^py-pip@22.2.2%gcc@12.2.0 build_system=generic arch=linux-rocky8-x86_64\n[^]  2ze2pnic7bshbeu635yejl6325b57bfh                      ^py-setuptools@65.5.0%gcc@12.2.0 build_system=generic arch=linux-rocky8-x86_64\n[^]  2fvi4pmz4mnqmufgbdusfg3jbxq2xkrn                      ^py-wheel@0.37.1%gcc@12.2.0 build_system=generic arch=linux-rocky8-x86_64\n[^]  flipoyygqdyq56ytifzzitgmb7x4bdno                      ^python@3.10.8%gcc@12.2.0+bz2+ctypes+dbm~debug+libxml2+lzma~nis~optimizations+pic+pyexpat+pythoncmd+readline+shared+sqlite3+ssl~tix~tkinter~ucs4+uuid+zlib build_system=generic patches=0d98e93,7d40923,f2fd060 arch=linux-rocky8-x86_64\n[^]  fw7viptexxob2jlhoimbbx2iuelorefg                          ^expat@2.4.8%gcc@12.2.0+libbsd build_system=autotools arch=linux-rocky8-x86_64\n[^]  wem5x2oesmhxmfp7x63sgdop57ribigd                              ^libbsd@0.11.5%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  jxvqhjl7uir2utgpdxzxho744wnbyicj                                  ^libmd@1.0.4%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  srssurbwe76knqj3m2a2g3yl6htki75r                          ^gettext@0.21.1%gcc@12.2.0+bzip2+curses+git~libunistring+libxml2+tar+xz build_system=autotools arch=linux-rocky8-x86_64\n[^]  4phps4fxpvnorsqhbbizp34nmxgcbhlv                              ^tar@1.30%gcc@12.2.0 build_system=autotools zip=pigz arch=linux-rocky8-x86_64\n[^]  ag3cenfsq7igpp33kkdgs5l6llm7mfaa                          ^libffi@3.4.2%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  uvuc44zwmae3ypmqvjrpymleierkahwj                          ^sqlite@3.39.4%gcc@12.2.0+column_metadata+dynamic_extensions+fts~functions+rtree build_system=autotools arch=linux-rocky8-x86_64\n[^]  f36megz6i272yo6gqj2h72byzupu2u3f                          ^util-linux-uuid@2.38.1%gcc@12.2.0 build_system=autotools arch=linux-rocky8-x86_64\n[^]  omne2gv2jqp4wmdqfnb6wfozhda6mznn              ^xpmem@2.6.5-36%gcc@12.2.0+kernel-module build_system=autotools patches=1a2660a,6be8c5f,7529939 arch=linux-rocky8-x86_64\n</code></pre></p>","tags":["Engaging","Howto Recipes","spack","LAMMPS","Rocky Linux"]},{"location":"recipes/mpi4py/","title":"Installing and Using MPI for Python","text":"<p>MPI for Python (<code>mpi4py</code>) provides Python bindings for the Message Passing Interface (MPI) standard, allowing Python applications to exploit multiple processors on workstations, clusters and supercomputers.</p> <p>You can learn about <code>mpi4py</code> here: https://mpi4py.readthedocs.io/en/stable/.</p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#mpi4py-on-openmind","title":"Mpi4py on OpenMind","text":"","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#install","title":"Install","text":"<p>If you use an Anaconda module, no installation is required.</p> <p>If you want to use Anaconda in your directory, refer to section 3 on this page to set it up, then intall <code>mpi4py</code>,  <pre><code>conda install -c conda-forge mpi4py\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#run-mpi4py","title":"Run Mpi4py","text":"<p>Prepare your Python codes. Example 1: The following is a code for sending and receiving a dictionary. Save it in a file named <code>p2p-send-recv.py</code>. <pre><code>from mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\nif rank == 0:\n    data = {'a': 7, 'b': 3.14}\n    comm.send(data, dest=1, tag=11)\n    print(rank,data)\nelif rank == 1:\n    data = comm.recv(source=0, tag=11)\n    print(rank,data)\n</code></pre></p> <p>Example 2: The following is a code for sending and receiving an array. Save it in a file named <code>p2p-array.py</code>. <pre><code>from mpi4py import MPI\nimport numpy\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\n# passing MPI datatypes explicitly\nif rank == 0:\n    data = numpy.arange(1000, dtype='i')\n    comm.Send([data, MPI.INT], dest=1, tag=77)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(1000, dtype='i')\n    comm.Recv([data, MPI.INT], source=0, tag=77)\n    print(rank,data)\n\n# automatic MPI datatype discovery\nif rank == 0:\n    data = numpy.arange(100, dtype=numpy.float64)\n    comm.Send(data, dest=1, tag=13)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(100, dtype=numpy.float64)\n    comm.Recv(data, source=0, tag=13)\n    print(rank,data)\n</code></pre></p> <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>. <pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n\nmodule load openmpi/gcc/64/1.8.1\nmodule load openmind/anaconda/3-2022.05\n\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre></p> <p>An Openmpi module is needed. If you use Anaconda in your directory, do not load the Anaconda module. </p> <p>Finally submit the job, <pre><code>sbatch p2p-job.sh\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mujoco/","title":"Installing and Using MuJoCo","text":"<p>MuJoCo is a free and open source physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed.</p> <p>You can learn about MuJoCo here: https://mujoco.org.</p> <p>Whether you are installing on Engaging or SuperCloud, you\u2019ll first have to install the MuJoCo binaries. This process is the same on all systems.</p>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#install-the-mujoco-binaries","title":"Install the MuJoCo Binaries","text":"<p>First, install MuJoCo itself somewhere in your home directory. This is as simple as downloading the MuJoCo binaries, which can be found on their web page. For the release that you want, select the file that ends with \u201clinux-x86_64.tar.gz\u201d, for example for 2.3.0 select mujoco-2.3.0-linux-x86_64.tar.gz. Right click, and copy the link address. Then you can download on one of the login nodes with the \u201cwget\u201d command, and untar:</p> <pre><code>wget https://github.com/deepmind/mujoco/releases/download/2.3.0/mujoco-2.3.0-linux-x86_64.tar.gz\ntar -xzf mujoco-2.3.0-linux-x86_64.tar.gz\n</code></pre> <p>In order for mujoco-py to find the MuJoCo binaries, set the following paths:</p> <pre><code>export MUJOCO_PY_MUJOCO_PATH=$HOME/path/to/mujoco230/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MUJOCO_PY_MUJOCO_PATH/bin\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#mujoco-on-engaging","title":"MuJoCo on Engaging","text":"","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#install-mujoco-py","title":"Install Mujoco-Py","text":"<p>First, make sure the <code>MUJOCO_PY_MUJOCO_PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables are set pointing to your mujoco installation. You can use the \u201cecho\u201d command to do this:</p> <pre><code>echo MUJOCO_PY_MUJOCO_PATH\necho LD_LIBRARY_PATH\n</code></pre> <p>If any of these are not set properly you can set them as described above (see here for MUJOCO_PY_MUJOCO_PATH, LD_LIBRARY_PATH.</p> <p>Next load either a Python or Anaconda module. In this example I loaded the latest anaconda3 module (run <code>module avail anaconda</code> to see the current list of available anaconda modules):</p> <pre><code>module load anaconda3/2022.10\n</code></pre> <p>From here on you can follow the standard instructions to install mujoco-py, using the <code>--user</code> flag where appropriate to install in your home directory, or install in an anaconda or virtual environment (do not use the <code>--user</code> flag if you want to install in a conda or virtual environment). Here I am installing in my home directory with <code>--user</code>:</p> <pre><code>pip install --user 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Start up python and import mujoco_py to complete the build process:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>If you\u2019d like you can run the few example lines listed on install section of the mujoco-py github page to verify the install went through properly:</p> <pre><code>import mujoco_py\nimport os\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#using-mujoco-in-a-job","title":"Using MuJoCo in a Job","text":"<p>To use MuJoCo you\u2019ll need to first load the same Python or Anaconda module you used to install mujoco-py. If you installed it into a conda environment or python virtual environment, load that environment as well. We recommend you do this in your job submission script rather than in your .bashrc or at the command line before you submit the job. This way you know your job is configured properly every time it runs. You can use the following test scripts to test your MuJoCo setup in a job environment, and as a starting point for your own job:</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> submit_test.sh<pre><code>#!/bin/bash\n\n# Load the same python/anaconda module you used to install mujoco-py\nmodule load python/3.8.3\n\n# Run the script\npython mujoco_test.py\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#mujoco-on-supercloud","title":"MuJoCo on SuperCloud","text":"<p>MuJoCo, particularly mujoco-py, can be tricky to install on SuperCloud as it uses file locking during the install and whenever the package is loaded. File locking is disabled on the SuperCloud shared filesystem performance reasons, but is available on the local disk of each node. Therefore, one workaround is to install mujoco-py on the local disk of one of the login nodes and then copy the install to your home directory. To load the package, the install then needs to be copied to the local disk.</p> <p>We\u2019ve found the most success by doing this with a python virtual environment. By using a python virtual environment you can install any additional packages you need with mujoco-py, and they can be used along with packages in our anaconda module, unlike conda environments.</p> <p>If you haven't already, first follow the instructions above to install the MuJoCo binaries.</p>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#create-the-virtual-environment","title":"Create the Virtual Environment","text":"<p>Next create the virtual environment on the local disk of the login node and install mujoco-py (install the version you would like to use):</p> <pre><code>module load anaconda/2023a\nmkdir /state/partition1/user/$USER\npython -m venv /state/partition1/user/$USER/mujoco_env\nsource /state/partition1/user/$USER/mujoco_env/bin/activate\npip install 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Now install any other packages you need to run your MuJoCo jobs. With virtual environments you won\u2019t see any of the packages you\u2019ve previously installed with <code>pip install --user</code> or what you may have installed in another environment. You should still be able to use any of the packages in the anaconda module you\u2019ve loaded, so no need to install any of those.</p> <pre><code>pip install pkgname1\npip install pkgname2\n</code></pre> <p>Since you are installing into virtual environment, do not use the <code>--user</code> flag.</p> <p>Once you\u2019ve installed the packages you need, start Python and import mujoco_py to finish the build:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>Now that your environment is created, copy it to your home directory for permanent storage.</p> <pre><code>cp -r /state/partition1/user/$USER/mujoco_env $/software/mujoco/\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#running-a-job","title":"Running a Job","text":"<p>Now whenever you use mujoco-py the installation will need to be on the local disk of the node(s) where you are running. In your job script you can add a few lines of code that will check whether the environment exists on the local disk, and if not copy it. You can run these lines during an interactive job as well.</p> <pre><code># Set some useful environment variables\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\n# Check if the environment exists on the local disk. If not copy it over from the home directory.\nif [ ! -d \"$MUJOCO_ENV\" ]; then\n    echo \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\n    mkdir -p /state/partition1/user/$USER\n    cp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\n\n# Load an anaconda module, then activate your mujoco environment\nmodule load anaconda/2023a\nsource $MUJOCO_ENV/bin/activate\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#supercloud-test-scripts","title":"SuperCloud Test Scripts","text":"<p>The following are some test scripts you can use to check that your configuration worked.</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> submit_test.sh<pre><code>#!/bin/bash\n\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\nif [ ! -d \"$MUJOCO_ENV\" ]; then\n    echo \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\n    mkdir -p /state/partition1/user/$USER\n    cp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\n\nmodule load anaconda/2022a\nsource $MUJOCO_ENV/bin/activate\n\npython mujoco_test.py\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/","title":"Example of a minimal program using the nvhpc stack with CUDA aware MPI","text":"","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#about-nvhpc","title":"About NVHPC","text":"<p>NVHPC is an integrated collection of software tools and libraries distributed by NVidia. An overview document describing nvhpc  can be found here. The aim of the nvhpc team is to provide up to date, preconfigured suites of compilers, libraries and tools that are  specifically optimized for NVidia GPU hardware. It supports single and multi-GPU execution.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#basic-usage-example","title":"Basic Usage Example","text":"<p>This example shows steps for using NVHPC to run a simple test MPI program, written in C, that communicates between two GPUs. The detailed steps, that can be executed in an interactive Slurm session, are explained  below.  A complete Slurm job example is shown at the end.</p> <p>Prerequisites</p> <p>This example assumes you have access to a Slurm partition with GPU resources and are working with a Rocky Linux environment.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#1-activate-the-relevant-nvhpc-module","title":"1. Activate the relevant NVHPC module","text":"<p>The NVHPC environment is installed as a module and can be made visible in a session using the command</p> <pre><code>module load nvhpc/2023_233/nvhpc/23.3\n</code></pre> <p>this will add a specific version of the nvhpc software (version 23.3 released in 2023) to a shell or batch script. The software added includes compilers for C, C++ and Fortran; base GPU optimized numerical libraries for linear algebra, Fourier transforms and others; GPU optimized communication libraries supporting MPI, SHMEM and NCCL APIs.</p> <p>An environment variable, <code>NVHPC_ROOT</code>, is also set. This can be used in scripts to reference the locations of libraries when needed.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#2-set-paths-needed-for-compile-step","title":"2. Set paths needed for compile step","text":"<p>Here we use the module environment variable, <code>NVHPC_ROOT</code>, to set environment variables that have paths needed for compilation and linking of code.</p> <pre><code>culibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#3-create-a-c-program-for-that-excutes-some-simple-multi-node-multi-gpu-test-code","title":"3. Create a C program for that excutes some simple multi-node, multi-GPU test code.","text":"<p>The next step is to create a file holding C code that uses MPI to send information between two GPUs  running in different processes. Paste the C code below into a file called <code>test.c</code>.</p> test.c<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;cuda_runtime.h&gt;\nint main(int argc, char *argv[])\n{\n  int myrank, mpi_nranks; \n  int LBUF=1000000;\n  float *sBuf_h, *rBuf_h;\n  float *sBuf_d, *rBuf_d;\n  int bSize;\n  int i;\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);                  // my MPI rank\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;mpi_nranks);\n\n  if ( mpi_nranks != 2 ) { printf(\"Program requires exactly 2 ranks\\n\");exit(-1); }\n\n  int deviceCount;\n  cudaGetDeviceCount(&amp;deviceCount);               // How many GPUs?\n  printf(\"Number of GPUs found = %d\\n\",deviceCount);\n  int device_id = myrank % deviceCount;\n  cudaSetDevice(device_id);                       // Map MPI-process to a GPU\n  printf(\"Assigned GPU %d to MPI rank %d of %d.\\n\",device_id, myrank, mpi_nranks);\n\n  // Allocate buffers on each host and device\n  bSize = sizeof(float)*LBUF;\n  sBuf_h = malloc(bSize);\n  rBuf_h = malloc(bSize);\n  for (i=0;i&lt;LBUF;++i){\n    sBuf_h[i]=(float)myrank;\n    rBuf_h[i]=-1.;\n  }\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  cudaMalloc((void **)&amp;sBuf_d,bSize);\n  cudaMalloc((void **)&amp;rBuf_d,bSize);\n\n  cudaMemcpy(sBuf_d,sBuf_h,bSize,cudaMemcpyHostToDevice);\n\n  if ( myrank == 0 ) {\n   MPI_Recv(rBuf_d,LBUF,MPI_REAL,1,0,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n  } \n  else if ( myrank == 1 ) {\n   MPI_Send(sBuf_d,LBUF,MPI_REAL,0,0,MPI_COMM_WORLD);\n  }\n  else\n  {\n   printf(\"Unexpected myrank value %d\\n\",myrank);\n   exit(-1);\n  }\n\n  cudaMemcpy(rBuf_h,rBuf_d,bSize,cudaMemcpyDeviceToHost);\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#4-compile-program","title":"4. Compile program","text":"<p>Here we use nvhpc MPI wrapper to compile. The two environment variables we set earlier (<code>cuincdir</code> and <code>culibdir</code>) are used to let the compile step know where to find the relevant CUDA header and library files. The CUDA runtime library (<code>cudart</code>) is added as a location for finding CUDA functions the code utilizes.</p> <pre><code>mpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#5-execute-program","title":"5. Execute program","text":"<p>Once code has been compiled the <code>mpiexec</code> command that is part of the <code>nvhpc</code> module can be used to run the test program. The <code>nvhpc</code> module defaults to using its builtin version of OpneMPI. The OpenMPI option <code>btl_openib_warn_no_device_params_found</code> is passed into the OpenMPI runtime library. This option supresses a warning that OpenMPI can generate when it encounters a network device card that is not present in a built-in list that OpenMPI has historically included.</p> <pre><code>mpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre> <p>Running this program using the command above should produce the following output.</p> <pre><code>Number of GPUs found = 1\nNumber of GPUs found = 1\nAssigned GPU 0 to MPI rank 0 of 2.\nrBuf_h[0] = -1.000000\nAssigned GPU 0 to MPI rank 1 of 2.\nrBuf_h[0] = 1.000000\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#example-of-slurm-job-file-for-excuting-this-example","title":"Example of Slurm job file for excuting this example","text":"<p>The job script file below will run all the steps described above. It can  be submitted to Slurm using the command <code>sbatch</code> followed by the filename holding the job script.</p> <pre><code>#!/bin/bash\n#SBATCH -p sched_system_all\n#SBATCH --constraint=rocky8\n#SBATCH -N 2\n#SBATCH -n 2\n#SBATCH --gres=gpu:2\n#SBATCH --time=00:02:00\n#\n# Basic slurm job that tests GPU aware MPI in the NVHPC tool stack.\n#\n#\n#   To submit through Slurm use:\n#\n#   $ sbatch test_cuda_and_mpi.sbatch\n#  \n#   in terminal.\n\n# Write a little log info\necho \"## Start time \\\"\"`date`\"\\\"\"\necho \"## Slurm job running on nodes \\\"${SLURM_JOB_NODELIST}\\\"\"\necho \"## Slurm submit directory \\\"${SLURM_SUBMIT_DIR}\\\"\"\necho \"## Slurm submit host \\\"${SLURM_SUBMIT_HOST}\\\"\"\necho \" \"\n\n\nmodule load nvhpc/2023_233/nvhpc/23.3\nculibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n\ncat &gt; test.c &lt;&lt;'EOFA'\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;cuda_runtime.h&gt;\nint main(int argc, char *argv[])\n{\n  int myrank, mpi_nranks; \n  int LBUF=1000000;\n  float *sBuf_h, *rBuf_h;\n  float *sBuf_d, *rBuf_d;\n  int bSize;\n  int i;\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);                  // my MPI rank\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;mpi_nranks);\n\n  if ( mpi_nranks != 2 ) { printf(\"Program requires exactly 2 ranks\\n\");exit(-1); }\n\n  int deviceCount;\n  cudaGetDeviceCount(&amp;deviceCount);               // How many GPUs?\n  printf(\"Number of GPUs found = %d\\n\",deviceCount);\n  int device_id = myrank % deviceCount;\n  cudaSetDevice(device_id);                       // Map MPI-process to a GPU\n  printf(\"Assigned GPU %d to MPI rank %d of %d.\\n\",device_id, myrank, mpi_nranks);\n\n  // Allocate buffers on each host and device\n  bSize = sizeof(float)*LBUF;\n  sBuf_h = malloc(bSize);\n  rBuf_h = malloc(bSize);\n  for (i=0;i&lt;LBUF;++i){\n    sBuf_h[i]=(float)myrank;\n    rBuf_h[i]=-1.;\n  }\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  cudaMalloc((void **)&amp;sBuf_d,bSize);\n  cudaMalloc((void **)&amp;rBuf_d,bSize);\n\n  cudaMemcpy(sBuf_d,sBuf_h,bSize,cudaMemcpyHostToDevice);\n\n  if ( myrank == 0 ) {\n   MPI_Recv(rBuf_d,LBUF,MPI_REAL,1,0,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n  } \n  else if ( myrank == 1 ) {\n   MPI_Send(sBuf_d,LBUF,MPI_REAL,0,0,MPI_COMM_WORLD);\n  }\n  else\n  {\n   printf(\"Unexpected myrank value %d\\n\",myrank);\n   exit(-1);\n  }\n\n  cudaMemcpy(rBuf_h,rBuf_d,bSize,cudaMemcpyDeviceToHost);\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}\nEOFA\n\nmpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n\nmpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#engaging","title":"Engaging","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>Example of building custom LAMMPS configuration using spack</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gpu","title":"GPU","text":"<ul> <li>GROMACS</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gromacs","title":"GROMACS","text":"<ul> <li>GROMACS</li> </ul>"},{"location":"tags/#h100","title":"H100","text":"<ul> <li>Getting started on 8-way H100 nodes on Satori</li> </ul>"},{"location":"tags/#howto-recipes","title":"Howto Recipes","text":"<ul> <li>GROMACS</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>Example of building custom LAMMPS configuration using spack</li> <li>MPI for Python</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#lammps","title":"LAMMPS","text":"<ul> <li>Example of building custom LAMMPS configuration using spack</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>GROMACS</li> <li>MPI for Python</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#mujoco","title":"MuJoCo","text":"<ul> <li>MuJoCo</li> </ul>"},{"location":"tags/#openmind","title":"OpenMind","text":"<ul> <li>MPI for Python</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>MPI for Python</li> </ul>"},{"location":"tags/#rocky-linux","title":"Rocky Linux","text":"<ul> <li>Example of building custom LAMMPS configuration using spack</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#satori","title":"Satori","text":"<ul> <li>ORCD Systems</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#supercloud","title":"SuperCloud","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MuJoCo</li> </ul>"},{"location":"tags/#cuda","title":"cuda","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#cuda-aware-mpi","title":"cuda aware mpi","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#nvhpc","title":"nvhpc","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#spack","title":"spack","text":"<ul> <li>Example of building custom LAMMPS configuration using spack</li> </ul>"}]}