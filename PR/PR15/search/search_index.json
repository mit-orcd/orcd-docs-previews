{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MIT Office of Research Computing and Data Hands On Help Pages","text":"<p>The MIT Office or Research Computing and Data (ORCD) provides access and support for the compute and data needs of a wide range of research activities. These pages provide  help material for hands-on working with ORCD supported services. </p> <p>Help working with ORCD services is also available through email to orcd-help@mit.edu, please  feel free to contact us with questions and suggestions.</p>"},{"location":"data-security/","title":"Data Security and Privacy","text":""},{"location":"orcd-systems/","title":"ORCD Systems","text":"<p>ORCD operates and provides support and training for a number of cluster computer systems available to all researchers. These systems all run with a Slurm scheduler and have a web portal for interactive computing. These are Engaging, SuperCloud, and Satori.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-cluster","title":"Engaging Cluster","text":"<p>The engaging cluster is a mixed CPU and GPU computng cluster that is openly available to all  research projects at MIT. It has around 80,000 x86 CPU cores and 300  GPU cards ranging from K80 generation to recent Voltas. Hardware access is through the Slurm  resource scheduler that supports batch and interactive workloads and allows dedicated reservations. The cluster has a large shared file system for working datasets. Additional compute and storage  resources can be purchased by PIs. A wide range of standard software is available and the Docker  compatible Singularity container tool is supported. User-level tools like Anaconda for Python,  R libraries and Julia packages are all supported. A range of PI group maintained custom software  stacks are also available through the widely adopted environment modules toolkit. A standard,  open-source, web-based portal supporting Jupyter notebooks, R studio, Mathematica and X graphics  is available at https://engaging-ood.mit.edu. Further information and support is available from orcd-help-engaging@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-engaging","title":"How to Get an Account on Engaging","text":"<p>Accounts on the engaging cluster are connected to your main MIT institutional kerberos id.  Connecting to the cluster for the first time through its web portal automatically activates an account with basic access to resources. See this page for instructions on how to log in.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-quick-links","title":"Engaging Quick Links","text":"<ul> <li>Documentation: https://engaging-web.mit.edu/eofe-wiki/</li> <li>OnDemand web portal: https://engaging-ood.mit.edu</li> <li>Help: Send email to orcd-help-engaging@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori","title":"Satori","text":"<p>Satori is an IBM Power 9 large memory node system. It is open to everyone on campus and has  optimized software stacks for machine learning and for image stack post-processing for  MIT.nano Cryo-EM facilities. The system has 256 NVidia Volta GPU cards attached in groups of  four to 1TB memory nodes and a total of 2560 Power 9 CPU cores. Hardware access is through the  Slurm resource scheduler that supports batch and interactive workload and allows dedicated  reservations. A wide range of standard software is available and the Docker compatible  Singularity container tool is supported. A standard web based portal  https://satori-portal.mit.edu with Jupyter notebook support is available. Additional compute  and storage resources can be purchased by PIs and integrated into the system. Further  information and support is available at orcd-help-satori@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-satori","title":"How to Get an Account on Satori","text":"<p>You can get an account by logging into https://satori-portal.mit.edu with your MIT credentials.  See this page for instructions: https://mit-satori.github.io/satori-basics.html#how-can-i-get-an-account.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori-quick-links","title":"Satori Quick Links","text":"<ul> <li>Documentation: (https://mit-satori.github.io/</li> <li>OnDemand web portal: https://satori-portal.mit.edu</li> <li>Help: Send email to orcd-help-satori@mit.edu</li> <li>Slack: https://mit-satori.slack.com/</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud","title":"SuperCloud","text":"<p>The SuperCloud system is a collaboration with MIT Lincoln Laboratory on a shared facility that  is optimized for streamlining open research collaborations with Lincoln Laboratory. The facility  is open to everyone on campus. The latest SuperCloud system has more than 16,000 x86 CPU cores  and more than 850 NVidia Volta GPUs in total. Hardware access is through the Slurm resource  scheduler that supports batch and interactive workload and allows dedicated reservations. A wide  range of standard software is available and the Docker compatible Singularity container tool is  supported. A custom, web-based portal supporting Jupyter notebooks is available at https://txe1-portal.mit.edu/. Further information and support is available at  supercloud@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-supercloud","title":"How to Get an Account on SuperCloud","text":"<p>To request a SuperCloud account follow the instructions on this page:  https://supercloud.mit.edu/requesting-account.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud-quick-links","title":"SuperCloud Quick Links","text":"<ul> <li>Documentation: https://supercloud.mit.edu/</li> <li>Online Course: https://learn.llx.edly.io/course/practical-hpc/</li> <li>Web portal: https://txe1-portal.mit.edu/</li> <li>Help: Send email to supercloud@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#engaging","title":"Engaging","text":"<ul> <li>ORCD Systems</li> </ul>"},{"location":"tags/#satori","title":"Satori","text":"<ul> <li>ORCD Systems</li> </ul>"},{"location":"tags/#supercloud","title":"SuperCloud","text":"<ul> <li>ORCD Systems</li> </ul>"},{"location":"filesystems-file-transfer/project-filesystems/","title":"Project Specific Filesystems","text":""},{"location":"filesystems-file-transfer/project-filesystems/#purchasing-storage","title":"Purchasing Storage","text":"<p>Additional project and lab storage can be purchased on ORCD shared clusters by individual PI groups. This storage is mounted on the cluster and access to the storage is managed  by the group through MIT Web Moira, https://groups.mit.edu/webmoira/ (see below for details).</p> <p>Current pricing for storage is</p> Storage Type Pricing Duration Backup NESE encrypted at rest   disk. $2.50/month  50TB minimum,    10TB increments. 12 month  minimum. No automated backup. <p>The NESE encrypted at rest disk uses a large centrally managed storage cloud at the MGHPCC facility. Any shared ORCD cluster at the MGHPCC can access this storage. Data on NESE disk is transparently encrypted at rest.</p> <p>To purchase storage please send an email to orcd-help@mit.edu.</p>"},{"location":"filesystems-file-transfer/project-filesystems/#managing-access-using-mit-web-moira","title":"Managing access using MIT Web Moira","text":"<p>Individual group storage is configured so that access is limited to a set of accounts belonging to a web moira list that is defined for the group store. The owner and administrators of group storage can manage access themselves, by modifying the membership of an associated moira list under https://groups.mit.edu/webmoira/list/. The name of the list corresponds to the UNIX group name associated with the ORCD shared  cluster storage.</p>"},{"location":"filesystems-file-transfer/project-filesystems/#moira-web-interface-example","title":"Moira Web Interface Example","text":"<p>The figure below shows a screenshot of the web moira management page at https://groups.mit.edu/webmoira/list/cnh_research_computing for a hypothetical storage group named <code>cnh_research_computing</code>. The interface provides a  self-service mechanism for controlling access to any storage belonging to this group. MIT account ids can be added and  removed as needed from this list by the storage access administrators.</p> <p></p>"},{"location":"images/","title":"Index","text":""},{"location":"images/#directory-of-static-images","title":"Directory of static images","text":""},{"location":"recipes/mujoco/","title":"Installing and Using MuJoCo","text":"<p>MuJoCo is a free and open source physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed.</p> <p>You can learn about MuJoCo here: https://mujoco.org.</p> <p>Whether you are installing on Engaging or SuperCloud, you\u2019ll first have to install the MuJoCo binaries. This process is the same on all systems.</p>"},{"location":"recipes/mujoco/#install-the-mujoco-binaries","title":"Install the MuJoCo Binaries","text":"<p>First, install MuJoCo itself somewhere in your home directory. This is as simple as downloading the MuJoCo binaries, which can be found on their web page. For the release that you want, select the file that ends with \u201clinux-x86_64.tar.gz\u201d, for example for 2.3.0 select mujoco-2.3.0-linux-x86_64.tar.gz. Right click, and copy the link address. Then you can download on one of the login nodes with the \u201cwget\u201d command, and untar:</p> <pre><code>wget https://github.com/deepmind/mujoco/releases/download/2.3.0/mujoco-2.3.0-linux-x86_64.tar.gz\ntar -xzf mujoco-2.3.0-linux-x86_64.tar.gz\n</code></pre> <p>In order for mujoco-py to find the MuJoCo binaries, set the following paths:</p> <pre><code>export MUJOCO_PY_MUJOCO_PATH=$HOME/path/to/mujoco230/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MUJOCO_PY_MUJOCO_PATH/bin\n</code></pre>"},{"location":"recipes/mujoco/#mujoco-on-engaging","title":"MuJoCo on Engaging","text":""},{"location":"recipes/mujoco/#install-dependency","title":"Install Dependency","text":"<p>On Engaging, MuJoCo requires an additional dependency, patchelf, that is very straightforward to install. First select a version on this page, right click the link that includes \u201c-x86_64.tar.gz\u201d and select \u201cCopy link address\u201d or similar. In this example I am using version 0.17.0. Then in a terminal on Engaging, create and go to the directory where you want to put the dependency and download the software with the wget or curl command and untar the download:</p> <pre><code>cd path/to/mujoco/deps\nwget https://github.com/NixOS/patchelf/releases/download/0.17.0/patchelf-0.17.0-x86_64.tar.gz\ntar -xzf patchelf-0.17.0-x86_64.tar.gz\n</code></pre> <p>If you are using a different version the URL and tar file name will be different.</p> <p>Once you\u2019ve run this command you should see a \u201cbin\u201d directory. Put the full path to this bin directory on your <code>PATH</code> environment variable:</p> <pre><code>export PATH=$HOME/path/to/mujoco/deps/bin:$PATH\n</code></pre>"},{"location":"recipes/mujoco/#install-mujoco-py","title":"Install Mujoco-Py","text":"<p>First, make sure the <code>MUJOCO_PY_MUJOCO_PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables are set pointing to your mujoco installation, and your PATH environment variable includes the path to the patchelf bin directory. You can use the \u201cecho\u201d command to do this:</p> <pre><code>echo MUJOCO_PY_MUJOCO_PATH\necho LD_LIBRARY_PATH\necho PATH\n</code></pre> <p>If any of these are not set properly you can set them as described above (see here for MUJOCO_PY_MUJOCO_PATH, LD_LIBRARY_PATH, and here for patchelf).</p> <p>Next load either a Python or Anaconda module. In this example I loaded the python/3.8.3 module:</p> <pre><code>module load python/3.8.3\n</code></pre> <p>From here on you can follow the standard instructions to install mujoco-py, using the <code>--user</code> flag where appropriate to install in your home directory, or install in an anaconda or virtual environment. Here I am installing in my home directory with <code>--user</code>:</p> <pre><code>pip install --user 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Start up python and import mujoco_py to complete the build process:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>If you\u2019d like you can run the few example lines listed on install section of the mujoco-py github page to verify the install went through properly:</p> <pre><code>import mujoco_py\nimport os\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre>"},{"location":"recipes/mujoco/#using-mujoco-in-a-job","title":"Using MuJoCo in a Job","text":"<p>To use MuJoCo you\u2019ll need to first load the same Python or Anaconda module you used to install mujoco-py. If you installed it into a conda environment or python virtual environment, load that environment as well. We recommend you do this in your job submission script rather than in your .bashrc or at the command line before you submit the job. This way you know your job is configured properly every time it runs. You can use the following test scripts to test your MuJoCo setup in a job environment, and as a starting point for your own job:</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> submit_test.sh<pre><code>#!/bin/bash\n# Load the same python/anaconda module you used to install mujoco-py\nmodule load python/3.8.3\n\n# Run the script\npython mujoco_test.py\n</code></pre>"},{"location":"recipes/mujoco/#mujoco-on-supercloud","title":"MuJoCo on SuperCloud","text":"<p>MuJoCo, particularly mujoco-py, can be tricky to install on SuperCloud as it uses file locking during the install and whenever the package is loaded. File locking is disabled on the SuperCloud shared filesystem performance reasons, but is available on the local disk of each node. Therefore, one workaround is to install mujoco-py on the local disk of one of the login nodes and then copy the install to your home directory. To load the package, the install then needs to be copied to the local disk.</p> <p>We\u2019ve found the most success by doing this with a python virtual environment. By using a python virtual environment you can install any additional packages you need with mujoco-py, and they can be used along with packages in our anaconda module, unlike conda environments.</p> <p>If you haven't already, first follow the instructions above to install the MuJoCo binaries.</p>"},{"location":"recipes/mujoco/#create-the-virtual-environment","title":"Create the Virtual Environment","text":"<p>Next create the virtual environment on the local disk of the login node and install mujoco-py (install the version you would like to use):</p> <pre><code>module load anaconda/2023a\nmkdir /state/partition1/user/$USER\npython -m venv /state/partition1/user/$USER/mujoco_env\nsource /state/partition1/user/$USER/mujoco_env/bin/activate\npip install 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Now install any other packages you need to run your MuJoCo jobs. With virtual environments you won\u2019t see any of the packages you\u2019ve previously installed with <code>pip install --user</code> or what you may have installed in another environment. You should still be able to use any of the packages in the anaconda module you\u2019ve loaded, so no need to install any of those.</p> <pre><code>pip install pkgname1\npip install pkgname2\n</code></pre> <p>Since you are installing into virtual environment, do not use the <code>--user</code> flag.</p> <p>Once you\u2019ve installed the packages you need, start Python and import mujoco_py to finish the build:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>Now that your environment is created, copy it to your home directory for permanent storage.</p> <pre><code>cp -r /state/partition1/user/$USER/mujoco_env $/software/mujoco/\n</code></pre>"},{"location":"recipes/mujoco/#running-a-job","title":"Running a Job","text":"<p>Now whenever you use mujoco-py the installation will need to be on the local disk of the node(s) where you are running. In your job script you can add a few lines of code that will check whether the environment exists on the local disk, and if not copy it. You can run these lines during an interactive job as well.</p> <pre><code># Set some useful environment variables\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\n# Check if the environment exists on the local disk. If not copy it over from the home directory.\nif [ ! -d \"$MUJOCO_ENV\" ]; then\necho \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\nmkdir -p /state/partition1/user/$USER\ncp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\n# Load an anaconda module, then activate your mujoco environment\nmodule load anaconda/2023a\nsource $MUJOCO_ENV/bin/activate\n</code></pre>"},{"location":"recipes/mujoco/#supercloud-test-scripts","title":"SuperCloud Test Scripts","text":"<p>The following are some test scripts you can use to check that your configuration worked.</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> submit_test.sh<pre><code>#!/bin/bash\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\nif [ ! -d \"$MUJOCO_ENV\" ]; then\necho \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\nmkdir -p /state/partition1/user/$USER\ncp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\nmodule load anaconda/2022a\nsource $MUJOCO_ENV/bin/activate\n\npython mujoco_test.py\n</code></pre>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#engaging","title":"Engaging","text":"<ul> <li>ORCD Systems</li> </ul>"},{"location":"tags/#satori","title":"Satori","text":"<ul> <li>ORCD Systems</li> </ul>"},{"location":"tags/#supercloud","title":"SuperCloud","text":"<ul> <li>ORCD Systems</li> </ul>"}]}