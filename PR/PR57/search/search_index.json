{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MIT Office of Research Computing and Data Hands On Help Pages","text":"<p>The MIT Office or Research Computing and Data (ORCD) provides access and support for the compute and data needs of a wide range of research activities. These pages provide  help material for hands-on working with ORCD supported services. </p> <p>Help working with ORCD services is also available through email to orcd-help@mit.edu, please  feel free to contact us with questions and suggestions. For more information on existing documentation,  office hours, and other ways to get help, please see our Getting Help page.</p>"},{"location":"acknowledgements/","title":"Acknowledging Us","text":"<p>If have used our systems or consultation services and you would like to acknowledge the MIT Office of Research Computing an Data in your paper, we recommend adding the following to your Acknowledgments section (be sure to select the applicable resource(s) from among those in the brackets below):</p> <p>Acknowledgement Statement</p> <p>The authors acknowledge the MIT Office of Research Computing and Data for providing [high performance computing, consultation, data] resources that have contributed to the research results reported within this paper.</p> <p>Thank you for acknowledging us \u2013 we appreciate it.</p>"},{"location":"code-of-conduct/","title":"Acceptable Use and Code of Conduct","text":"<p>The ORCD systems are operated by the MIT Office of Research Computing and Data and certain appropriate common sense rules apply to working on it.</p>"},{"location":"code-of-conduct/#acceptable-use-guidelines","title":"Acceptable Use Guidelines","text":"<p>ORCD systems are intended for research associated with MIT projects or collaborations around MIT research projects. That can cover a lot of things, but all account holders are expected to use judgement and apply common sense to their use of the system. The system is not to be used to support commercial activities or for non-MIT related activities. It is not to be used for anything that might be construed as illegal or criminal. Datasets on the system must have been obtained legitimately and the system is not to be used for working with unanonymized data or data subject to ITAR or other national security  restrictions. See the Data Security and Privacy page for more information about data. If you are unsure about a planned use, please feel free to contact orcd-help@mit.edu. </p> <p>All systems are covered by MIT Institute wide policies for acceptable use of information technology, including the MITnet Rules of Use. For data practices refer to our page on Data Security and Privacy and the links on that page, particularly MIT's page on Information Protection.</p> <p>Account holders should not share accounts and should take reasonable precautions to ensure  that credentials for accessing the system (passwords, ssh keys, etc.) are kept secure.  All account holders agree to respect requests from support staff around how they use  the system. The support staff may, as needed, impose whatever policies are required to ensure the system runs well for all projects on the system. </p>"},{"location":"code-of-conduct/#code-of-conduct","title":"Code of Conduct","text":"<p>ORCD systems are shared resource used by a wide community. All people involved in its use and operations should try their utmost to be courteous and kind at all times. Members of the  ORCD community should be respectful toward one another and endeavor to ensure a  welcoming and collegial environment for all. Account holders are also expected to respect privacy of others activities on the system, and not to try to gain access to parts of the system they are not explicitly authorized to access.</p>"},{"location":"data-security/","title":"Data Security and Privacy","text":""},{"location":"data-security/#what-data-can-be-stored","title":"What data can be stored?","text":"<p>All ORCD current systems are only suitable for storing data with low-level security requirements. This means that they are not to be used to store sensitive data, such as personal information, financial information, or intellectual property. Additionally, they are not to be used to store data that is subject to use agreements that require security controls or audit tracking.</p> <p>The following data types are suitable for ORCD systems:</p> <ul> <li>Anything in the low-risk category described at the MIT IS&amp;T data risk classification pages</li> <li>Drafts of unpublished research papers and results that are based on low-risk data</li> </ul> <p>Anything else in the medium-risk or higher risk levels of the MIT IS&amp;T data risk classification pages should not be stored or analyzed on current ORCD systems.</p> <p>If you have any questions about whether your data is appropriate for storing on ORCD systems please feel free to reach out to us at orcd-help@mit.edu. </p>"},{"location":"data-security/#where-can-more-sensitive-data-be-processed-and-stored","title":"Where can more sensitive data be processed and stored?","text":"<p>The ORCD team is currently developing a system for more sensitive data. If you have sensitive data and would like to learn about our plans please feel free to get in touch at orcd-help@mit.edu.</p>"},{"location":"data-security/#support-team-access-to-orcd-system-accounts-and-resources","title":"Support team access to ORCD system accounts and resources","text":"<p>Support staff may occasionally access accounts of other users to help debug and troubleshoot problems. All access will be limited to the minimum reasonably needed to address a problem. Staff will not share any data or contents beyond the needs for providing adequate systems support and ensuring stability and security of the systems. </p>"},{"location":"getting-help/","title":"Getting Help","text":"","tags":["Getting Help"]},{"location":"getting-help/#additional-documentation","title":"Additional Documentation","text":"<p>If you haven't found your answer elsewhere in these pages, your answer may be in the documentation for the system you are using:</p> <ul> <li>Engaging Documentation</li> <li>Satori Documentation</li> <li>SuperCloud Documentation</li> <li>OpenMind Documentation</li> </ul>","tags":["Getting Help"]},{"location":"getting-help/#email","title":"Email","text":"<p>If you can't find your answer in the documentation, please use one of the email lists below to contact us. With the exception of SuperCloud, these lists will create a ticket that our team can assign and track. In all cases these mailing lists includes the entire team, so the best available person to answer your question will respond. Sending email to the entire team will also likely get you the fastest response. Please do not send email directly to individual team members.</p> <ul> <li>General ORCD Questions: orcd-help@mit.edu</li> <li>Engaging: orcd-help-engaging@mit.edu</li> <li>Satori: orcd-help-satori@mit.edu</li> <li>OpenMind: orcd-help-openmind@mit.edu</li> <li>SuperCloud: supercloud@mit.edu</li> </ul> <p>In this email, please provide, where applicable:</p> <ul> <li>Description of your issue or request</li> <li>The command that you used to launch your job</li> <li>Job ID(s)</li> <li>What you tried</li> <li>The full error message you are receiving</li> <li>Any supporting files (code, submission scripts, screenshots, etc)</li> </ul>","tags":["Getting Help"]},{"location":"getting-help/#office-hours","title":"Office Hours","text":"<p>We host weekly office hours. Office Hours are a time when you can drop in and ask us questions. It is a great time to discuss or troubleshoot something that is difficult over email. See the table below for the available Office Hours sessions.</p> Session Time Location Tuesday In Person Office Hours Tuesdays 10-11 am 46-4199 Thursday In Person Office Hours Thursdays 2-3 pm GIS and Data Lab in the Rotch Library (7-238) Friday Virtual Office Hours First, Third, and Fifth Fridays 2-3pm Zoom, email orcd-help@mit.edu for the link","tags":["Getting Help"]},{"location":"getting-started/","title":"Getting Started Tutorial","text":"<p>This page contains the most common steps for setting up and getting started with your ORCD system account. We provide this page as a convenient reference to get started. Each system has its own in-depth documentation which can be found on the ORCD Systems page.</p> <p>Sections that are system-specific will be shown under a list of tabs. Click on the tab for the system you are using and the rest of the page will show the information for that system.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#getting-an-account","title":"Getting an Account","text":"<p>If you don't already have an account, click on the tab for the system you are interested in and follow the instructions.</p> EngagingSatoriSuperCloudOpenMind <p>Login into the respective OnDemand Portal https://engaging-ood.mit.edu using your MIT kerberos credentials. The system will then be prompted to create your account automatically. Wait couple minutes for the system to create all the pieces for your account before submitting your first job.</p> <p>Login into the respective OnDemand Portal https://satori-portal.mit.edu/ using your MIT kerberos credentials. The system will then be prompted to create your account automatically. Wait couple minutes for the system to create all the pieces for your account before submitting your first job.</p> <p>Follow the instructions on the Account Request Page.</p> <p>OpenMind will be available to the general MIT Community starting 2024. Those currently eligible can follow the instructions on the Getting an Account page.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#logging-in","title":"Logging In","text":"<p>The first thing you should do when you get a new account is verify that you can log in. The different ORCD systems provide multiple ways to log in, including both ssh and web portals. Links to instructions for the different systems are below.</p> EngagingSatoriSuperCloudOpenMind <p>See the Logging into Engaging page for full documentation.</p> <p>See the Logging into Satori page for full documentation.</p> <p>See the Logging into SuperCloud page for full documentation.</p> <p>See the Logging into OpenMind page for full documentation.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#terminal-with-ssh","title":"Terminal with SSH","text":"EngagingSatoriSuperCloudOpenMind <p>Engaging has four login nodes: <code>eofe7</code>, <code>eofe8</code>, <code>eofe9</code>, and <code>eofe10</code>. Replace <code>USERNAME</code> below with your Kerberos username and use the login node you would like to log in with, the example below is using <code>eofe10</code>.</p> <pre><code>ssh USERNAME@eofe10.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password.  You can add an ssh key if you do not want to enter your Kerberos password at login. <code>eofe9</code> and <code>eofe10</code> also require Two-Factor Authentication.</p> <p>Satori has two login nodes: <code>satori-login-001</code> and <code>satori-login-002</code>. Replace <code>USERNAME</code> below with your Kerberos username and use the login node you would like to log in with, the example below is using <code>satori-login-001</code>.</p> <pre><code>ssh USERNAME@satori-login-001.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password. You can add an ssh key if you do not want to enter your Kerberos password at login.</p> <p>In order to log into SuperCloud with ssh you will need to add ssh keys to your account on the Web Portal. Follow the instructions on the SuperCloud Getting Started page to add your keys.</p> <p>Then you can log in with ssh using the following command, where <code>USERNAME</code> is your username on the MIT SuperCloud system:</p> <pre><code>ssh USERNAME@txe1-login.mit.edu\n</code></pre> <p>Log into OpenMind with the following command in a terminal window. Replace <code>USERNAME</code> below with your Kerberos username.</p> <pre><code>ssh USERNAME@openmind.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password. You can add an ssh key if you do not want to enter your Kerberos password at login.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#web-portal","title":"Web Portal","text":"EngagingSatoriSuperCloudOpenMind <p>You can log into OnDemand Web Portal with the link: https://engaging-ood.mit.edu. For full detailed instructions please see the Engaging Documentation.</p> <p>You can log into the OnDemand Web Portal with the link: https://satori-portal.mit.edu. For full detailed instructions please see the Satori Documentation.</p> <p>You can log into the SuperCloud Web Portal with the link: https://txe1-portal.mit.edu. For full detailed instructions please see the SuperCloud Documentation.</p> <p>OpenMind does not currently have a web portal, but there are plans to add one in the future. Check back, and in the meantime check out OpenMind's documentation on the FastX Remote Desktop. You may find it provides what you are looking for.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#shared-hpc-clusters","title":"Shared HPC Clusters","text":"<p>Each ORCD system is a shared HPC cluster. You are sharing this resources with a number of other researchers, staff, and students so it is important that you read this page and use the system as intended.</p> <p>Being a cluster, there are several machines connected together with a network. We refer to these as nodes. Most nodes in the cluster are referred to as compute nodes, this is where the computation is done on the system (where you will run your code). When you ssh into the system you are on a special purpose node called the login node. The login node, as its name suggests, is where you log in and is for editing code and files, installing packages and software, downloading data, and starting jobs to run your code on one of the compute nodes.</p> <p>Each job is started using a piece of software called the scheduler, which you can think of as a resource manager. You let it know what resources you need and what you want to run, and the scheduler will find those resources and start your job on them. When your job completes those resources are relinquished. The scheduler is what ensures that no two jobs are using the same resources, so it is very important not to run anything unless it is submitted properly through the scheduler.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#software-and-packages","title":"Software and Packages","text":"<p>The first thing you may want to do is make sure the system has the software and packages you need. We have installed a lot of software and packages on the system already, even though it may not be immediately obvious that it is there. Review the page for the system you are using paying particular attention to the section on modules and installing packages for the language that you use:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging Software Documentation Page</p> <p>Satori Software Documentation Page</p> <p>SuperCloud Software Documentation Page</p> <p>OpenMind Software Documentation Page</p> <p>If you are ever unsure if we have a particular software, and you cannot find it, please send us an email and ask before you spend a lot of time trying to install it. If we have it, we can point you to it, provide advice on how to use it, and if we don't have it we can often give pointers on how to install it. Further, if a lot of people request the same software, we may consider adding it to the system image.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#linux-command-line","title":"Linux Command Line","text":"<p>Every ORCD system runs Linux, so much of what you do on the cluster involves the Linux command line. That doesn't mean you have to be a Linux expert to use the system! However the more you can get comfortable with the Linux command line and a handful of basic commands, the easier using the system will be. If you are already familiar with Linux, feel free to skip this section, or skim as a refresher.</p> <p>Most Linux commands deal with directories and files. A directory, synonymous to a folder, contains files and other directories. The list of directories that lead to a particular directory or file is called its path. In Linux, directories on a path are separated by forward slashes <code>/</code>. It is also important to note that everything in Linux is case sensitive, so a file <code>myScript.sh</code> is not the same as the file <code>myscript.sh</code>. When you first log in you are in you home directory. Your home directory is where you can put all the code and data you need to run your job. Your home directory is not accessible to other users, if you need a space to share files with other users, let us know and we can make a shared group directory for you.</p> EngagingSatoriSuperCloudOpenMind <p>The path to your home directory on Satori is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on Satori is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on SuperCloud is <code>/home/gridsan/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on OpenMind is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>Anytime after you start typing a Linux command you can press the \"Tab\" button your your keyboard. This called tab-complete, and will try to autocomplete what you are typing. This is particularly helpful when typing out long directory paths and file names. Pressing \"Tab\" once will complete if there is a single completion, pressing it twice will list all potential completions. It is a bit difficult to explain in text, but you can try it out yourself and watch the short demonstration here.</p> <p>Finally, click on the box below for a list of Linux Commands. If you are new to Linux try them out for yourself at the command line.</p> Common Linux Commands <ul> <li>Creating, navigating and viewing directories:<ul> <li><code>pwd</code>: tells you the full path of the directory you are     currently in</li> <li><code>mkdir dirname</code>: creates a directory with the name \"dirname\"</li> <li><code>cd dirname</code>: change directory to directory \"dirname\"<ul> <li><code>cd ../</code>: takes you up one level</li> </ul> </li> <li><code>ls</code>: lists the files in the directory<ul> <li><code>ls -a</code>: lists all files including hidden files</li> <li><code>ls -l</code>: lists files in \"long format\" including ownership     and date of last update</li> <li><code>ls -t</code>: lists files by date stamp, most recently updated     file first</li> <li><code>ls -tr</code>: lists files by dates stamp in reverse order, most     recently updated file is listed last (this is useful if you     have a lot of files, you want to know which file you changed     last and the list of files results in a scrolling window)</li> <li><code>ls dirname</code>: lists the files in the directory \"dirname\"</li> </ul> </li> </ul> </li> <li>Viewing files<ul> <li><code>more filename</code>: shows the first part of a file, hitting the     space bar allows you to scroll through the rest of the file, q     will cause you to exit out of the file.</li> <li><code>less filename</code>: allows you to scroll through the file, forward     and backward, using the arrow keys.</li> <li><code>tail filename</code>: shows the last 10 lines of a file (useful when     you are monitoring a log file or output file to see that the     values are correct)<ul> <li>t<code>ail &lt;number&gt; filename</code>: show you the last &lt;number&gt;     lines of a file.</li> <li><code>tail -f filename</code>: shows you new lines as they are written     to the end of the file. Press CMD+C or Control+C to exit.     This is helpful to monitor the log file of a batch job.</li> </ul> </li> </ul> </li> <li>Copying, moving, renaming, and deleting files<ul> <li><code>mv filename dirname</code>: moves filename to directory dirname.<ul> <li><code>mv filename1 filename2</code>: moves filename1 to filename2, in     essence renames the file. The date and time are not changed     by the mv command.</li> </ul> </li> <li><code>cp filename dirname</code>: copies to directory dirname.<ul> <li><code>cp filename1 filename2</code>: copies filename1 to filename2. The     date stamp on filename2 will be the date/time that the file     was moved</li> <li><code>cp -r dirname1 dirname2</code>: copies directory dirname1 and its     contents to dirname2.</li> </ul> </li> <li><code>rm filename</code>: removes (deletes) the file</li> </ul> </li> </ul>","tags":["Getting Started","Linux"]},{"location":"getting-started/#transferring-files","title":"Transferring Files","text":"<p>One of the first tasks is to get your code, data, and any other files you need into your home directory on the system. If your code is in github you can use git commands on the system to clone your repository to your home directory. You can also transfer your files to your home directory from your computer by using the commands <code>scp</code> or <code>rsync</code>. Read the page on Transferring Files for the system you are using to learn how to use these commands and transfer what you need to your home directory.</p> <p>You can use <code>scp</code> or <code>rsync</code> from the command line on your local computer for any ORCD system. Both commands work similarly to the <code>cp</code> command, following the pattern <code>&lt;command&gt; &lt;source&gt; &lt;destination&gt;</code>, the only difference being that you will need to include the hostname of the system you are transferring to or from. For this reason you must run this command from the terminal on your computer before you've logged in.</p> <p>To transfer a file from your computer to the ORCD system:</p> EngagingSatoriSuperCloudOpenMind <pre><code>scp &lt;file-name&gt; USERNAME@eofe8.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp &lt;local-file-name&gt; USERNAME@satori-login-001:&lt;path-to-satori-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp &lt;local-file-name&gt; USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; &lt;user&gt;@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <p>To transfer a file from an ORCD system to your computer:</p> EngagingSatoriSuperCloudOpenMind <pre><code>scp USERNAME@eofe8.mit.edu:&lt;path-to-engaging-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp USERNAME@satori-login-001:&lt;path-to-satori-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <pre><code>scp &lt;user&gt;@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>Similar to <code>cp</code>, use the <code>-r</code> flag to copy over an entire directory and its contents. </p> EngagingSatoriSuperCloudOpenMind <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@eofe8.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@satori-login-001:&lt;path-to-satori-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-dir-name&gt; &lt;user&gt;@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <p>The <code>rsync</code> command can be used similarly and has some additional flags you can use. It also can be used to transfer only new or modified files to the destination, which makes it easy to keep a directory in \"sync\".</p> <p>For more information on transferring files and additional methods please see the documentation page for your system:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging Transferring Files Documentation Page Coming Soon</p> <p>Satori Transferring Files Documentation Page</p> <p>SuperCloud Transferring Files Documentation Page</p> <p>OpenMind Transferring Files Documentation Page</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#running-your-first-job","title":"Running your First Job","text":"<p>At this point you may want to do a test-run of your code. You always want to start small in your test runs, so you should choose a small example that tests the functionality of what you would ultimately like to run on the system. If your test code is serial and runs okay on a moderate personal laptop or desktop you can request an interactive session to run your code in by executing the command:</p> EngagingSatoriSuperCloudOpenMind <pre><code># Requesting a single core for an interactive job for 1 hour\nsrun -n 1  -t 01:00:00 --pty /bin/bash\n</code></pre> <pre><code>#Requesting a single core for an interactive job for 1 hour\nsrun -n 1  -t 01:00:00 --pty /bin/bash\n</code></pre> <pre><code># Requesting a single core for an interactive job\nLLsub -i\n</code></pre> <pre><code># Requesting a single core for an interactive job for 1 hour\nsrun -n 1 -t 01:00:00  --pty bash  \n</code></pre> <p>After you run this command you will be on a compute node and you can do a test-run of your code. This command will allocate one core to your job. If your test code is multithreaded or parallel, uses a lot of memory, or requires a GPU you should request additional resources as needed. Not requesting the resources you will be using can negatively impact others on the system.</p> <p>Please see your system's documentation pages for more information on requesting more resources for running interactive jobs, and how to run batch jobs.</p> EngagingSatoriSuperCloudOpenMind <p>Engaging's Documentation for Running Jobs</p> <p>Satori's Documentation for Running Jobs</p> <p>SuperCloud's Documentation for Running Jobs</p> <p>OpenMind's Documentation for Running Jobs</p>","tags":["Getting Started","Linux"]},{"location":"orcd-systems/","title":"ORCD Systems","text":"<p>ORCD operates and provides support and training for a number of cluster computer systems available to all researchers. These systems all run with a Slurm scheduler and most have a web portal for interactive computing. These are Engaging, SuperCloud, Satori, and OpenMind.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#maintenance-schedule","title":"Maintenance Schedule","text":"<p>With the exception of SuperCloud, the maintenance schedule for all ORCD systems is:</p> <ul> <li>Monthly downtimes on the 3rd Tuesday of the month lasting about a day.</li> <li>Weekly restarts of login nodes Monday mornings starting at 7am for about 15 minutes. If Monday is a holiday this restart will occur on Tuesday.</li> </ul> <p>SuperCloud has monthly downtimes on the 2nd Tuesday of each month.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-cluster","title":"Engaging Cluster","text":"<p>The Engaging cluster is a mixed CPU and GPU computing cluster that is openly available to all  research projects at MIT. It has around 80,000 x86 CPU cores and 300  GPU cards ranging from K80 generation to recent Voltas. Hardware access is through the Slurm  resource scheduler that supports batch and interactive workloads and allows dedicated reservations. The cluster has a large shared file system for working datasets. Additional compute and storage  resources can be purchased by PIs. A wide range of standard software is available and the Docker  compatible Singularity container tool is supported. User-level tools like Anaconda for Python,  R libraries, and Julia packages are all supported. A range of PI group maintained custom software  stacks are also available through the widely adopted environment modules toolkit. A standard,  open-source, web-based portal supporting Jupyter notebooks, R studio, Mathematica, and X graphics  is available at https://engaging-ood.mit.edu. Further information and support is available from orcd-help-engaging@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-engaging","title":"How to Get an Account on Engaging","text":"<p>Accounts on the engaging cluster are connected to your main MIT institutional kerberos id.  Connecting to the cluster for the first time through its web portal automatically activates an account with basic access to resources. See this page for instructions on how to log in.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-quick-links","title":"Engaging Quick Links","text":"<ul> <li>Documentation: https://engaging-web.mit.edu/eofe-wiki/</li> <li>OnDemand web portal: https://engaging-ood.mit.edu</li> <li>Help: Send email to orcd-help-engaging@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori","title":"Satori","text":"<p>Satori is an IBM Power 9 large memory node system. It is open to everyone on campus and has  optimized software stacks for machine learning and for image stack post-processing for  MIT.nano Cryo-EM facilities. The system has 256 NVidia Volta GPU cards attached in groups of  four to 1TB memory nodes and a total of 2560 Power 9 CPU cores. Hardware access is through the  Slurm resource scheduler that supports batch and interactive workloads and allows dedicated  reservations. A wide range of standard software is available and the Docker compatible  Singularity container tool is supported. A standard web based portal  https://satori-portal.mit.edu with Jupyter notebook support is available. Additional compute and storage resources can be purchased by PIs and integrated into the system. Further  information and support is available at orcd-help-satori@mit.edu</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-satori","title":"How to Get an Account on Satori","text":"<p>You can get an account by logging into https://satori-portal.mit.edu with your MIT credentials. This automatically activates an account with basic access to resources. See this page for more information.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori-quick-links","title":"Satori Quick Links","text":"<ul> <li>Documentation: https://mit-satori.github.io/</li> <li>OnDemand web portal: https://satori-portal.mit.edu</li> <li>Help: Send email to orcd-help-satori@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud","title":"SuperCloud","text":"<p>The SuperCloud system is a collaboration with MIT Lincoln Laboratory on a shared facility that  is optimized for streamlining open research collaborations with Lincoln Laboratory. The facility  is open to everyone on campus. The latest SuperCloud system has more than 16,000 x86 CPU cores  and more than 850 NVidia Volta GPUs in total. Hardware access is through the Slurm resource  scheduler that supports batch and interactive workloads and allows dedicated reservations. A wide  range of standard software is available and the Docker compatible Singularity container tool is  supported. User-level tools like Anaconda for Python, R libraries, and Julia packages are all supported. A custom, web-based portal supporting Jupyter notebooks is available at https://txe1-portal.mit.edu/. Further information and support is available at supercloud@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-supercloud","title":"How to Get an Account on SuperCloud","text":"<p>To request a SuperCloud account follow the instructions on SuperCloud's Requesting an Account page.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud-quick-links","title":"SuperCloud Quick Links","text":"<ul> <li>Documentation: https://supercloud.mit.edu/</li> <li>Online Course: https://learn.llx.edly.io/course/practical-hpc/</li> <li>Web portal: https://txe1-portal.mit.edu/</li> <li>Help: Send email to supercloud@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#openmind","title":"OpenMind","text":"<p>The OpenMind system is a collaboration with Department of Brain and Cognitive Sciences (BCS) and McGovern Institute. OpenMind is mainly a GPU computing cluster optimized for artificial intelligence (AI) research and data science. Totally there are around 70 compute nodes, 3500 CPU cores, 48 TB of RAM, and 340 GPUs, including 142 A100-80GB GPUs. It also provides around 2 PB of flash storage supporting fast read/write data speed. Hardware access is through the Slurm resource scheduler that supports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and Docker compatible Apptainer/Singularity container tool is supported. User-level tools like Anaconda for Python, R libraries, and Julia packages are all supported. Further information and support is available at orcd-help-openmind@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-openmind","title":"How to Get an Account on OpenMind","text":"<p>Accounts will be available for MIT users in 2024.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#openmind-quick-links","title":"OpenMind Quick Links","text":"<ul> <li>Documentation: https://github.mit.edu/MGHPCC/OpenMind/wiki</li> <li>Home Page and Online Course: https://openmind.mit.edu/</li> <li>Help: Send email to orcd-help-openmind@mit.edu</li> <li>Slack: https://openmind-46.slack.com</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"filesystems-file-transfer/filesystems/","title":"General Use Filesystems","text":"<p>Large HPC systems often have different filesystems for different purposes. ORCD systems are no different, and each have their own approach. This page documents these.</p>"},{"location":"filesystems-file-transfer/filesystems/#engaging","title":"Engaging","text":"<p>Users each get a small home directory that is backed up and meant for important files. Larger scratch space is not backed up. Additional storage can be purchased. The Lustre scratch space will be faster than NFS for the majority of workloads, however having large numbers of small files will make it slower than NFS and can slow down the filesystem overall, so it is important to follow the Lustre Best Practices. See the Engaging Documentation Page on Storage for more information.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory  NFS <code>/home/&lt;username&gt;</code> 100 GB Backed up Use for important files Lustre <code>/nobackup1/&lt;username&gt;</code> 1 TB Not backed up Scratch space  Faster than NFS NFS <code>/pool001/&lt;username&gt;</code> 1 TB Not backed up Scratch space"},{"location":"filesystems-file-transfer/filesystems/#satori","title":"Satori","text":"Storage Type Path Quota Backed up Purpose/Notes Home Directory <code>/home/&lt;username&gt;</code> 25GB Backed up Use for important files. Quota increase request to 100GB. GPFS <code>/nobackup/users/&lt;username&gt;</code> 500GB Not backed up Scratch space. Quota increase request to 2TB."},{"location":"filesystems-file-transfer/filesystems/#supercloud","title":"SuperCloud","text":"<p>SuperCloud uses Lustre for all central/shared storage (accessible to all nodes in the system). This storage is not backed up. See the SuperCloud Best Practices and Performance Tips page for best practices using the Lustre filesystem. Quotas or limits are set on the storage as guardrails. Individual and group storage use and quotas can been viewed on the User Profile Page on the SuperCloud Web Portal (only accessible if you have an account). Additional storage may be granted on a case by case basis. Local disk spaces will be faster than the Lustre shared filesystem, but all are temporary and can only be accessed on the node where they are created.</p> Storage Type Path Access Backed up Limits Home Directory  Lustre <code>/home/gridsan/&lt;username&gt;</code> User only Not backed up See User Profile Page Group Directories  Lustre <code>/home/gridsan/groups/&lt;groupname&gt;</code> Files shared within a group Not backed up See User Profile Page Job-specific Temporary Storage  Local Disk Access using the <code>$TMPDIR</code> environment variable User or Group Not backed up   Temporary directory created at the start of a job and cleaned up at the end of the job NA Local Disk Space Create the directory <code>/state/partition1/user/$USER</code> as needed User or Group Not backed up  Cleaned up monthly during downtimes NA"},{"location":"filesystems-file-transfer/filesystems/#openmind","title":"OpenMind","text":"<p>OpenMind provides a number of different storage options. See the OpenMind Documentation page on Storage for more information, best practices, and recommendations.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory <code>/home/&lt;username&gt;</code> 20 GB Backed up Use for very important files. Physically located on Flash 2. Flash 1 <code>/om/user/&lt;username&gt;</code> (individual users) and <code>/om/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Flash 1 Scratch <code>/om/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 3 weeks after creation Scratch space Flash 2 <code>/om2/user/&lt;username&gt;</code> (individual users) and <code>/om2/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Flash 2 Scratch <code>/om2/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 2 weeks after creation Scratch space NFS <code>/om3</code>, <code>/om4</code>, <code>/om5</code> Per group Backed up Slow internal long-term storage NESE <code>/nese</code> Per group Backed up Slow internal long-term storage"},{"location":"filesystems-file-transfer/project-filesystems/","title":"Project Specific Filesystems","text":""},{"location":"filesystems-file-transfer/project-filesystems/#purchasing-storage","title":"Purchasing Storage","text":"<p>Additional project and lab storage can be purchased on ORCD shared clusters by individual PI groups. This storage is mounted on the cluster and access to the storage is managed  by the group through MIT Web Moira (see below for details).</p> <p>The current options for storage are:</p> Storage Type Description Encryption at Rest Backup Namespace Notes Compute Very frequent data access Optional No Limited Very fast access, special needs, high IO Data Frequent data access Optional No Limited Day to day research storage, active projects, instrument data buffers, etc. Archival Infrequent data access Yes No Nearly Unlimited Infrequently accessed data, unlimited namespace <p>Please note that all types of storage are not backed up by default.</p> <p>Storage is charged at the start of each fiscal year. The first year is prorated by the number of months left in the current fiscal year. A purchase must be a minimum of 20 TiB and in increments of 20 TiB.</p> <p>If you anticipate needing more than a few 100 TiB let us know when you request your storage. We may suggest purchasing a dedicated server for your lab.</p> <p>For more information, including pricing, and to purchase storage please send an email to orcd-help@mit.edu. If you are purchasing storage please include the following in your request:</p> <ul> <li>The storage type (compute, data, or archival)</li> <li>Amount in TiB (20 TiB increments)</li> <li>Cost object</li> <li>The lab PI</li> </ul>"},{"location":"filesystems-file-transfer/project-filesystems/#managing-access-using-mit-web-moira","title":"Managing access using MIT Web Moira","text":"<p>Individual group storage is configured so that access is limited to a set of accounts belonging to a web moira list that is defined for the group store. The owner and administrators of group storage can manage access themselves, by modifying the membership of an associated moira list under https://groups.mit.edu/webmoira/list/[list_name]. The name of the list corresponds to the UNIX group name associated with the ORCD shared  cluster storage.</p>"},{"location":"filesystems-file-transfer/project-filesystems/#moira-web-interface-example","title":"Moira Web Interface Example","text":"<p>The figure below shows a screenshot of the web moira management page at https://groups.mit.edu/webmoira/list/cnh_research_computing for a hypothetical storage group named <code>cnh_research_computing</code>. The interface provides a  self-service mechanism for controlling access to any storage belonging to this group. MIT account IDs can be added and  removed as needed from this list by the storage access administrators.</p> <p></p>"},{"location":"filesystems-file-transfer/transferring-files/","title":"Transferring Files","text":"<p>There are a few different ways to transfer files depending on your goals, the data you are transferring, and what you are comfortable with. On this page we cover the different methods of transferring files, as well as touch on how to transfer files between systems.</p> <p>For most of these options you will need to know the hostname of the node where you will be transferring files. This is often a login node, but may also be a dedicated data transfer node. Select the system you are using to see options for the hostname here:</p> EngagingSatoriOpenMindSuperCloud <ul> <li><code>eofe4.mit.edu</code></li> <li><code>eofe9.mit.edu</code></li> <li><code>eofe10.mit.edu</code></li> </ul> <ul> <li><code>satori-login-001.mit.edu</code></li> <li><code>satori-login-002.mit.edu</code></li> </ul> <ul> <li><code>openmind-dtn.mit.edu</code></li> </ul> <ul> <li><code>txe1-login.mit.edu</code></li> </ul> <p>For more information specific to the system you are using, you can consult your system's documentation here:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging does not have an additional documentation page.</p> <p>Satori Transferring Files Documentation Page</p> <p>SuperCloud Transferring Files Documentation Page</p> <p>OpenMind Transferring Files Documentation Page</p>"},{"location":"filesystems-file-transfer/transferring-files/#transferring-files-with-the-command-line","title":"Transferring Files with the Command Line","text":"<p>Two of the most common commands used to transfer files are <code>scp</code> and <code>rsync</code>. You will need to run both of these commands from your local computer, before logging into any ORCD system. In order to use these two command you will need:</p> <ul> <li>The hostname of the remote machine you are transferring files or from (usually the login node)</li> <li>The path on the remote machine where you are copying the file to or from</li> <li>To be able to ssh to the remote machine where you are transferring files to or from</li> </ul> <p>Both <code>scp</code> and <code>rsync</code> work similar to <code>cp</code>, in that you specify a source (where the file is coming from) and destination (where the file is going to).</p> <p>Unless you have your paths memorized, the easiest way to do this is to have two terminals open. One logged into the ORCD system you are transferring files to or from, one not logged in. In each navigate to the respective directories where the file exists or you want to transfer it to. In the local tab navigate to where you want to put the transferred file or to to the file you want to transfer. In the ORCD system tab use the <code>pwd</code> command to print out the path to your current location. You can use this to run the <code>scp</code> or <code>rsync</code> command.</p>"},{"location":"filesystems-file-transfer/transferring-files/#scp","title":"scp","text":"<p>First, open a terminal on your computer (not logged into any ORCD system).</p> <p>To transfer a file from your local computer to an ORCD system you would use the command:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp &lt;file-name&gt; USERNAME@eofe9.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; USERNAME@satori-login-001.mit.edu:&lt;path-to-satori-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; USERNAME@txe1-login.mit.edu:&lt;path-to-supercloud-dir&gt;\n</code></pre> <p>For example, let's say you have the local file <code>myscript.py</code> and you want to transfer it to the directory <code>mycode</code> in your home directory. The command would be:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp myscript.py USERNAME@eofe9.mit.edu:/home/USERNAME/mycode/\n</code></pre> <pre><code>scp  myscript.py USERNAME@satori-login-001.mit.edu:/home/USERNAME/mycode/\n</code></pre> <pre><code>scp  myscript.py USERNAME@openmind-dtn.mit.edu:/home/USERNAME/mycode/\n</code></pre> <pre><code>scp myscript.py USERNAME@txe1-login.mit.edu:/home/gridsan/USERNAME/mycode/\n</code></pre> <p>To transfer the other direction (from an ORCD system to your local computer) switch the order:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp USERNAME@eofe9.mit.edu:&lt;path-to-engaging-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <pre><code>scp USERNAME@satori-login-001.mit.edu:&lt;path-to-satori-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <pre><code>scp USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <pre><code>scp USERNAME@txe1-login.mit.edu:&lt;path-to-supercloud-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <p>If you were to have the file <code>results.csv</code> that you want to copy from the <code>output</code> directory in your home directory to the current directory on your computer the command would be:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp USERNAME@eofe9.mit.edu:/home/USERNAME/output/results.csv .\n</code></pre> <pre><code>scp USERNAME@satori-login-001.mit.edu:/home/USERNAME/output/results.csv .\n</code></pre> <pre><code>scp USERNAME@openmind-dtn.mit.edu:/home/USERNAME/output/results.csv .\n</code></pre> <pre><code>scp USERNAME@txe1-login.mit.edu:/home/gridsan/USERNAME/output/results.csv .\n</code></pre> <p>Note the <code>.</code> in the command above means the current directory.</p> <p>Similar to the <code>cp</code> command, if you want to transfer an entire directory and all of its subdirectories, use the <code>-r</code> (recursive) flag for either direction:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp -r &lt;file-name&gt; USERNAME@eofe9.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-file-name&gt; USERNAME@satori-login-001.mit.edu:&lt;path-to-satori-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-file-name&gt; USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-file-name&gt; USERNAME@txe1-login.mit.edu:&lt;path-to-supercloud-dir&gt;\n</code></pre>"},{"location":"filesystems-file-transfer/transferring-files/#rsync","title":"rsync","text":"<p>The use of <code>rsync</code> is very similar to <code>scp</code>, but the behavior is different. By default <code>rsync</code> will not transfer files that are identical at both the source and destination. There are additional flags you can use to specify what <code>rsync</code> should do when files differ. The <code>rsync</code> command can be very useful when you want to \"sync\" updates to a directory or when transferring large directories. If a transfer fails during <code>rsync</code> you can re-run the command and it will pick up where it left off, rather than re-transfer everything.</p> <p>For general use, the example commands above for <code>scp</code> apply, use the same command but replace <code>scp</code> with <code>rsync</code>.</p> <p>Some useful flags include:</p> <ul> <li><code>-r</code>, <code>--recursive</code> to recursively copy files in all sub-directories</li> <li><code>-l</code>, <code>--links</code> to copy and retain symbolic links</li> <li><code>-u</code>, <code>--update</code> skips any files for which the destination file already exists and has a date later than the source file</li> <li><code>-v</code>, <code>--verbose</code> prints out more information during the file transfer, add more <code>v</code>s for more information</li> <li><code>--partial</code> keeps partially transferred files, useful when transferring large files so rsync can continue where it left off if the transfer fails</li> <li><code>--progress</code> prints information about the progress of the transfer</li> <li><code>-n</code>, <code>--dry-run</code> does not run the transfer but prints out what actions it would be taken, useful to avoid unintended file overwrites</li> </ul> <p>You can run <code>rsync --help</code> to print out a full list of flags that can be used with the <code>rsync</code> command.</p>"},{"location":"filesystems-file-transfer/transferring-files/#moving-files-between-orcd-systems","title":"Moving files between ORCD Systems","text":"<p>If you need to move files between ORCD systems you can do so one of two ways.</p> <ol> <li>ssh to one of the ORCD systems and initiate the transfer from that system to the other. Once you are logged into one system the process is the same as if you were to transfer files to or from your own computer.</li> <li>Run the <code>scp</code> or <code>rsync</code> command on your local system and specify the hostnames and paths for each of the source and destination systems. For example to move a file from Engaging to Satori using <code>scp</code> you would run:</li> </ol> Transferring files from Engaging to Satori<pre><code>scp USERNAME@eofe9.mit.edu:&lt;path-to-engaging-file&gt; USERNAME@satori-login-001.mit.edu:&lt;path-to-satori-dir&gt;\n</code></pre>"},{"location":"filesystems-file-transfer/transferring-files/#graphical-applications-for-file-transfer","title":"Graphical Applications for File Transfer","text":"<p>There are a few applications you can download that will allow you to transfer files with  drag-and-drop, similar to how you would move files around on your own computer.</p> <p>Some of the most common options are:</p> <ul> <li>Cyberduck (Mac and Windows)</li> <li>FileZilla (Mac, Windows, and Linux)</li> <li>WinSCP (Windows only)</li> </ul> <p>To use these you will need to know the hostname of the ORCD system you are accessing, either one of the login nodes or a dedicated data transfer node. See the list of hostnames at the top of this page to see which you should use for the system you are transferring files to.</p>"},{"location":"filesystems-file-transfer/transferring-files/#transferring-files-with-a-web-portal","title":"Transferring Files with a Web Portal","text":"<p>Most ORCD systems have some form of portal that can be accessed through your browser and used to transfer or download files. Engaging and Satori both use OnDemand. SuperCloud has its own custom portal.</p> <ul> <li>Engaging OnDemand Portal</li> <li>Satori OnDemand Portal</li> <li>SuperCloud Web Portal (Documentation)</li> </ul> <p>For documentation on how to download and transfer files on the SuperCloud Web Portal, see the link above.</p> <p>If you are using Engaging or Satori, you can use the file browser by selecting Files -&gt; Home Directory in the menu bar at the top of the page. You can drag and drop files into and out of this page or use the \"Upload\" and \"Download\" buttons. Select multiple files by holding the Control (or Command) key and clicking on the files you'd like to select. Those files can then be downloaded with the \"Download\" button.</p>"},{"location":"images/","title":"Index","text":""},{"location":"images/#directory-of-static-images","title":"Directory of static images","text":""},{"location":"recipes/gromacs/","title":"Installing and Using GROMACS","text":"<p>GROMACS is a free and open-source software suite for high-performance molecular dynamics and output analysis.</p> <p>You can learn about GROMACS here: https://www.gromacs.org/.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#install-gromacs-with-mpi","title":"Install GROMACS with MPI","text":"EngagingSuperCloud <p>Select a version on the GROMACS website, then download and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget --no-check-certificate http://ftp.gromacs.org/pub/gromacs/gromacs-2019.6.tar.gz\ntar xvfz gromacs-2019.6.tar.gz\n</code></pre></p> <p>Load MPI and Cmake modules, <pre><code>module load engaging/openmpi/2.0.3 cmake/3.17.3\n</code></pre></p> <p>Create build and install directories, <pre><code>mkdir -p 2019.6/build\nmkdir 2019.6/install\ncd 2019.6/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2019.6 -DGMX_MPI=ON -DCMAKE_INSTALL_PREFIX=~/gromacs/2019.6/install\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage environment, <pre><code>source ~/gromacs/2019.6/install/bin/GMXRC\n</code></pre></p> <p>Select a version on the GROMACS website, then download and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget http://ftp.gromacs.org/pub/gromacs/gromacs-2023.2.tar.gz\ntar xvfz gromacs-2023.2.tar.gz\n</code></pre></p> <p>Load CUDA, Anaconda and MPI modules, <pre><code>module load cuda/11.8 anaconda/2023a\nmodule load mpi/openmpi-4.1.5\n</code></pre></p> <p>Create build and install directories, <pre><code>mkdir -p 2023.2/build\nmkdir 2023.2/install\ncd 2023.2/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2023.2 -DGMX_MPI=ON -DGMX_GPU=CUDA -DCMAKE_INSTALL_PREFIX=~/gromacs/2023.2-gpu\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage environment before running GROMACS programs, <pre><code>source ~/gromacs/2023.2/install/bin/GMXRC\n</code></pre></p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#run-gromacs","title":"Run GROMACS","text":"<p>Firstly, prepare for an input file. Refer to file formats. Here shows an example with an input file named <code>benchPEP-h.tpr</code> downloaded from this page. In these examples we have saved the input files in the <code>~/gromacs/bench/</code> directory.</p> <p>Secondly, create a batch job script, for example, named <code>job.sh</code>.</p> EngagingSuperCloud <p>This job script requests 2 nodes with a total of 8 CPU cores and 50GB of memory.</p> job.sh<pre><code>#!/bin/bash\n#SBATCH --job-name=\"production run\"\n#SBATCH --partition=sched_mit_hill\n#SBATCH --constraint=centos7\n#SBATCH --mem=50G\n#SBATCH -N 2\n#SBATCH --ntasks 8\n#SBATCH --time=12:00:00\n\n\nmodule purge\nmodule load gromacs/2018.4\n\ngmx_mpi=/home/software/gromacs/2018.4/bin/gmx_mpi\n\nif [ -n \"$SLURM_CPUS_PER_TASK\" ]; then\n    ntomp=\"$SLURM_CPUS_PER_TASK\"\nelse\n    ntomp=\"1\"\nfi\n\n\n# setting OMP_NUM_THREADS to the value used for ntomp to avoid complaints from gromacs\nexport OMP_NUM_THREADS=$ntomp\n\nmpirun -np $SLURM_NTASKS $gmx_mpi mdrun -ntomp $ntomp -deffnm ~/gromacs/bench/benchPEP-h -v\n</code></pre> <p>This job requests 2 nodes with 4 CPU cores and 2 GPUs per node.</p> job.sh<pre><code>#!/bin/bash\n#SBATCH --nodes=2              # 2 nodes\n#SBATCH --ntasks-per-node=2    # 2 MPI tasks per node\n#SBATCH --cpus-per-task=2      # 2 CPU cores per task\n#SBATCH --gres=gpu:volta:2     # 2 GPUs per node\n#SBATCH --time=01:00:00        # 1 hour\n\n\n# Load required modules\nmodule load cuda/11.8 mpi/openmpi-4.1.5\n\n# Enable direct GPU to GPU communications\nexport GMX_ENABLE_DIRECT_GPU_COMM=true\n\n# Activate user install of GROMACS\nsource ~/gromacs/2023.2-gpu/bin/GMXRC\n\n# Check MPI, GPU and GROMACS\nmpirun hostname\nnvidia-smi\nwhich gmx_mpi\n\n# Run GROMACS\nmpirun gmx_mpi mdrun -s ~/gromacs/bench/benchPEP-h.tpr -ntomp ${SLURM_CPUS_PER_TASK} -pme gpu -update gpu -bonded gpu -npme 1\n</code></pre> <p>Finally, submit the job, <pre><code>sbatch job.sh\n</code></pre></p> <p>Refer to GROMACS user guide for more info.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/h100_getting_started/","title":"Getting started on 8-way H100 nodes on Satori","text":"<p>This page provides information on how to run a first job on the IBM Watson AI Lab H100 GPU nodes on Satori. The page describes how to request an access to the Slurm partition associated  with the H100 nodes and how to run a first example pytorch script on the systems. </p> <p>A first set of H100 GPU systems has been added to Satori. These are for priority use by IBM Watson AI Lab research collaborators. They are also available for general opportunistic use when they are idle.</p> <p>Currently ( 2023-06-19 ) there are 4 H100 systems installed.  Each system has 8 H100 GPU cards, two Intel 8468 CPUs each with 48 physical cores and 1TiB of main memory.</p> <p>Below are some instructions for getting started with these systems. </p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#access-to-the-nodes","title":"Access to the nodes","text":"<p>To access the nodes in the priority group you need your satori login id to be listed in the WebMoira  group https://groups.mit.edu/webmoira/list/sched_oliva.  Either Alex Andonian and Vincent Sitzmann are able to add accounts to the <code>sched_oliva</code> Moira list.</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#interactive-access-through-slurm","title":"Interactive access through Slurm","text":"<p>To access an entire node through Slurm, the command below can be used from the satori login node</p> <pre><code>srun -p sched_oliva --gres=gpu:8 -N 1 --mem=0 -c 192 --time 1:00:00 --pty /bin/bash\n</code></pre> <p>this command will launch an interactive shell on one of the nodes (when a full node becomes available).  From this shell the NVidia status command  <pre><code>nvidia-smi\n</code></pre> should list 8 H100 GPUs as available. </p> <p>Single node, multi-gpu training examples (for example https://github.com/artidoro/qlora ) should run  on all 8 GPUs. </p> <p>To use a single GPU interactively the following command can be used <pre><code>srun -p sched_oliva --gres=gpu:1 --mem=128 -c 24 --time 1:00:00 --pty /bin/bash\n</code></pre></p> <p>this will request a single GPU. This request will allow other Slurm sessions to run on other GPUs  simultaneously with this session.</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-nightly-build-pytorch-example-with-a-fresh-miniconda-and-pytorch","title":"Running a nightly build pytorch example with a fresh miniconda and pytorch","text":"<p>A miniconda environment can be used to run the latest nightly build pytorch code on these  systems. To do this, first create a software install directory and install the needed pytorch software</p> <pre><code>mkdir -p /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre> <p>and then switch your shell to that directory. <pre><code>cd /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre></p> <p>now install miniconda and create an environment with the needed software <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \nchmod +x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh -b -p minic\n. ./minic/bin/activate \nconda create -y -n pytorch_test python=3.10\nconda activate pytorch_test                          \nconda install -y -c conda-forge cupy\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n</code></pre></p> <p>Once the software is installed, the following script can be used to test the installation. test.py<pre><code>import torch\ndevice_id = torch.cuda.current_device()\ngpu_properties = torch.cuda.get_device_properties(device_id)\nprint(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n          \"%.1fGb total memory.\\n\" % \n          (torch.cuda.device_count(),\n          device_id,\n          gpu_properties.name,\n          gpu_properties.major,\n          gpu_properties.minor,\n          gpu_properties.total_memory / 1e9))\n</code></pre> Run this script with <pre><code>python test.py\n</code></pre></p> <p>To exit the Slurm srun session enter the command <pre><code>exit\n</code></pre></p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-simple-batch-script-using-an-installed-miniconda-environment","title":"Running a simple batch script using an installed miniconda environment","text":"<p>To run a batch script on one of the H100 nodes in partition <code>sched_oliva</code> first paste the content in the  box below into a slurm script file called, for example, <code>test_script.slurm</code> ( change the <code>RUNDIR</code> setting to assign the  path to a directory where you have already installed a conda environment in a sub-directory called <code>minic</code> ).</p> <p>First create the <code>mytest.py</code> script with the contents above.</p> <pre><code>#!/bin/bash\n#\n#SBATCH --gres=gpu:8\n#SBATCH --partition=sched_oliva\n#SBATCH --time=1:00:00\n#SBATCH --mem=0\n#\n\nnvidia-smi\n\nRUNDIR=/nobackup/users/${USER}/h100-testing/minic\ncd ${RUNDIR}\n\n. ./minic/bin/activate\n\nconda activate pytorch_test\n\npython mytest.py\n</code></pre> <p>This script can then be submitted to Slurm to run in a background batch node using the command.</p> <pre><code>sbatch &lt; test_script.slurm\n</code></pre>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#getting-help","title":"Getting help","text":"<p>As always, please feel welcome to email orcd-help@mit.edu with questions, comments or suggestions. We would be happy to hear from you!</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/mpi/","title":"Message Passing Interface (MPI)","text":"<p>Message Passing Interface (MPI) is a standard designed for data communication in parallel computing. The MPI standard defines useful library functions/routines in C, C++, and Fortran. Python interface is available for MPI.   </p> <p>There are several MPI implementationos, such as <code>OpenMPI</code>, <code>MPICH</code>, <code>MVAPICH</code>, and <code>Intel MPI</code>, which work with Infiniband network for high-bandwidth data communication.</p> Engaging","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#mpi-modules","title":"MPI modules","text":"<p>There are OpenMPI modules available on the cluster. Before building or runrning your MPI programs, load the modules of a <code>gcc</code> compiler and an <code>openmpi</code> libraries to set up environment varialbes.</p> <p>There are two different operations systems (OS) on the cluster: CentOS 7 and Rocky 8. For CentOS 7 nodes, load these modules, <pre><code>module load gcc/6.2.0 openmpi/3.0.4\n</code></pre> or <pre><code>module load gcc/9.3.0 openmpi/4.0.5\n</code></pre> For Rocky 8 nodes, load these modules, <pre><code>module use /orcd/software/community/001/modulefiles/rocky8\nmodule load gcc/12.2.0 openmpi/4.1.4-pmi-ucx-x86_64\n</code></pre> If you need to run MPI programs with GPUs, load these modules instead, <pre><code>module load gcc/12.2.0 openmpi/4.1.4-pmi-cuda-ucx-x86_64\n</code></pre></p> <p>Note</p> <p>Load a <code>gcc</code> module first, then the openmpi modules built with this <code>gcc</code> will be shown in the output of <code>module avail</code> and can be loaded. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#build-mpi-programs","title":"Build MPI programs","text":"<p>This section will be focused on building MPI programs in C or Fortran. Python users can refer to this page for using the <code>mpi4py</code> package.</p> <p>Most MPI software should be built from source codes. First, download the package from the internet. Load one of the OpenMPI modules mentioned above. A typical building process is like this, <pre><code>./configure CC=mpicc CXX=mpicxx --prefix=&lt;/path/to/your/installation&gt;\nmake\nmake install\n</code></pre> Create an install directory and assign its full path to the flag <code>--prefix</code>. This is where the binaries will be saved.</p> <p>Widely-used MPI software includes <code>Gromacs</code>, <code>Lammps</code>, <code>NWchem</code>, <code>OpenFOAM</code> and many others. The building process of every software is different. Refer to its official installation guide for details.</p> Side note: MPI binaries <p>Some MPI software are provided with prebuilt binaries only. In this case, download the binaries compatible with the <code>linux</code> OS and the <code>x86_64</code> CPU architecture. If possible, try to choose an OpenMPI version, that the binary was built with, as close as possible to that of a module on the cluster. This type of MPI software includes <code>ORCA</code>. </p> <p>Spack is a popular tool to build many software packages systematically on clusters. It makes building processes convenient in many cases. If you want to use Spack to build your software package on the cluster, refer to the recipe page for Spack.</p> <p>If you develop your MPI codes, the codes can be compiled and linked like this <pre><code>mpicc -O3 name.c -o my_program\n</code></pre> or <pre><code>mpif90 -O3 name.f90 -o my_program\n</code></pre> This will create an executable file named <code>my_program</code>. Prepare a GNU Makefile to build programs with multiple source files. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#mpi-jobs","title":"MPI jobs","text":"<p>MPI programs are suitable to run on multiple CPU cores of a single node or on multiple nodes. </p> <p>Here is an example script (e.g. named <code>job.sh</code>) to run an MPI job using multiple cores on a single node.  <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 8\n#SBATCH --mem=10GB   \n\nsrun hostname\nmodule load gcc/6.2.0 openmpi/3.0.4\nmpirun -n $SLURM_NTASKS my_program\n</code></pre></p> <p>This job requests 8 cores (with <code>-n</code>) and 10 GB of memory (with <code>--mem</code>) on 1 node (with <code>-N</code>) for 30 minutes (with <code>-t 30</code>). The <code>-n</code> flag is the same as <code>--ntasks</code>. The specified value is saved to the variable <code>SLURM_NTASKS</code>. In this case, the requested number of cores is equal to <code>SLURM_NTASKS</code>. The command <code>mpirun -n $SLURM_NTASKS</code> ensures that each MPI task runs on one core. </p> <p>The command <code>srun hostname</code> is to check if the correct number of cores and nodes are assigned to the job. It is not needed in production runs. </p> Side note: partitions and modules <p>The modules used in the above example is for the CentOS 7 OS, which works for these partitions: <code>sched_mit_hill</code>, <code>newnodes</code>, and <code>sched_any</code>. If using a partition with the Rocky 8 OS, such as <code>sched_mit_orcd</code>, change the modules accrodingly (see the first session). These are public partitions that are avaiable to most users. Many labs have partitions for their purchased nodes. </p> <p>Submit the job with the <code>sbatch</code> command, <pre><code>sbatch job.sh\n</code></pre></p> <p>To run an MPI job on multiple nodes, refer to the following exmaple script. <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=8\n#SBATCH --mem=10GB\n\nsrun hostname\nmodule load gcc/6.2.0 openmpi/3.0.4\nmpirun -n $SLURM_NTASKS my_program\n</code></pre></p> <p>This job requests 2 nodes with 8 cores and 10 GB of memory per node. In this case, the total number of cores (saved to <code>SLURM_NTASKS</code>) is equal to the number of nodes (saved to <code>SLURM_NNODES</code>) times the number of cores per node (saved to <code>SLURM_NTASKS_PER_NODE</code>). The command <code>mpirun -n $SLURM_NTASKS</code> ensures that MPI tasks are distributed to both nodes and each MPI task runs on one core. </p> <p>Alternatively, users can specify the number of cores per node using an OpenMPI option like this <code>mpirun -npernode $SLURM_NTASKS_PER_NODE my_program</code>.</p> <p>If replacing <code>--ntasks-per-node=8</code> with <code>-n 16</code> in the above script, the job will request 16 cores on 2 nodes, but it is not always the case that there are 8 cores per node. For example, there may be 7 cores on one node and 9 cores on another, or 1 core on one node and 15 cores on another, etc, depending on the current available resources on the cluster. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#computing-resources-for-mpi-jobs","title":"Computing resources for MPI jobs","text":"<p>To get a better idea on how many nodes, cores and memory should be requested, users need to consider the following two questions. </p> <p>First, what resources are available on the cluster? Use this command to check node and job info on the cluster, including the constraint associated with OS (<code>%f</code>), the nubmer of CPU cores (<code>%c</code>), the memory size (<code>%m</code>), the wall time limit (<code>%l</code>), and the current usage status (<code>%t</code>).  <pre><code> sinfo -N -p sched_mit_hill,newnodes,sched_any,sched_mit_orcd -o %f,%c,%m,%l,%t |grep -v drain\n</code></pre> On typical nodes of the cluster, the number of cores per node varies from 16 to 128, and the memory per node varies from 63 GB to 515 GB.</p> <p>To obtain a good performance of MPI programs, it is recommended to request all physical CPU cores and memory on each node. For example, request two nodes with 16 physical cores per node and all of the memory like this, <pre><code>#SBATCH -N 2\n#SBATCH --ntasks-per-node=16\n#SBATCH --mem=0\n</code></pre></p> <p>As MPI is a distributed-memory parallelism, sometimes it is good to use the <code>--mem-per-core</code> flag assigning a certain amount of memory to each core. The total memory is increased with the number of cores in this case. Double check that the total memory fits in the maximum memory of a node to avoid failed jobs.</p> <p>Second, what is the speedup of your MPI program? According to Amdahl's law, well-performing MPI programs are usually speeded up almost linearly as the number of cores is increased until it is saturated at some point. In practice, try to run testing cases investigating the speedup of your program, and then decide how many cores are needed to speed it up efficiently. Do not increase the number of cores when speedup is poor. </p> <p>Note</p> <p>After a job starts to run, execute the command <code>squeue -u $USER</code> to check which node the job is running on, and then log in to the node with <code>ssh &lt;hostname&gt;</code> and execute the <code>top</code> command to check how many CPU cores are being used by the program and what the CPU efficiency is. The efficiency may vary with the number of CPU cores. Try to keep your jobs at a high efficiency. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#hybrid-mpi-and-multithreading-jobs","title":"Hybrid MPI and multithreading jobs","text":"<p>MPI programs are based on distributed-memory parallelism, that says, each MPI task owns a faction of data, such as arrays, matrices, or tensors. In contrast to MPI, the multithreading technique is based on a shared-memory parallelism, in which data is shared by multiple threads. A common implementation of the multithreading technique is OpenMP. For Python users, the <code>numpy</code> package is based on C libraries, such as Openblas, which are usually built with OpenMP. </p> Side note: OpenMP <p>OpenMP is an abbreviation of Open Multi-Processing. It is not related to OpenMPI.</p> <p>Some programs are designed in a hybrid scheme such that MPI and OpenMP are combined to enable two-level parallelization. Set MPI tasks and OpenMP threads in hybrid programs based on the following equation, <pre><code>(Number of MPI Tasks) * (Nubmer of Threads) = Total Number of Cores          (1)\n</code></pre></p> Side note: hyperthreads <p>There are two or multiple hyperthreads on each CPU core in modern CPU architecture. The hyperthread technique is turned off for most nodes of this cluster, but it may be turned on for some nodes as requested by the owner labs. In the case that there are two hyerthreads per physical core, the right side of the equation should be <code>2 * (Total Number of Cores)</code> instead.</p> <p>One way to run hybrid progmrams in Slurm jobs is to use the <code>-n</code> flag for the number of MPI tasks and the <code>-c</code> flag for the number of threads. The follwing is a job script that runs a program with 2 MPI tasks and 8 threads per task on a node with 16 cores. <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 2\n#SBATCH -c 8\n#SBATCH --mem=10GB\n\nmodule load gcc/6.2.0 openmpi/3.0.4\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nmpirun -n $SLURM_NTASKS my_program\n</code></pre> The <code>-c</code> flag is the same as <code>--cpus-per-task</code>. The specified value is saved in the variable <code>SLURM_CPUS_PER_TASK</code>. In this case, the total number of cores equals <code>SLURM_NTASKS * SLURM_CPUS_PER_TASK</code>, that is 16. </p> <p>The built-in environment variable <code>OMP_NUM_THREADS</code> is used to set the number of threads for an OpenMP program. Here it is equal to <code>SLURM_CPUS_PER_TASK</code>. The number of MPI tasks is set to be <code>SLURM_NTASK</code> in the <code>mpirun</code> command line, therefore, the nubmer of MPI tasks times the number of threads equals the total number of CPU cores. </p> <p>Users only need to specify the numbers following Slurm flags <code>-n</code> and <code>-c</code>, for example, <code>-n 4 -c 4</code> or <code>-n 8 -c 2</code>, keeping the product unchanged, then the MPI tasks and threads are all set automatically.  </p> <p>Similarly, here is an exmple script to request two nodes,  <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=2\n#SBATCH -c 8\n#SBATCH --mem=0\n\nmodule load gcc/6.2.0 openmpi/3.0.4\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nmpirun -n $SLURM_NTASKS my_program\n</code></pre> In this case, the total number of cores is equal to <code>SLURM_NNODES * SLURM_NTASKS_PER_NODE * SLURM_CPUS_PER_TASK</code>, that is <code>2 * 2 * 8 = 32</code>. The job will run 4 MPI tasks (i.e. 2 tasks per node) and 8 threads per task. Equation (1) is satisfied as <code>4 * 8 = 32</code>. </p> <p>As mentioned in the previous section, it is recommended to run testing cases to determine the values for the flags <code>-N</code>, <code>-n</code> and <code>-c</code> to obtain a better performance.</p> <p>There is another way to submit jobs for hybrid programs, in which the <code>-c</code> flag is not used at all. For example, it also works like this, <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 16\n#SBATCH --mem=10GB\n\nmodule load gcc/6.2.0 openmpi/3.0.4\nexport OMP_NUM_THREADS=8\nMPI_NTASKS=$((SLURM_NTASK / $OMP_NUM_THREADS))\nmpirun -n $MPI_NTASKS my_program\n</code></pre> This job requests 16 CPU cores on 1 node and runs 2 MPI tasks with 8 threads per task, so equation (1) is satisfied as <code>2 * 8 = 16</code>. In this case, users need to set the values for the Slurm flag <code>-n</code> and the variable <code>OMP_NUM_THREADS</code>.</p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi4py/","title":"MPI for Python","text":"<p>MPI for Python (<code>mpi4py</code>) provides Python bindings for the Message Passing Interface (MPI) standard, allowing Python applications to exploit multiple processors on workstations, clusters and supercomputers.</p> <p>You can learn about <code>mpi4py</code> here: https://mpi4py.readthedocs.io/en/stable/.</p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#installation","title":"Installation","text":"OpenMindEngaging <p>The support team has installed <code>mpi4py</code> in an Anaconda module. You can load the module and do not need to install anything,  <pre><code>module load openmind/anaconda/3-2022.05\n</code></pre></p> <p>If you want to use Anaconda in your directory, refer to section 3 on this page to set it up, then install <code>mpi4py</code>,   <pre><code>conda install -c conda-forge mpi4py\n</code></pre></p> <p>First, load an Anaconda module on a CentOS 7 head node (such as eofe7 or eofe8),  <pre><code>module load anaconda3/2023.07\n</code></pre>  then install <code>mpi4py</code>,   <pre><code>conda create -n mpi\nsource activate mpi\nconda install mpi4py\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#example-codes","title":"Example codes","text":"<p>Prepare your Python codes. Example 1: The following is a code for sending and receiving a dictionary. Save it in a file named <code>p2p-send-recv.py</code>. p2p-send-recv.py<pre><code>from mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\nif rank == 0:\n    data = {'a': 7, 'b': 3.14}\n    comm.send(data, dest=1, tag=11)\n    print(rank,data)\nelif rank == 1:\n    data = comm.recv(source=0, tag=11)\n    print(rank,data)\n</code></pre></p> <p>Example 2: The following is a code for sending and receiving an array. Save it in a file named <code>p2p-array.py</code>. p2p-array.py<pre><code>from mpi4py import MPI\nimport numpy\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\n# passing MPI datatypes explicitly\nif rank == 0:\n    data = numpy.arange(1000, dtype='i')\n    comm.Send([data, MPI.INT], dest=1, tag=77)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(1000, dtype='i')\n    comm.Recv([data, MPI.INT], source=0, tag=77)\n    print(rank,data)\n\n# automatic MPI datatype discovery\nif rank == 0:\n    data = numpy.arange(100, dtype=numpy.float64)\n    comm.Send(data, dest=1, tag=13)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(100, dtype=numpy.float64)\n    comm.Recv(data, source=0, tag=13)\n    print(rank,data)\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#submitting-jobs","title":"Submitting jobs","text":"OpenMindEngaging <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>.  p2p-job.sh<pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n\nmodule load openmind/anaconda/3-2022.05\n\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre>  !!! note       If you use Anaconda in your directory, do not load the Anaconda module. </p> <p>Finally submit the job,  <pre><code>sbatch p2p-job.sh\n</code></pre></p> <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>.  p2p-job.sh<pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n\nmodule load anaconda3/2023.07\n\nsource activate mpi\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\n\nsource activate mpi\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre></p> <p>Finally submit the job,  <pre><code>sbatch p2p-job.sh\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py_snippets/","title":"MPI for Python (Snippets)","text":"<p>MPI for Python (<code>mpi4py</code>) provides Python bindings for the Message Passing Interface (MPI) standard, allowing Python applications to exploit multiple processors on workstations, clusters and supercomputers.</p> <p>You can learn about <code>mpi4py</code> here: https://mpi4py.readthedocs.io/en/stable/.</p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py_snippets/#installation","title":"Installation","text":"OpenMindEngaging <p>The support team has installed <code>mpi4py</code> in an Anaconda module. You can load the module and do not need to install anything,  <pre><code>module load openmind/anaconda/3-2022.05\n</code></pre></p> <p>If you want to use Anaconda in your directory, refer to section 3 on this page to set it up, then install <code>mpi4py</code>,   <pre><code>conda install -c conda-forge mpi4py\n</code></pre></p> <p>First, load an Anaconda module on a CentOS 7 head node (such as eofe7 or eofe8),  <pre><code>module load anaconda3/2023.07\n</code></pre>  then install <code>mpi4py</code>,   <pre><code>conda create -n mpi\nsource activate mpi\nconda install mpi4py\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py_snippets/#example-codes","title":"Example codes","text":"<p>Prepare your Python codes. Example 1: The following is a code for sending and receiving a dictionary. Save it in a file named <code>p2p-send-recv.py</code>. p2p-send-recv.py<pre><code>from mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\nif rank == 0:\n    data = {'a': 7, 'b': 3.14}\n    comm.send(data, dest=1, tag=11)\n    print(rank,data)\nelif rank == 1:\n    data = comm.recv(source=0, tag=11)\n    print(rank,data)\n</code></pre></p> <p>Example 2: The following is a code for sending and receiving an array. Save it in a file named <code>p2p-array.py</code>. p2p-array.py<pre><code>from mpi4py import MPI\nimport numpy\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\n# passing MPI datatypes explicitly\nif rank == 0:\n    data = numpy.arange(1000, dtype='i')\n    comm.Send([data, MPI.INT], dest=1, tag=77)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(1000, dtype='i')\n    comm.Recv([data, MPI.INT], source=0, tag=77)\n    print(rank,data)\n\n# automatic MPI datatype discovery\nif rank == 0:\n    data = numpy.arange(100, dtype=numpy.float64)\n    comm.Send(data, dest=1, tag=13)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(100, dtype=numpy.float64)\n    comm.Recv(data, source=0, tag=13)\n    print(rank,data)\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py_snippets/#submitting-jobs","title":"Submitting jobs","text":"OpenMindEngaging <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>.  p2p-job.sh<pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n\nmodule load openmind/anaconda/3-2022.05\n\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre>  !!! note       If you use Anaconda in your directory, do not load the Anaconda module. </p> <p>Finally submit the job,  <pre><code>sbatch p2p-job.sh\n</code></pre></p> <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>.  p2p-job.sh<pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n\nmodule load anaconda3/2023.07\n\nsource activate mpi\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\n\nsource activate mpi\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre></p> <p>Finally submit the job,  <pre><code>sbatch p2p-job.sh\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mujoco/","title":"Installing and Using MuJoCo","text":"<p>MuJoCo is a free and open source physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed.</p> <p>You can learn about MuJoCo here: https://mujoco.org.</p> <p>Whether you are installing on Engaging or SuperCloud, you\u2019ll first have to install the MuJoCo binaries. This process is the same on all systems.</p>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#install-the-mujoco-binaries","title":"Install the MuJoCo Binaries","text":"<p>First, install MuJoCo itself somewhere in your home directory. This is as simple as downloading the MuJoCo binaries, which can be found on their web page. For the release that you want, select the file that ends with \u201clinux-x86_64.tar.gz\u201d, for example for 2.3.0 select mujoco-2.3.0-linux-x86_64.tar.gz. Right click, and copy the link address. Then you can download on one of the login nodes with the \u201cwget\u201d command, and untar:</p> <pre><code>wget https://github.com/deepmind/mujoco/releases/download/2.3.0/mujoco-2.3.0-linux-x86_64.tar.gz\ntar -xzf mujoco-2.3.0-linux-x86_64.tar.gz\n</code></pre> <p>In order for mujoco-py to find the MuJoCo binaries, set the following paths:</p> <pre><code>export MUJOCO_PY_MUJOCO_PATH=$HOME/path/to/mujoco230/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MUJOCO_PY_MUJOCO_PATH/bin\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#install-mujoco-py","title":"Install Mujoco-Py","text":"<p>First, make sure the <code>MUJOCO_PY_MUJOCO_PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables are set pointing to your mujoco installation. You can use the \u201cecho\u201d command to do this:</p> <pre><code>echo MUJOCO_PY_MUJOCO_PATH\necho LD_LIBRARY_PATH\n</code></pre> <p>If any of these are not set properly you can set them as described above (see here for MUJOCO_PY_MUJOCO_PATH, LD_LIBRARY_PATH.</p> EngagingSuperCloud <p>Next load either a Python or Anaconda module. In this example I loaded the latest anaconda3 module (run <code>module avail anaconda</code> to see the current list of available anaconda modules):</p> <pre><code>module load anaconda3/2022.10\n</code></pre> <p>From here on you can follow the standard instructions to install mujoco-py, using the <code>--user</code> flag where appropriate to install in your home directory, or install in an anaconda or virtual environment (do not use the <code>--user</code> flag if you want to install in a conda or virtual environment). Here I am installing in my home directory with <code>--user</code>:</p> <pre><code>pip install --user 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Start up python and import mujoco_py to complete the build process:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>MuJoCo, particularly mujoco-py, can be tricky to install on SuperCloud as it uses file locking during the install and whenever the package is loaded. File locking is disabled on the SuperCloud shared filesystem performance reasons, but is available on the local disk of each node. Therefore, one workaround is to install mujoco-py on the local disk of one of the login nodes and then copy the install to your home directory. To load the package, the install then needs to be copied to the local disk.</p> <p>We\u2019ve found the most success by doing this with a python virtual environment. By using a python virtual environment you can install any additional packages you need with mujoco-py, and they can be used along with packages in our anaconda module, unlike conda environments.</p> <p>Create the virtual environment on the local disk of the login node and install mujoco-py (install the version you would like to use):</p> <pre><code>module load anaconda/2023a\nmkdir /state/partition1/user/$USER\npython -m venv /state/partition1/user/$USER/mujoco_env\nsource /state/partition1/user/$USER/mujoco_env/bin/activate\npip install 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Now install any other packages you need to run your MuJoCo jobs. With virtual environments you won\u2019t see any of the packages you\u2019ve previously installed with <code>pip install --user</code> or what you may have installed in another environment. You should still be able to use any of the packages in the anaconda module you\u2019ve loaded, so no need to install any of those.</p> <pre><code>pip install pkgname1\npip install pkgname2\n</code></pre> <p>Since you are installing into virtual environment, do not use the <code>--user</code> flag.</p> <p>Once you\u2019ve installed the packages you need, start Python and import mujoco_py to finish the build:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>Now that your environment is created, copy it to your home directory for permanent storage.</p> <pre><code>cp -r /state/partition1/user/$USER/mujoco_env $/software/mujoco/\n</code></pre> <p>If you\u2019d like you can run the few example lines listed on install section of the mujoco-py github page to verify the install went through properly:</p> <pre><code>import mujoco_py\nimport os\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#using-mujoco-in-a-job","title":"Using MuJoCo in a Job","text":"<p>To use MuJoCo you\u2019ll need to first load the same Python or Anaconda module you used to install mujoco-py. If you installed it into a conda environment or python virtual environment, load that environment as well. We recommend you do this in your job submission script rather than in your .bashrc or at the command line before you submit the job. This way you know your job is configured properly every time it runs.</p> <p>You can use the following test scripts to test your MuJoCo setup in a job environment, and as a starting point for your own job:</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> EngagingSuperCloud submit_test.sh<pre><code>#!/bin/bash\n\n# Load the same python/anaconda module you used to install mujoco-py\nmodule load python/3.8.3\n\n# Run the script\npython mujoco_test.py\n</code></pre> <p>Now whenever you use mujoco-py the installation will need to be on the local disk of the node(s) where you are running. In your job script you can add a few lines of code that will check whether the environment exists on the local disk, and if not copy it. You can run these lines during an interactive job as well.</p> submit_test.sh<pre><code>#!/bin/bash``\n\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\nif [ ! -d \"$MUJOCO_ENV\" ]; then\n    echo \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\n    mkdir -p /state/partition1/user/$USER\n    cp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\n\nmodule load anaconda/2022a\nsource $MUJOCO_ENV/bin/activate\n\npython mujoco_test.py\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco_snippets/","title":"Installing and Using MuJoCo (Snippets)","text":"<p>MuJoCo is a free and open source physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed.</p> <p>You can learn about MuJoCo here: https://mujoco.org.</p> <p>Whether you are installing on Engaging or SuperCloud, you\u2019ll first have to install the MuJoCo binaries. This process is the same on all systems.</p>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco_snippets/#install-the-mujoco-binaries","title":"Install the MuJoCo Binaries","text":"<p>First, install MuJoCo itself somewhere in your home directory. This is as simple as downloading the MuJoCo binaries, which can be found on their web page. For the release that you want, select the file that ends with \u201clinux-x86_64.tar.gz\u201d, for example for 2.3.0 select mujoco-2.3.0-linux-x86_64.tar.gz. Right click, and copy the link address. Then you can download on one of the login nodes with the \u201cwget\u201d command, and untar:</p> <pre><code>wget https://github.com/deepmind/mujoco/releases/download/2.3.0/mujoco-2.3.0-linux-x86_64.tar.gz\ntar -xzf mujoco-2.3.0-linux-x86_64.tar.gz\n</code></pre> <p>In order for mujoco-py to find the MuJoCo binaries, set the following paths:</p> <pre><code>export MUJOCO_PY_MUJOCO_PATH=$HOME/path/to/mujoco230/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MUJOCO_PY_MUJOCO_PATH/bin\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco_snippets/#install-mujoco-py","title":"Install Mujoco-Py","text":"<p>First, make sure the <code>MUJOCO_PY_MUJOCO_PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables are set pointing to your mujoco installation. You can use the \u201cecho\u201d command to do this:</p> <pre><code>echo MUJOCO_PY_MUJOCO_PATH\necho LD_LIBRARY_PATH\n</code></pre> <p>If any of these are not set properly you can set them as described above (see here for MUJOCO_PY_MUJOCO_PATH, LD_LIBRARY_PATH.</p> EngagingSuperCloud <p>Next load either a Python or Anaconda module. In this example I loaded the latest anaconda3 module (run <code>module avail anaconda</code> to see the current list of available anaconda modules):</p> <pre><code>module load anaconda3/2022.10\n</code></pre> <p>From here on you can follow the standard instructions to install mujoco-py, using the <code>--user</code> flag where appropriate to install in your home directory, or install in an anaconda or virtual environment (do not use the <code>--user</code> flag if you want to install in a conda or virtual environment). Here I am installing in my home directory with <code>--user</code>:</p> <pre><code>pip install --user 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Start up python and import mujoco_py to complete the build process:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>MuJoCo, particularly mujoco-py, can be tricky to install on SuperCloud as it uses file locking during the install and whenever the package is loaded. File locking is disabled on the SuperCloud shared filesystem performance reasons, but is available on the local disk of each node. Therefore, one workaround is to install mujoco-py on the local disk of one of the login nodes and then copy the install to your home directory. To load the package, the install then needs to be copied to the local disk.</p> <p>We\u2019ve found the most success by doing this with a python virtual environment. By using a python virtual environment you can install any additional packages you need with mujoco-py, and they can be used along with packages in our anaconda module, unlike conda environments.</p> <p>Create the virtual environment on the local disk of the login node and install mujoco-py (install the version you would like to use):</p> <pre><code>module load anaconda/2023a\nmkdir /state/partition1/user/$USER\npython -m venv /state/partition1/user/$USER/mujoco_env\nsource /state/partition1/user/$USER/mujoco_env/bin/activate\npip install 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Now install any other packages you need to run your MuJoCo jobs. With virtual environments you won\u2019t see any of the packages you\u2019ve previously installed with <code>pip install --user</code> or what you may have installed in another environment. You should still be able to use any of the packages in the anaconda module you\u2019ve loaded, so no need to install any of those.</p> <pre><code>pip install pkgname1\npip install pkgname2\n</code></pre> <p>Since you are installing into virtual environment, do not use the <code>--user</code> flag.</p> <p>Once you\u2019ve installed the packages you need, start Python and import mujoco_py to finish the build:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>Now that your environment is created, copy it to your home directory for permanent storage.</p> <pre><code>cp -r /state/partition1/user/$USER/mujoco_env $/software/mujoco/\n</code></pre> <p>If you\u2019d like you can run the few example lines listed on install section of the mujoco-py github page to verify the install went through properly:</p> <pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco_snippets/#using-mujoco-in-a-job","title":"Using MuJoCo in a Job","text":"<p>To use MuJoCo you\u2019ll need to first load the same Python or Anaconda module you used to install mujoco-py. If you installed it into a conda environment or python virtual environment, load that environment as well. We recommend you do this in your job submission script rather than in your .bashrc or at the command line before you submit the job. This way you know your job is configured properly every time it runs.</p> <p>You can use the following test scripts to test your MuJoCo setup in a job environment, and as a starting point for your own job:</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> EngagingSuperCloud submit_test.sh<pre><code>#!/bin/bash\n\n# Load the same python/anaconda module you used to install mujoco-py\nmodule load python/3.8.3\n\n# Run the script\npython mujoco_test.py\n</code></pre> <p>Now whenever you use mujoco-py the installation will need to be on the local disk of the node(s) where you are running. In your job script you can add a few lines of code that will check whether the environment exists on the local disk, and if not copy it. You can run these lines during an interactive job as well.</p> submit_test.sh<pre><code>#!/bin/bash``\n\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\nif [ ! -d \"$MUJOCO_ENV\" ]; then\n    echo \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\n    mkdir -p /state/partition1/user/$USER\n    cp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\n\nmodule load anaconda/2022a\nsource $MUJOCO_ENV/bin/activate\n\npython mujoco_test.py\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/","title":"Example of a minimal program using the nvhpc stack with CUDA aware MPI","text":"","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#about-nvhpc","title":"About NVHPC","text":"<p>NVHPC is an integrated collection of software tools and libraries distributed by NVidia. An overview document describing nvhpc  can be found here. The aim of the NVHPC team is to provide up to date, preconfigured suites of compilers, libraries and tools that are  specifically optimized for NVidia GPU hardware. It supports single and multi-GPU execution.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#basic-usage-example","title":"Basic Usage Example","text":"<p>This example shows steps for using NVHPC to run a simple test MPI program, written in C, that communicates between two GPUs. The detailed steps, that can be executed in an interactive Slurm session, are explained  below.  A complete Slurm job example is shown at the end.</p> <p>Prerequisites</p> <p>This example assumes you have access to a Slurm partition with GPU resources and if using Engaging are working with a Rocky Linux environment.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#1-activate-the-relevant-nvhpc-module","title":"1. Activate the relevant NVHPC module","text":"<p>The NVHPC environment is installed as a module and can be made visible in a session using the command</p> EngagingSatori <pre><code>  module load nvhpc/2023_233/nvhpc/23.3\n</code></pre> <pre><code>module load module load nvhpc/21.5\n</code></pre> <p>this will add a specific version of the nvhpc software (version 23.3 released in 2023 for Engaging and version 21.5 released in 2021 for Satori) to a shell or batch script. The software added includes compilers for C, C++ and Fortran; base GPU optimized numerical libraries for linear algebra, Fourier transforms and others; GPU optimized communication libraries supporting MPI, SHMEM and NCCL APIs.</p> <p>An environment variable, <code>NVHPC_ROOT</code>, is also set. This can be used in scripts to reference the locations of libraries when needed.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#2-set-paths-needed-for-compile-step","title":"2. Set paths needed for compile step","text":"<p>Here we use the module environment variable, <code>NVHPC_ROOT</code>, to set environment variables that have paths needed for compilation and linking of code.</p> <pre><code>culibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#3-create-a-c-program-for-that-executes-some-simple-multi-node-multi-gpu-test-code","title":"3. Create a C program for that executes some simple multi-node, multi-GPU test code.","text":"<p>The next step is to create a file holding C code that uses MPI to send information between two GPUs  running in different processes. Paste the C code below into a file called <code>test.c</code>.</p> test.c<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;cuda_runtime.h&gt;\nint main(int argc, char *argv[])\n{\n  int myrank, mpi_nranks; \n  int LBUF=1000000;\n  float *sBuf_h, *rBuf_h;\n  float *sBuf_d, *rBuf_d;\n  int bSize;\n  int i;\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);                  // my MPI rank\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;mpi_nranks);\n\n  if ( mpi_nranks != 2 ) { printf(\"Program requires exactly 2 ranks\\n\");exit(-1); }\n\n  int deviceCount;\n  cudaGetDeviceCount(&amp;deviceCount);               // How many GPUs?\n  printf(\"Number of GPUs found = %d\\n\",deviceCount);\n  int device_id = myrank % deviceCount;\n  cudaSetDevice(device_id);                       // Map MPI-process to a GPU\n  printf(\"Assigned GPU %d to MPI rank %d of %d.\\n\",device_id, myrank, mpi_nranks);\n\n  // Allocate buffers on each host and device\n  bSize = sizeof(float)*LBUF;\n  sBuf_h = malloc(bSize);\n  rBuf_h = malloc(bSize);\n  for (i=0;i&lt;LBUF;++i){\n    sBuf_h[i]=(float)myrank;\n    rBuf_h[i]=-1.;\n  }\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  cudaMalloc((void **)&amp;sBuf_d,bSize);\n  cudaMalloc((void **)&amp;rBuf_d,bSize);\n\n  cudaMemcpy(sBuf_d,sBuf_h,bSize,cudaMemcpyHostToDevice);\n\n  if ( myrank == 0 ) {\n   MPI_Recv(rBuf_d,LBUF,MPI_REAL,1,0,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n  } \n  else if ( myrank == 1 ) {\n   MPI_Send(sBuf_d,LBUF,MPI_REAL,0,0,MPI_COMM_WORLD);\n  }\n  else\n  {\n   printf(\"Unexpected myrank value %d\\n\",myrank);\n   exit(-1);\n  }\n\n  cudaMemcpy(rBuf_h,rBuf_d,bSize,cudaMemcpyDeviceToHost);\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#4-compile-program","title":"4. Compile program","text":"<p>Here we use nvhpc MPI wrapper to compile. The two environment variables we set earlier (<code>cuincdir</code> and <code>culibdir</code>) are used to let the compile step know where to find the relevant CUDA header and library files. The CUDA runtime library (<code>cudart</code>) is added as a location for finding CUDA functions the code utilizes.</p> <pre><code>mpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#5-execute-program","title":"5. Execute program","text":"<p>Once code has been compiled the <code>mpiexec</code> command that is part of the <code>nvhpc</code> module can be used to run the test program. The <code>nvhpc</code> module defaults to using its builtin version of OpneMPI. The OpenMPI option <code>btl_openib_warn_no_device_params_found</code> is passed into the OpenMPI runtime library. This option suppresses a warning that OpenMPI can generate when it encounters a network device card that is not present in a built-in list that OpenMPI has historically included.</p> EngagingSatori <pre><code>mpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre> <pre><code>salloc -n 2 --gres=gpu:2\nmpiexec -n 2 ./a.out \n</code></pre> <p>Note the <code>salloc</code> command is only needed to run interactively from the login node. If you are running in a batch job or are already in an interactive job on a compute node you will not need to first run <code>salloc</code>.</p> <p>Running this program using the command above should produce the following output.</p> <pre><code>Number of GPUs found = 1\nNumber of GPUs found = 1\nAssigned GPU 0 to MPI rank 0 of 2.\nrBuf_h[0] = -1.000000\nAssigned GPU 0 to MPI rank 1 of 2.\nrBuf_h[0] = 1.000000\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#example-of-slurm-job-file-for-executing-this-example","title":"Example of Slurm job file for executing this example","text":"<p>First create a file called \"test.c\" containing the example C program above. The job script file below will run all the steps described above for \"test.c\". It can be submitted to Slurm using the command <code>sbatch</code> followed by the filename holding the job script.</p> EngagingSatori test_cuda_and_mpi.sbatch<pre><code>#!/bin/bash\n#SBATCH -p sched_system_all\n#SBATCH --constraint=rocky8\n#SBATCH -N 2\n#SBATCH -n 2\n#SBATCH --gres=gpu:2\n#SBATCH --time=00:02:00\n#\n# Basic slurm job that tests GPU aware MPI in the NVHPC tool stack.\n#\n#\n#   To submit through Slurm use:\n#\n#   $ sbatch test_cuda_and_mpi.sbatch\n#  \n#   in terminal.\n\n# Write a little log info\necho \"## Start time \\\"\"`date`\"\\\"\"\necho \"## Slurm job running on nodes \\\"${SLURM_JOB_NODELIST}\\\"\"\necho \"## Slurm submit directory \\\"${SLURM_SUBMIT_DIR}\\\"\"\necho \"## Slurm submit host \\\"${SLURM_SUBMIT_HOST}\\\"\"\necho \" \"\n\n\nmodule load nvhpc/2023_233/nvhpc/23.3\nculibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n\nmpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n\nmpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre> test_cuda_and_mpi.sbatch<pre><code>#!/bin/bash\n#SBATCH -n 2\n#SBATCH --gres=gpu:2\n#SBATCH --time=00:02:00\n#\n# Basic slurm job that tests GPU aware MPI in the NVHPC tool stack.\n#\n#\n#   To submit through Slurm use:\n#\n#   $ sbatch test_cuda_and_mpi.sbatch\n#  \n#   in terminal.\n\n# Write a little log info\necho \"## Start time \\\"\"`date`\"\\\"\"\necho \"## Slurm job running on nodes \\\"${SLURM_JOB_NODELIST}\\\"\"\necho \"## Slurm submit directory \\\"${SLURM_SUBMIT_DIR}\\\"\"\necho \"## Slurm submit host \\\"${SLURM_SUBMIT_HOST}\\\"\"\necho \" \"\n\n\nmodule load nvhpc/21.5\nculibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n\nmpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n\nmpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/relion/","title":"Installing and Using RELION","text":"<p>RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a software package that employs an empirical Bayesian approach for electron cryo-microscopy (cryo-EM) structure determination. </p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/relion/#relion-on-satori","title":"RELION on Satori","text":"<p>This recipe is for building and using RELION on x86 nodes on Satori. It is different from working on IBM power9 nodes on Satori.</p> <p>Note</p> <p>The x86 nodes are available to some labs only. </p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/relion/#install-relion","title":"Install RELION","text":"<p>Go to your directory and download RELION, <pre><code>cd /nobackup/users/$USER\ngit clone https://github.com/3dem/relion.git\n</code></pre></p> <p>Get an interactive session on x86 nodes of Satori, <pre><code>srun -p sched_mit_mbathe -c 2 -t 60 --pty bash\n</code></pre></p> <p>Load modules for the GCC compiler and Openmpi implementation, <pre><code>module use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64  \nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64 \n</code></pre></p> <p>Note: These modules are installed for the x86 nodes only. </p> <p>Build RELION with CUDA and FFTW features, <pre><code>cd ~\nmkdir relion\ncd relion\ngit checkout master \ncd ..\nmkdir 4.0.1\ncd 4.0.1\nmkdir install\nmkdir build\ncd build\n\ncmake -DCMAKE_INSTALL_PREFIX=/home/$USER/relion/4.0.1/install -DFORCE_OWN_FFTW=ON -DAMDFFTW=ON -DCUDA_ARCH=80 ../../relion\nmake\nmake install\n</code></pre></p> <p>It is all set for the installation.</p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/relion/#use-relion","title":"Use RELION","text":"<p>There is a nice Graphical User Interface (GUI) for RELION. To use the GUI, first log in Satori with x-forwardig support, <pre><code>ssh -Y &lt;user&gt;@satori-login-002.mit.edu\n</code></pre></p> <p>Get an interactive session with GPU and x-forwarding support on x86 nodes of Satori, <pre><code>srun --x11 -p sched_mit_mbathe --gres=gpu:1 -c 6 -t 60 --pty bash\n</code></pre></p> <p>Set up environment for compilers, mpi implementation, FFTw, and RELION, <pre><code>module use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64  \nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64 \nmodule load fftw/3.3.10-x86_64\nexport LD_LIBRARY_PATH=/nfs/software001/home/software-r8-x86_64/spack-20230328/opt/spack/linux-rocky8-x86_64/gcc-12.2.0/fftw-3.3.10-qiaruimvw6zu2h4f5eolqom7tixem6vk/lib:$LD_LIBRARY_PATH\nexport RELION_HOME=/home/$USER/relion/4.0.1/install\nexport PATH=${RELION_HOME}/bin:$PATH\nexport LD_LIBRARY_PATH=${RELION_HOME}/lib:$LD_LIBRARY_PATH\n</code></pre></p> <p>then open the RELION GUI,  <pre><code>relion &amp;\n</code></pre></p> <p>Users can use GUI to edit files or submit jobs. Refer to details on this page. </p> <p>Alternatively, users can prepare a batch job script to submit jobs. </p> <p>Download RELION benchmarks for testing,  <pre><code>cd ~/relion\nwget ftp://ftp.mrc-lmb.cam.ac.uk/pub/scheres/relion_benchmark.tar.gz\n</code></pre> then all benchmark files will be saved in a directory named <code>relion_benchmark</code>.</p> <p>Here is an exmaple job script, <pre><code>#!/bin/bash\n#SBATCH --partition=sched_mit_mbathe\n#SBATCH --time=12:00:00\n#SBATCH --nodes=1\n#SBATCH -n 20\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=10000\n#SBATCH --gres=gpu:4\n#SBATCH --chdir='.'\n\nmodule use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64\nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64\nexport LD_LIBRARY_PATH=/nfs/software001/home/software-r8-x86_64/spack-20230328/opt/spack/linux-rocky8-x86_64/gcc-12.2.0/fftw-3.3.10-qiaruimvw6zu2h4f5eolqom7tixem6vk/lib:$LD_LIBRARY_PATH\nexport RELION_HOME=\"/home/$USER/relion/4.0.1/install\"\nexport PATH=${RELION_HOME}/bin:$PATH\nexport LD_LIBRARY_PATH=${RELION_HOME}/lib:$LD_LIBRARY_PATH\n\ncd ~/relion/relion_benchmark\nmkdir output\n\nmpirun -np 20 relion_refine_mpi \\\n  --i Particles/shiny_2sets.star \\\n  --o output \\\n  --ref emd_2660.map:mrc \\\n  --ini_high 60 \\\n  --pool 100 \\\n  --pad 2  \\\n  --ctf \\\n  --iter 25 \\\n  --tau2_fudge 4 \\\n  --particle_diameter 360 \\\n  --K 4 \\\n  --flatten_solvent \\\n  --zero_mask \\\n  --oversampling 1 \\\n  --healpix_order 2 \\\n  --offset_range 5 \\\n  --offset_step 2 \\\n  --sym C1 \\\n  --norm \\\n  --scale \\\n  --j 1   \\\n  --gpu \"\" \\\n --dont_combine_weights_via_disc \\\n  --scratch_dir /tmp\n</code></pre></p> <p>Add the above lines in a file named <code>job.sh</code>, then submit the job, <pre><code>sbatch job.sh\n</code></pre></p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/vscode/","title":"Using VSCode on an ORCD System","text":"<p>VSCode is a convenient IDE for development, and one of its nicest features is its ability to run on a remote system using its RemoteSSH extension. This means you can have the VSCode window on your computer, while the files and anything you run will be on the remote system you are connected to.</p> <p>Once you've installed the RemoteSSH extension this is fairly easy to set up. However, it is also very easy to set up in such a way that it is not only slow for you, but it also puts excess load on the login nodes and in turn slows things down for others on that node. Luckily, with a few extra steps you can run VSCode on a compute node where it can have more resources to run and won't impact others as much.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#setting-up-your-config-file","title":"Setting up your Config File","text":"<p>Click the \"Open a Remote Window\" button in the bottom left corner of your VSCode window (It is a small blue rectangle labeled with <code>&gt;&lt;</code>). In the bar at the top of the page select \"Connect to Host...\", then \"Configure SSH Hosts\", and select first option, which will differ depending on your operating system. This will open your config file in a VSCode tab.</p> <p>To run on a compute node you will need at least 2 entries in this file. The first will be a login node that you'll \"jump\" through and the second will be the compute node that is your final destination.</p> EngagingSatoriOpen Mind config<pre><code>Host eofe-login\n  HostName eofe8.mit.edu\n  User USERNAME\n\nHost eofe-compute\n  User USERNAME\n  HostName nodename\n  ProxyJump eofe-login\n</code></pre> <p>Note</p> <p>If you are using one of the login nodes that requires 2-Factor authentication be ready to receive your default 2-Factor prompt when you connect. If you do not respond right away the connection will time out.</p> config<pre><code>Host satori-login\n  HostName satori-login-001.mit.edu\n  User USERNAME\n\nHost satori-compute\n  User USERNAME\n  HostName nodename\n  ProxyJump satori-login\n</code></pre> config<pre><code>Host om-login\n  HostName openmind7.mit.edu\n  User USERNAME\n\nHost om-compute\n  User USERNAME\n  HostName nodename\n  ProxyJump om-login\n</code></pre> <p>Replace <code>USERNAME</code> with your username on the system you are connecting to. We will fill in \"nodename\" later.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#starting-your-vscode-session-on-a-compute-node","title":"Starting your VSCode Session on a Compute Node","text":"<p>Each time you sit down to do remote work through VSCode you will have three steps:</p> <ol> <li>Start an interactive job on the target system and note the name of the node your job is running on</li> <li>Update your config file with the node name</li> <li>Connect to the compute node using your updated config</li> </ol> <p>We go through these steps in detail below.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#start-an-interactive-job","title":"Start an Interactive Job","text":"<p>Open a terminal window and ssh into the login node. If you are not used to doing this you can open a terminal in VSCode and run:</p> EngagingSatoriOpen Mind <pre><code>ssh eofe-login\n</code></pre> <pre><code>ssh satori-login\n</code></pre> <pre><code>ssh om-login\n</code></pre> <p>Use the name you have used for the login <code>Host</code> in your config file if different than the one above. The example screenshot below shows logging into one of the Engaging login nodes with ssh in a VSCode terminal window.</p> <p></p> <p>Once you are logged in start an interactive session. If you are planning to only edit files a single core may be sufficient, but if you plan to run code or Jupyter Notebooks you may want to allocate more resources accordingly. Refer to the documentation for your system on how to request an interactive job:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging's Documentation for Running Jobs</p> <p>Satori's Documentation for Running Jobs</p> <p>SuperCloud's Documentation for Running Jobs</p> <p>OpenMind's Documentation for Running Jobs</p> <p>Once your job has started you can run the <code>hostname</code> command to get the name of the node your interactive job is running on. You can also run the <code>squeue --me</code> command to list all your running jobs and get the hostname from the last column.</p> <p>The screenshot below shows requesting a single interactive core for 1 hour on Engaging:</p> <p></p> <p>Note that the scheduler will also tell you which node you are allocated in its output. In this screenshot my node name is <code>node020</code>.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#update-your-config-file","title":"Update your Config File","text":"<p>Update the <code>HostName</code> of your compute node entry in your config file. If your config file is not open, follow the instructions above to open it again. Then replace whatever you have for <code>HostName</code> in your config file with the output of the <code>hostname</code> command you ran in your interactive session, or got from <code>squeue --me</code>.</p> <p>If your compute node is <code>node1234</code> then your config file should look something like:</p> EngagingSatoriOpen Mind config<pre><code>Host eofe-login\n  HostName eofe8.mit.edu\n  User USERNAME\n\nHost eofe-compute\n  User USERNAME\n  HostName node1234\n  ProxyJump eofe-login\n</code></pre> config<pre><code>Host satori-login\n  HostName satori-login-001.mit.edu\n  User USERNAME\n\nHost satori-compute\n  User USERNAME\n  HostName node1234\n  ProxyJump satori-login\n</code></pre> config<pre><code>Host om-login\n  HostName openmind7.mit.edu\n  User USERNAME\n\nHost om-compute\n  User USERNAME\n  HostName node1234\n  ProxyJump om-login\n</code></pre> <p>Where <code>USERNAME</code> is replaced by your username.</p> <p>This screenshot shows updating the config file for an interactive job running on Engaging:</p> <p></p> <p>Since the interactive job in my screenshot is running on <code>node020</code>, I have updated <code>HostName</code> to <code>node-020</code> for the <code>eofe-compute</code> entry in my config file.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#connect-to-the-compute-node","title":"Connect to the Compute Node","text":"<p>You are ready to connect to the compute node you have allocated through your interactive job from VSCode. Select the \"Open a Remote Window\" button in the bottom left corner of your VSCode window. In the bar at the top of the page select \"Connect to Host...\" and select the Host for the compute node that you have created.</p> EngagingSatoriOpen Mind <p>In the example config file above this would be <code>eofe-compute</code>.</p> <p>In the example config file above this would be <code>satori-compute</code>.</p> <p>In the example config file above this would be <code>om-compute</code>.</p> <p>Here is what this might look like for Engaging:</p> <p></p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#other-vscode-best-practices-tips-and-tricks","title":"Other VSCode Best Practices, Tips, and Tricks","text":"<ul> <li>Avoid running VSCode through RemoteSSH on the login nodes. If you are only editing files this might be okay, although it is not encouraged. Beyond editing files please use a compute node for VSCode, as described on this page.</li> <li>Add the specific directories you need to your workspace. VSCode constantly scans all the files files and runs git commands on any local git repositories in your workspace, and it does this recursively. For this reason adding high-level directories to your workspace can slow things down quite a bit. For example, avoid adding your entire home directory or group storage to your VSCode session workspace.</li> <li>If you are having trouble authenticating, particularly if you are prompted for a password or 2 Factor authentication options, you can set <code>\"remote.SSH.showLoginTerminal\": true</code> in your settings.json file. See this page for more information.</li> <li>If VSCode is slow to start up on an ORCD System, check to see whether you are activating a conda environment at login. If you are, run the command <code>conda config --set auto_activate_base false</code> to prevent this. You will only have to do this once.</li> </ul>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#best-practices","title":"Best Practices","text":"<ul> <li>VSCode</li> </ul>"},{"location":"tags/#engaging","title":"Engaging","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MPI jobs</li> <li>MuJoCo</li> <li>Installing and Using MuJoCo (Snippets)</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gpu","title":"GPU","text":"<ul> <li>GROMACS</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gromacs","title":"GROMACS","text":"<ul> <li>GROMACS</li> </ul>"},{"location":"tags/#getting-help","title":"Getting Help","text":"<ul> <li>Getting Help</li> </ul>"},{"location":"tags/#getting-started","title":"Getting Started","text":"<ul> <li>Getting Started Tutorial</li> </ul>"},{"location":"tags/#h100","title":"H100","text":"<ul> <li>Getting started on 8-way H100 nodes on Satori</li> </ul>"},{"location":"tags/#howto-recipes","title":"Howto Recipes","text":"<ul> <li>GROMACS</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>MPI jobs</li> <li>MPI for Python</li> <li>MPI for Python (Snippets)</li> <li>MuJoCo</li> <li>Installing and Using MuJoCo (Snippets)</li> <li>NVHPC with CUDA aware MPI</li> <li>RELION</li> <li>VSCode</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>Getting Started Tutorial</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>GROMACS</li> <li>MPI jobs</li> <li>MPI for Python</li> <li>MPI for Python (Snippets)</li> <li>NVHPC with CUDA aware MPI</li> <li>RELION</li> </ul>"},{"location":"tags/#mujoco","title":"MuJoCo","text":"<ul> <li>MuJoCo</li> <li>Installing and Using MuJoCo (Snippets)</li> </ul>"},{"location":"tags/#openmind","title":"OpenMind","text":"<ul> <li>MPI for Python</li> <li>MPI for Python (Snippets)</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>MPI for Python</li> <li>MPI for Python (Snippets)</li> </ul>"},{"location":"tags/#relion","title":"RELION","text":"<ul> <li>RELION</li> </ul>"},{"location":"tags/#rocky-linux","title":"Rocky Linux","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#satori","title":"Satori","text":"<ul> <li>ORCD Systems</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>NVHPC with CUDA aware MPI</li> <li>RELION</li> </ul>"},{"location":"tags/#supercloud","title":"SuperCloud","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MuJoCo</li> <li>Installing and Using MuJoCo (Snippets)</li> </ul>"},{"location":"tags/#cuda","title":"cuda","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#cuda-aware-mpi","title":"cuda aware mpi","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#nvhpc","title":"nvhpc","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#vscode","title":"vscode","text":"<ul> <li>VSCode</li> </ul>"}]}