
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://orcd.mit.edu/recipes/torch-gpu/">
      
      
        <link rel="prev" href="../pycharm/">
      
      
        <link rel="next" href="../torch-gpu-intermediate/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>PyTorch on GPUs - I - ORCD Docs</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#deep-learning-with-pytorch-on-gpus" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ORCD Docs" class="md-header__button md-logo" aria-label="ORCD Docs" data-md-component="logo">
      
  <img src="../../images/ORCD_logo_icononly.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ORCD Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PyTorch on GPUs - I
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ORCD Docs" class="md-nav__button md-logo" aria-label="ORCD Docs" data-md-component="logo">
      
  <img src="../../images/ORCD_logo_icononly.png" alt="logo">

    </a>
    ORCD Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../orcd-systems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ORCD Systems
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Started Tutorial
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Accessing ORCD Systems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Accessing ORCD Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../accessing-orcd/ondemand-login/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logging in with OnDemand
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../accessing-orcd/ssh-login/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logging in with SSH via Terminal
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../accessing-orcd/ssh-setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SSH Key Setup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vscode/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VSCode Remote SSH
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Filesystems and File Transfer
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Filesystems and File Transfer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../filesystems-file-transfer/filesystems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    General Use Filesystems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../filesystems-file-transfer/project-filesystems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Project Specific Filesystems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../filesystems-file-transfer/transferring-files/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transferring Files
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Software
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Software
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/compile/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Compiling Codes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/apptainer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Containers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/julia/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Julia
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/modules/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Modules
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/R/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    R
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/building-software/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Other Software
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Running Jobs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Running Jobs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/requesting-resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Requesting Resources
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/job-arrays/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Job Arrays
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/available-resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Available Resources
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/application-analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Application Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" checked>
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ORCD Recipes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            ORCD Recipes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../af3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AlphaFold 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gromacs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GROMACS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../intel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Intel compiler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jupyter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jupyter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpi4py/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MPI for Python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MPI Jobs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mujoco/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MuJoCo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nvhpc-a100-with-cuda-and-mpi-example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    NVHPC with CUDA aware MPI
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../orca/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ORCA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pycharm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PyCharm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    PyTorch on GPUs - I
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    PyTorch on GPUs - I
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installing-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Installing PyTorch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch-on-cpu-and-a-single-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch on CPU and a single GPU
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch-on-multiple-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch on multiple GPUs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PyTorch on multiple GPUs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#single-node-multi-gpu-data-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Single-node multi-GPU data parallel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-node-multi-gpu-data-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-node multi-GPU data parallel
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../torch-gpu-intermediate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PyTorch on GPUs - II
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../relion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RELION
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../build-vasp-gcc-cpu/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VASP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vscode/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VSCode Remote SSH
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../X11/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    X11
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../acknowledgements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Acknowledging Us
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data-security/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Security and Privacy
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code-of-conduct/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Acceptable Use and Code of Conduct
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-help/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Help
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faqs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAQs
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tags/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Index
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installing-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Installing PyTorch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch-on-cpu-and-a-single-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch on CPU and a single GPU
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch-on-multiple-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch on multiple GPUs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PyTorch on multiple GPUs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#single-node-multi-gpu-data-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Single-node multi-GPU data parallel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-node-multi-gpu-data-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-node multi-GPU data parallel
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <nav class="md-tags" >
    
      
      
      
        <a href="../../tags/#tag:engaging" class="md-tag">Engaging</a>
      
    
      
      
      
        <a href="../../tags/#tag:gpu" class="md-tag">GPU</a>
      
    
      
      
      
        <a href="../../tags/#tag:howto-recipes" class="md-tag">Howto Recipes</a>
      
    
      
      
      
        <a href="../../tags/#tag:pytorch" class="md-tag">Pytorch</a>
      
    
  </nav>



<h1 id="deep-learning-with-pytorch-on-gpus">Deep Learning with PyTorch on GPUs<a class="headerlink" href="#deep-learning-with-pytorch-on-gpus" title="Permanent link">&para;</a></h1>
<p>Deep learning is the foundation of artificial intelligence nowadays. Deep learning programs can be accelerated substantially on GPUs. </p>
<p>PyTorch is a popular Python package for working on deep learning projects.</p>
<p>This page introduces recipes to run deep-learning programs on GPUs with Pytorch. </p>
<h2 id="installing-pytorch">Installing PyTorch<a class="headerlink" href="#installing-pytorch" title="Permanent link">&para;</a></h2>
<div class="tabbed-set tabbed-alternate" data-tabs="1:1"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="__tabbed_1_1">Engaging</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>First, load a Miniforge module to provide Python platform, 
 <div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>module load miniforge/24.3.0-0
</span></code></pre></div>
 Create a new environment and install PyTorch,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>conda create -n torch
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>source activate torch
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>pip install torch
</span></code></pre></div>
 This installs PyTorch with CUDA support by default, which enables it to run on GPUs.  </p>
</div>
</div>
</div>
<h2 id="pytorch-on-cpu-and-a-single-gpu">PyTorch on CPU and a single GPU<a class="headerlink" href="#pytorch-on-cpu-and-a-single-gpu" title="Permanent link">&para;</a></h2>
<p>We start with a recipe to run PyTorch on CPU and a single GPU.</p>
<p>We use an example code training a convolutional neural network (CNN) with the CIFAR10 data set. Refer to <a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html">description of this example</a>. Download the codes <a href="../scripts/torch-gpu/cnn_cifar10_cpu.py">for CPU</a> and <a href="../scripts/torch-gpu/cnn_cifar10_gpu.py">for GPU</a>. </p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:1"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="__tabbed_2_1">Engaging</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>Prepare a job script named <code>job.sh</code> like this,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>#!/bin/bash
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>#SBATCH -p mit_normal_gpu   
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>#SBATCH --gres=gpu:1 
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>#SBATCH -t 30
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>#SBATCH -N 1
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>#SBATCH -n 2
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>#SBATCH --mem=10GB
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>module load miniforge/24.3.0-0
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>source activate torch
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>echo &quot;~~~~~~~~ Run the program on CPU ~~~~~~~~~&quot;
</span><span id="__span-2-13"><a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>time python cnn_cifar10_cpu.py
</span><span id="__span-2-14"><a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>echo &quot;~~~~~~~~ Run the program on GPU ~~~~~~~~~&quot;
</span><span id="__span-2-15"><a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>time python cnn_cifar10_gpu.py
</span></code></pre></div>
 then submit it,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>sbatch job.sh
</span></code></pre></div></p>
</div>
</div>
</div>
<p>The <code>mit_normal_gpu</code> partition is for all MIT users. If your lab has a partition with GPUs, you can use it too.  </p>
<p>The <code>#SBATCH</code> flags <code>-N 1 -n 2</code> requests two CPU cores on one node, and the <code>--mem=10GB</code> means 10 GB of memory per node (not per core).</p>
<p>The programs <code>cnn_cifar10_cpu.py</code> and <code>cnn_cifar10_gpu.py</code> will run on CPUs and a GPU, respectively. When the problem size is large, the program will be accelerated on a GPU. </p>
<p>While the job is running, you can check if the program runs on a GPU. First, check the hostname that it runs on,
<div class="language-text highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>squeue -u $USER
</span></code></pre></div>
and then log in the node,
<div class="language-text highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>ssh &lt;nodeXXX&gt;
</span></code></pre></div>
and check the GPU usage with the <code>nvtop</code> command.</p>
<h2 id="pytorch-on-multiple-gpus">PyTorch on multiple GPUs<a class="headerlink" href="#pytorch-on-multiple-gpus" title="Permanent link">&para;</a></h2>
<p>Deep learning programs can be further accelerated on multiple GPUs. </p>
<p>There are various parallelisms to enable distributed deep learning on multiple GPUs, including data parallel and model parallel. We will focus on data parallel on this page.   </p>
<p>Data parallel allows training a model with multiple batches of data simultaneously. The model has to fit into the GPU memory.</p>
<p>On a cluster, there are many nodes and multiple GPUs on each node. We will first introduce a recipe to run PyTorch programs with multiple GPUs within one node, and then extend it to multiple nodes. </p>
<p>We use an example code that trains a linear network with a random data set, which is implemented with the <a href="https://PyTorch.org/docs/stable/notes/ddp.html">Distributed Data Parallel</a> package in PyTorch. Refer to the description of this example <a href="https://pytorch.org/tutorials/beginner/ddp_series_multigpu.html">for multiple GPUs within one node</a> and <a href="https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html">for multiple GPUs across multiple nodes</a>. </p>
<p>Download the codes for this example: <a href="../scripts/torch-gpu/datautils.py">datautils.py</a>, <a href="../scripts/torch-gpu/multigpu.py">multigpu.py</a>, <a href="../scripts/torch-gpu/multigpu_torchrun.py">multigpu_torchrun.py</a>, and <a href="../scripts/torch-gpu/multinode.py">multinode.py</a>.</p>
<h3 id="single-node-multi-gpu-data-parallel">Single-node multi-GPU data parallel<a class="headerlink" href="#single-node-multi-gpu-data-parallel" title="Permanent link">&para;</a></h3>
<p>In this section, we introduce a recipe for single-node multi-GPU data parallel. The program <code>multigpu.py</code> is set up for this purpose. </p>
<div class="tabbed-set tabbed-alternate" data-tabs="3:1"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio" /><div class="tabbed-labels"><label for="__tabbed_3_1">Engaging</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>To run the program on 4 GPUs within one node, prepare a job script named <code>job.sh</code> like this,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>#!/bin/bash
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>#SBATCH -p mit_normal_gpu
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>#SBATCH --job-name=ddp
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>#SBATCH -N 1
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>#SBATCH -n 4
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>#SBATCH --mem=20GB
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>#SBATCH --gres=gpu:4   
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>
</span><span id="__span-6-9"><a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>module load miniforge/24.3.0-0
</span><span id="__span-6-10"><a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a>source activate torch
</span><span id="__span-6-11"><a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>
</span><span id="__span-6-12"><a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a>echo &quot;======== Run on multiple GPUs ========&quot;
</span><span id="__span-6-13"><a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a># Set 100 epochs and save checkpoints every 20 epochs
</span><span id="__span-6-14"><a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a>python multigpu.py --batch_size=1024 100 20
</span></code></pre></div>
 then submit it,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>sbatch job.sh
</span></code></pre></div></p>
</div>
</div>
</div>
<p>The <code>-N 1 -n 4 --gres=gpu:4</code> flags request 4 CPU cores and 4 GPUs on one node. For most GPU programs, it is recommended to set the number of CPU cores no less than the number of GPUs.</p>
<p>As is set up in the code <code>multigpu.py</code>, it will run on all of the GPUs requested in Slurm, which means 4 GPUs within one node in this case. The training process happens on 4 batches of data simultaneously. </p>
<p>Check if the program runs on multiple GPUs using the <code>nvtop</code> command as described in the previous section.  </p>
<p>There is another way to run a Pytorch prgram with multiple GPUs, that is to use the <code>torchrun</code> command. The program for this purpose is <code>multigpu_torchrun.py</code>. In the above job script, change the last line to this, 
<div class="language-text highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>torchrun --nnodes=1 --nproc_per_node=4 \
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>         --rdzv_id=$SLURM_JOB_ID \
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>         --rdzv_endpoint=&quot;localhost:1234&quot; \
</span><span id="__span-8-4"><a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>         multigpu_torchrun.py --batch_size=1024 100 20
</span></code></pre></div></p>
<p>With the flags <code>--nnodes=1 --nproc-per-node=4</code>, the <code>torchrun</code> command will run the program on 4 GPUs within one node. </p>
<p>The flags with <code>rdzv</code> (meaning the Rendezvous protocol) are required by <code>torchrun</code> to coordinate multiple processes. The flag <code>--rdzv-id=$SLURM_JOB_ID</code> sets to the <code>rdzv</code> ID be the job ID, but it can be any random number. The flag <code>--rdzv-endpoint=localhost:1234</code> is to set the host and the port. Use <code>localhost</code> when there is only one node. The port can be any 4- or 5-digit number larger than 1024. </p>
<p>The <code>torchrun</code> command will be useful for running the program across multiple nodes in the next section. </p>
<details>
<summary>GPU communication within one node</summary>
<p>The NVIDIA Collective Communications Library (NCCL) is set as the backend in the PyTorch programs <code>multigpu.py</code> and <code>multigpu_torchrun.py</code>, so that the data communication between GPUs within one node benefits from the high bandwidth of NVLinks.  </p>
</details>
<h3 id="multi-node-multi-gpu-data-parallel">Multi-node multi-GPU data parallel<a class="headerlink" href="#multi-node-multi-gpu-data-parallel" title="Permanent link">&para;</a></h3>
<p>Now we extend the above example to multi-node multi-GPU data parallel. The program <code>multinode.py</code> is set up for this purpose.</p>
<p>There are two key points in this approach.</p>
<ol>
<li>
<p>Use the <code>srun</code> command in Slurm to launch a <code>torchrun</code> command on each node.</p>
</li>
<li>
<p>Set up <code>torchrun</code> to coordinate processes on different nodes.</p>
</li>
</ol>
<div class="tabbed-set tabbed-alternate" data-tabs="4:1"><input checked="checked" id="__tabbed_4_1" name="__tabbed_4" type="radio" /><div class="tabbed-labels"><label for="__tabbed_4_1">Engaging</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>To run on multiple GPUs across multiple nodes, prepare a job script like this,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>#!/bin/bash
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>#SBATCH -p mit_normal_gpu
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>#SBATCH --job-name=ddp-2nodes
</span><span id="__span-9-4"><a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a>#SBATCH -N 2
</span><span id="__span-9-5"><a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>#SBATCH --ntasks-per-node=1
</span><span id="__span-9-6"><a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a>#SBATCH --cpus-per-task=4
</span><span id="__span-9-7"><a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>#SBATCH --gpus-per-node=4 
</span><span id="__span-9-8"><a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a>#SBATCH --mem=20GB
</span><span id="__span-9-9"><a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a>
</span><span id="__span-9-10"><a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a>module load miniforge/24.3.0-0
</span><span id="__span-9-11"><a id="__codelineno-9-11" name="__codelineno-9-11" href="#__codelineno-9-11"></a>source activate torch
</span><span id="__span-9-12"><a id="__codelineno-9-12" name="__codelineno-9-12" href="#__codelineno-9-12"></a>
</span><span id="__span-9-13"><a id="__codelineno-9-13" name="__codelineno-9-13" href="#__codelineno-9-13"></a># Get IP address of the master node
</span><span id="__span-9-14"><a id="__codelineno-9-14" name="__codelineno-9-14" href="#__codelineno-9-14"></a>nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
</span><span id="__span-9-15"><a id="__codelineno-9-15" name="__codelineno-9-15" href="#__codelineno-9-15"></a>nodes_array=($nodes)
</span><span id="__span-9-16"><a id="__codelineno-9-16" name="__codelineno-9-16" href="#__codelineno-9-16"></a>master_node=${nodes_array[0]}
</span><span id="__span-9-17"><a id="__codelineno-9-17" name="__codelineno-9-17" href="#__codelineno-9-17"></a>master_node_ip=$(srun --nodes=1 --ntasks=1 -w &quot;$master_node&quot; hostname --ip-address)
</span><span id="__span-9-18"><a id="__codelineno-9-18" name="__codelineno-9-18" href="#__codelineno-9-18"></a>
</span><span id="__span-9-19"><a id="__codelineno-9-19" name="__codelineno-9-19" href="#__codelineno-9-19"></a>echo &quot;======== Run on multiple GPUs across multiple nodes ======&quot;     
</span><span id="__span-9-20"><a id="__codelineno-9-20" name="__codelineno-9-20" href="#__codelineno-9-20"></a>srun torchrun --nnodes=$SLURM_NNODES \
</span><span id="__span-9-21"><a id="__codelineno-9-21" name="__codelineno-9-21" href="#__codelineno-9-21"></a>     --nproc-per-node=$SLURM_CPUS_PER_TASK \
</span><span id="__span-9-22"><a id="__codelineno-9-22" name="__codelineno-9-22" href="#__codelineno-9-22"></a>     --rdzv-id=$SLURM_JOB_ID   \
</span><span id="__span-9-23"><a id="__codelineno-9-23" name="__codelineno-9-23" href="#__codelineno-9-23"></a>     --rdzv-backend=c10d \
</span><span id="__span-9-24"><a id="__codelineno-9-24" name="__codelineno-9-24" href="#__codelineno-9-24"></a>     --rdzv-endpoint=$master_node_ip:1234 \
</span><span id="__span-9-25"><a id="__codelineno-9-25" name="__codelineno-9-25" href="#__codelineno-9-25"></a>     multinode.py --batch_size=1024 100 20
</span></code></pre></div>
 then submit it,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>sbatch job.sh
</span></code></pre></div></p>
</div>
</div>
</div>
<p>As the <code>#SBATCH</code> flags <code>-N 2</code> and <code>--ntasks-per-node=1</code> request for two nodes with one task per node, the <code>srun</code> command launches a <code>torchrun</code> command on each of the two nodes.</p>
<p>The <code>#SBATCH</code> flags <code>--cpus-per-task=4</code> and <code>--gpus-per-node=4</code> request 4 GPU cores and 4 GPUs on each node. Accordingly, the <code>torchrun</code> flags are set as <code>--nnodes=$SLURM_NNODES --nproc-per-node=$SLURM_CPUS_PER_TASK</code>, so that the <code>torchrun</code> command runs the program on 4 GPUs on each of the two nodes. That says the program runs on 8 GPUs, and thus the training process happens on 8 batches of data simultaneously. </p>
<p>The flags with <code>rdzv</code> are required by <code>torchrun</code> to coordinate processes across nodes. The <code>--rdzv-backend=c10d</code> is to use a C10d store (by default TCPStore) as the rendezvous backend, the advantage of which is that it requires no 3rd-party dependency. The <code>--rdzv-endpoint=$master_node_ip:1234</code> is to set up the IP address and the port of the master node. The IP address is obtained in a previous part of the job script.</p>
<p>Refer to details of torchrun on <a href="https://pytorch.org/docs/stable/elastic/run.html">this page</a>.</p>
<details>
<summary>GPU communication across nodes</summary>
<p>The NCCL is set as backend in the PyTorch program <code>multinode.py</code>, so that the data communication between GPUs within one node benefits from the high bandwidth of NVLinks, and the data communication between GPUs across nodes benefits from the bandwidth of the Infiniband network. </p>
</details>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="April 10, 2025 14:59:07 UTC">April 10, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      <a href="https://accessibility.mit.edu/">Accessibility</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
        <script src="../../js/open_in_new_tab.js"></script>
      
    
  </body>
</html>