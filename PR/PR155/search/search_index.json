{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MIT Office of Research Computing and Data Hands On Help Pages","text":"<p>The MIT Office or Research Computing and Data (ORCD) provides access and support for the compute and data needs of a wide range of research activities. These pages provide  help material for hands-on working with ORCD supported services. </p> <p>Help working with ORCD services is also available through email to orcd-help@mit.edu, please  feel free to contact us with questions and suggestions. For more information on existing documentation,  office hours, and other ways to get help, please see our Getting Help page.</p>"},{"location":"acknowledgements/","title":"Acknowledging Us","text":"<p>If have used our systems or consultation services and you would like to acknowledge the MIT Office of Research Computing an Data in your paper, we recommend adding the following to your Acknowledgments section (be sure to select the applicable resource(s) from among those in the brackets below):</p> <p>Acknowledgement Statement</p> <p>The authors acknowledge the MIT Office of Research Computing and Data for providing [high performance computing, consultation, data] resources that have contributed to the research results reported within this paper.</p> <p>Thank you for acknowledging us \u2013 we appreciate it.</p>"},{"location":"code-of-conduct/","title":"Acceptable Use and Code of Conduct","text":"<p>The ORCD systems are operated by the MIT Office of Research Computing and Data and certain appropriate common sense rules apply to working on it.</p>"},{"location":"code-of-conduct/#acceptable-use-guidelines","title":"Acceptable Use Guidelines","text":"<p>ORCD systems are intended for research associated with MIT projects or collaborations around MIT research projects. That can cover a lot of things, but all account holders are expected to use judgement and apply common sense to their use of the system. The system is not to be used to support commercial activities or for non-MIT related activities. It is not to be used for anything that might be construed as illegal or criminal. Datasets on the system must have been obtained legitimately and the system is not to be used for working with unanonymized data or data subject to ITAR or other national security  restrictions. See the Data Security and Privacy page for more information about data. If you are unsure about a planned use, please feel free to contact orcd-help@mit.edu. </p> <p>All systems are covered by MIT Institute wide policies for acceptable use of information technology, including the MITnet Rules of Use. For data practices refer to our page on Data Security and Privacy and the links on that page, particularly MIT's page on Information Protection.</p> <p>Account holders should not share accounts and should take reasonable precautions to ensure  that credentials for accessing the system (passwords, ssh keys, etc.) are kept secure.  All account holders agree to respect requests from support staff around how they use  the system. The support staff may, as needed, impose whatever policies are required to ensure the system runs well for all projects on the system. </p>"},{"location":"code-of-conduct/#code-of-conduct","title":"Code of Conduct","text":"<p>ORCD systems are shared resource used by a wide community. All people involved in its use and operations should try their utmost to be courteous and kind at all times. Members of the  ORCD community should be respectful toward one another and endeavor to ensure a  welcoming and collegial environment for all. Account holders are also expected to respect privacy of others activities on the system, and not to try to gain access to parts of the system they are not explicitly authorized to access.</p>"},{"location":"data-security/","title":"Data Security and Privacy","text":""},{"location":"data-security/#what-data-can-be-stored","title":"What data can be stored?","text":"<p>All ORCD current systems are only suitable for storing data with low-level security requirements. This means that they are not to be used to store sensitive data, such as personal information, financial information, or intellectual property. Additionally, they are not to be used to store data that is subject to use agreements that require security controls or audit tracking.</p> <p>The following data types are suitable for ORCD systems:</p> <ul> <li>Anything in the low-risk category described at the MIT IS&amp;T data risk classification pages</li> <li>Drafts of unpublished research papers and results that are based on low-risk data</li> </ul> <p>Anything else in the medium-risk or higher risk levels of the MIT IS&amp;T data risk classification pages should not be stored or analyzed on current ORCD systems.</p> <p>If you have any questions about whether your data is appropriate for storing on ORCD systems please feel free to reach out to us at orcd-help@mit.edu. </p>"},{"location":"data-security/#where-can-more-sensitive-data-be-processed-and-stored","title":"Where can more sensitive data be processed and stored?","text":"<p>The ORCD team is currently developing a system for more sensitive data. If you have sensitive data and would like to learn about our plans please feel free to get in touch at orcd-help@mit.edu.</p>"},{"location":"data-security/#support-team-access-to-orcd-system-accounts-and-resources","title":"Support team access to ORCD system accounts and resources","text":"<p>Support staff may occasionally access accounts of other users to help debug and troubleshoot problems. All access will be limited to the minimum reasonably needed to address a problem. Staff will not share any data or contents beyond the needs for providing adequate systems support and ensuring stability and security of the systems. </p>"},{"location":"faqs/","title":"Frequently Asked Questions","text":""},{"location":"faqs/#how-do-i-get-gpu-access","title":"How do I get GPU access?","text":"<p>We have L40S and H200 GPUs available on Engaging through the <code>mit_normal_gpu</code> partition. There are also a variety of GPU types available through the <code>mit_preemptable</code> partition. Take a look at out page on requesting resources to see how to request them for your job.</p> <p>If your lab would like to purchase GPUs to be hosted on Engaging, please contact orcd-help-engaging@mit.edu.</p>"},{"location":"faqs/#how-do-i-check-the-status-of-my-job","title":"How do I check the status of my job?","text":"<p>Instructions for checking job status can be found here.</p>"},{"location":"faqs/#how-can-i-submit-a-module-request","title":"How can I submit a module request?","text":"<p>We are open to creating new modules for the Engaging cluster. You can submit all module requests to orcd-help-engaging@mit.edu.</p>"},{"location":"faqs/#i-am-unable-to-install-a-package-in-r-how-can-i-debug-the-issue","title":"I am unable to install a package in R. How can I debug the issue?","text":"<p>We recommend using Conda to manage R packages. Please refer to the R user guide.</p>"},{"location":"faqs/#can-i-use-export-controlled-software-on-the-cluster","title":"Can I use export controlled software on the cluster?","text":"<p>Export controlled software has specific requirements around who is allowed to access the software. Often, our clusters do not meet these requirements, so we generally do not allow such software to be used on our systems. Please refer to the terms of use of the software and direct any questions to orcd-help@mit.edu.</p>"},{"location":"faqs/#how-do-i-increase-the-time-limit-for-my-job","title":"How do I increase the time limit for my job?","text":"<p>Use the <code>-t</code> flag in your job script. If you do not specify, Slurm will give you the maximum time limit for that partition. You can check the maximum time limit by running <code>sinfo -p &lt;partition name&gt;</code>.</p> <p>For public partitions on Engaging, such as <code>mit_normal</code>, we cannot increase the maximum job time limit, as these resources are shared. For jobs that need to run longer than the time limit, we encourage checkpointing, which is a way of periodically saving progress so that subsequent jobs can pick up where previous jobs left off. The implementation of checkpointing is domain specific and can vary greatly. You can find more information on checkpointing here.</p> <p>For increasing the maximum time limit on partitions owned by other groups, please email orcd-help-engaging@mit.edu.</p>"},{"location":"faqs/#how-do-i-get-an-account","title":"How do I get an account?","text":"EngagingSuperCloud <p>If you have an MIT Kerberos account, then you can get an account on Engaging. To register, navigate to the Engaging OnDemand Portal and log in.</p> <p>Access to SuperCloud is more restrictive and the account generation process is more involved. For more information, see the SuperCloud documentation.</p>"},{"location":"faqs/#how-do-i-install-a-python-package","title":"How do I install a Python package?","text":"<p>See our documentation on Python.</p>"},{"location":"faqs/#why-wont-my-application-run-on-a-different-partition","title":"Why won't my application run on a different partition?","text":"<p>On Engaging, the older nodes (such as the <code>sched_mit_hill</code> and <code>newnodes</code> partitions) run on CentOS 7 while the newer nodes (such as <code>mit_normal</code> and <code>mit_preemptable</code>) run on the Rocky 8 operating system (OS). Each set of nodes has a different set of modules, so if you have set up software to run on one OS, it will probably not work on the other OS.</p>"},{"location":"faqs/#how-do-i-run-jupyter-notebooks","title":"How do I run Jupyter notebooks?","text":"<p>You can run Jupyter a few different ways:</p> <ol> <li>Web portal for the cluster you're using</li> <li>VS Code</li> <li>Port forwarding</li> </ol> <p>See our Jupyter documentation.</p>"},{"location":"faqs/#xfce-desktop-has-failed-to-start-how-can-i-fix-this","title":"Xfce desktop has failed to start. How can I fix this?","text":"<p>This issue is often caused by Conda setup commands existing in your <code>~/.bashrc</code> file. This happens when you run <code>conda init</code> when using Miniforge or another Anaconda install. We recommend not running <code>conda init</code> as it can lead to errors such as this one.</p> <p>To fix this, remove or comment out all conda setup commands from your <code>~/.bashrc</code> file.</p>"},{"location":"faqs/#how-do-i-use-git-on-the-cluster","title":"How do I use Git on the cluster?","text":"<p>Git is highly encouraged for use on the cluster. It is useful for backing up code and version control, especially when collaborating with others.</p> <p>We recommend setting up an SSH key with GitHub for security and convenience. This allows you to use the \"SSH\" link rather than the \"HTTPS\" link when cloning repositories. To set up an SSH key, follow these steps:</p> <ol> <li> <p>SSH to the cluster you're using</p> </li> <li> <p>Enter the following from the command line:</p> <pre><code>ssh-keygen -t ed25519 -C \"$USER@mit.edu\"\n</code></pre> </li> <li> <p>Press \"enter\" to save your private and public keys to the default <code>~/.ssh</code> location. When prompted, optionally enter a passphrase for higher security. You will now have two new files in your <code>~/.ssh</code> directory: <code>id_ed25519</code> and <code>id_ed25519.pub</code>.</p> </li> <li> <p>Print the contents of your public key (using <code>cat id_ed25519.pub</code>) and copy the output</p> </li> <li> <p>Navigate to GitHub.com &gt; click your profile in the top right corner &gt; select \"Settings\" &gt; \"SSH and GPG keys\" &gt; \"New SSH key\"</p> </li> <li> <p>Add a title (e.g., \"engaging\"), paste your public key, and click \"Add SSH key\"</p> </li> </ol> <p>See GitHub's documentation on SSH keys for more information.</p>"},{"location":"faqs/#why-doesnt-my-password-work-when-i-try-to-run-the-sudo-command","title":"Why doesn't my password work when I try to run the sudo command?","text":"<p>Regular users are not allowed to use sudo on engaging. Engaging is a shared environment. Sudo enables root-level access which allows our system administrators to modify system files, install software and change permissions. If misused unintentionally or accidentally, it could compromise the entire cluster. Therefore, use of sudo is reserved for engaging system administrators who work to secure, maintain, and tune the cluster. If you need specific software and you are having difficultly installing it, contact orcd-help@mit.edu and someone on the staff can assist you. Please see <code>https://orcd-docs.mit.edu/software/overview/</code> for more information. </p>"},{"location":"faqs/#what-is-the-mit_preemptable-partition-what-is-preemption","title":"What is the <code>mit_preemptable</code> partition? What is preemption?","text":"<p>The <code>mit_preemptable</code> partition allows you to run programs on lab-owned nodes while they're not being used. While this partition has higher resource limits and longer runtimes than other public partitions like <code>mit_normal</code> and <code>mit_normal_gpu</code>, jobs submitted to <code>mit_preemptable</code> are low priority and preemptable. See Preemptable Jobs for more information.</p>"},{"location":"faqs/#i-got-locked-out-of-my-engaging-account-how-do-i-restore-my-access","title":"I got locked out of my Engaging account. How do I restore my access?","text":"<p>People often get locked out of their account due to repeated failed authentication attempts, specifically from Duo 2FA. This is usually caused by third-party software that connects to Engaging over SSH, such as VS Code. Your account will be automatically re-activated after a bit of time.</p>"},{"location":"faqs/#i-cannot-connect-to-a-compute-node-using-vs-code-remote-ssh","title":"I cannot connect to a compute node using VS Code remote SSH.","text":"<p>Sometimes, when following our instructions for running VS Code on the cluster, users are prompted to enter their password when they connect to the compute node and they get \"permission denied.\" This is most often because they do not have an SSH key set up on Engaging. You can do so following these instructions.</p>"},{"location":"getting-help/","title":"Getting Help","text":"","tags":["Getting Help"]},{"location":"getting-help/#additional-documentation","title":"Additional Documentation","text":"<p>Engaging documentation can be found on these pages. If you haven't found your answer for other systems elsewhere in these pages, your answer may be in the documentation for the system you are using:</p> <ul> <li>SuperCloud Documentation</li> <li>OpenMind Documentation</li> </ul>","tags":["Getting Help"]},{"location":"getting-help/#email","title":"Email","text":"<p>If you can't find your answer in the documentation, please use one of the email lists below to contact us. With the exception of SuperCloud, these lists will create a ticket that our team can assign and track. In all cases these mailing lists includes the entire team, so the best available person to answer your question will respond. Sending email to the entire team will also likely get you the fastest response. Please do not send email directly to individual team members.</p> <ul> <li>General ORCD Questions: orcd-help@mit.edu</li> <li>Engaging: orcd-help-engaging@mit.edu</li> <li>OpenMind: orcd-help-openmind@mit.edu</li> <li>SuperCloud: supercloud@mit.edu</li> </ul> <p>In this email, please provide, where applicable:</p> <ul> <li>Description of your issue or request</li> <li>The command that you used to launch your job</li> <li>Job ID(s)</li> <li>What you tried</li> <li>The full error message you are receiving</li> <li>Any supporting files (code, submission scripts, screenshots, etc)</li> </ul>","tags":["Getting Help"]},{"location":"getting-help/#office-hours","title":"Office Hours","text":"<p>We host weekly office hours. Office Hours are a time when you can drop in and ask us questions. It is a great time to discuss or troubleshoot something that is difficult over email. See the table below for the available Office Hours sessions.</p> Session Time Location Tuesday In Person Office Hours Tuesdays 10-11 am 46-4199 Thursday In Person Office Hours Thursdays 2-3 pm GIS and Data Lab in the Rotch Library (7-238) Friday Virtual Office Hours First, Third, and Fifth Fridays 2-3pm Zoom, email orcd-help@mit.edu for the link","tags":["Getting Help"]},{"location":"getting-started/","title":"Getting Started Tutorial","text":"<p>This page contains the most common steps for setting up and getting started with your ORCD system account. We provide this page as a convenient reference to get started. Each system has its own in-depth documentation which can be found on the ORCD Systems page.</p> <p>Sections that are system-specific will be shown under a list of tabs. Click on the tab for the system you are using and the rest of the page will show the information for that system.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#getting-an-account","title":"Getting an Account","text":"<p>If you don't already have an account, click on the tab for the system you are interested in and follow the instructions.</p> EngagingSuperCloudOpenMind <p>Login into the respective OnDemand Portal https://engaging-ood.mit.edu using your MIT kerberos credentials. The system will then be prompted to create your account automatically. Wait a couple of minutes for the system to create all the pieces for your account before submitting your first job.</p> <p>Follow the instructions on the Account Request Page.</p> <p>OpenMind will be retired later this year. BCS users are encouraged to transition their workflows to Engaging, where they can use the <code>ou_bcs_low</code>, <code>ou_bcs_normal</code>, and <code>ou_bcs_high</code> partitions for access to GPUs. All other MIT users can access these resources through the <code>mit_preemptable</code> partition on Engaging.</p> <p>If you are in BCS and you would still like an account on OpenMind, see the Getting an Account page on OpenMind's documentation..</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#logging-in","title":"Logging In","text":"<p>The first thing you should do when you get a new account is verify that you can log in. The different ORCD systems provide multiple ways to log in, including both ssh and web portals. Links to instructions for the different systems are below.</p> SuperCloudOpenMind <p>See the Logging into SuperCloud page for full documentation.</p> <p>See the Logging into OpenMind page for full documentation.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#terminal-with-ssh","title":"Terminal with SSH","text":"EngagingSuperCloudOpenMind <p>Log into Engaging with the following command in a terminal window. Replace <code>USERNAME</code> below with your MIT Kerberos username:</p> <p><pre><code>ssh USERNAME@orcd-login.mit.edu\n</code></pre> You will be prompted for your Kerberos password and then for Duo two-factor authentication.</p> <p>If you are using older Centos 7 nodes you can use one of the Centos 7 login nodes instead:</p> <ul> <li><code>orcd-vlogin001</code></li> <li><code>orcd-vlogin002</code></li> <li><code>orcd-vlogin003</code></li> <li><code>orcd-vlogin004</code></li> </ul> <p>See Logging in with SSH for more information.</p> <p>In order to log into SuperCloud with ssh you will need to add ssh keys to your account on the Web Portal. Follow the instructions on the SuperCloud Getting Started page to add your keys.</p> <p>Then you can log in with ssh using the following command, where <code>USERNAME</code> is your username on the MIT SuperCloud system:</p> <pre><code>ssh USERNAME@txe1-login.mit.edu\n</code></pre> <p>Log into OpenMind with the following command in a terminal window. Replace <code>USERNAME</code> below with your Kerberos username.</p> <pre><code>ssh USERNAME@openmind.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password. You can add an ssh key if you do not want to enter your Kerberos password at login.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#web-portal","title":"Web Portal","text":"EngagingSuperCloudOpenMind <p>You can log into OnDemand Web Portal with the link: https://engaging-ood.mit.edu. For full detailed instructions please see the OnDemand Documentation.</p> <p>You can log into the SuperCloud Web Portal with the link: https://txe1-portal.mit.edu. For full detailed instructions please see the SuperCloud Documentation.</p> <p>OpenMind does not have a web portal. However, check out OpenMind's documentation on the FastX Remote Desktop. You may find it provides what you are looking for.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#shared-hpc-clusters","title":"Shared HPC Clusters","text":"<p>Each ORCD system is a shared HPC cluster. You are sharing this resources with a number of other researchers, staff, and students, so it is important that you read this page and use the system as intended.</p> <p>Being a cluster, there are several machines connected together with a network. We refer to these as nodes. Most nodes in the cluster are referred to as compute nodes, this is where the computation is done on the system (where you will run your code). When you ssh into the system you are on a special purpose node called the login node. The login node, as its name suggests, is where you log in and is for editing code and files, installing packages and software, downloading data, and starting jobs to run your code on one of the compute nodes.</p> <p>Each job is started using a piece of software called the scheduler, which you can think of as a resource manager. You let it know what resources you need and what you want to run, and the scheduler will find those resources and start your job on them. When your job completes those resources are relinquished. The scheduler is what ensures that no two jobs are using the same resources, so it is very important not to run anything unless it is submitted properly through the scheduler.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#software-and-packages","title":"Software and Packages","text":"<p>The first thing you may want to do is make sure the system has the software and packages you need. We have installed a lot of software and packages on the system already, even though it may not be immediately obvious that it is there. Review the page for the system you are using paying particular attention to the section on modules and installing packages for the language that you use:</p> EngagingSuperCloudOpenMind <p>The Engaging Software documentation is available under the \"Software\" section on this site (see the sidebar on the left). We recommend reading through both the Overview and Modules pages, and then select the additional pages most relevant to you.</p> <p>SuperCloud Software Documentation Page</p> <p>OpenMind Software Documentation Page</p> <p>If you are ever unsure if we have a particular software, and you cannot find it, please send us an email and ask before you spend a lot of time trying to install it. If we have it, we can point you to it, provide advice on how to use it, and if we don't have it we can often give pointers on how to install it. Further, if a lot of people request the same software, we may consider adding it to the system image.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#linux-command-line","title":"Linux Command Line","text":"<p>Every ORCD system runs Linux, so much of what you do on the cluster involves the Linux command line. That doesn't mean you have to be a Linux expert to use the system! However the more you can get comfortable with the Linux command line and a handful of basic commands, the easier using the system will be. If you are already familiar with Linux, feel free to skip this section, or skim as a refresher.</p> <p>Most Linux commands deal with directories and files. A directory, synonymous to a folder, contains files and other directories. The list of directories that lead to a particular directory or file is called its path. In Linux, directories on a path are separated by forward slashes <code>/</code>. It is also important to note that everything in Linux is case sensitive, so a file <code>myScript.sh</code> is not the same as the file <code>myscript.sh</code>. When you first log in, you are in your home directory. Your home directory is where you can put all the code and data you need to run your job. Your home directory is not accessible to other users, so if you need a space to share files with other users, let us know and we can make a shared group directory for you.</p> EngagingSuperCloudOpenMind <p>The path to your home directory on Engaging is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on SuperCloud is <code>/home/gridsan/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on OpenMind is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>Anytime after you start typing a Linux command you can press the \"Tab\" button your your keyboard. This called tab-complete, and will try to autocomplete what you are typing. This is particularly helpful when typing out long directory paths and file names. Pressing \"Tab\" once will complete if there is a single completion, pressing it twice will list all potential completions. It is a bit difficult to explain in text, but you can try it out yourself and watch the short demonstration here.</p> <p>Finally, click on the box below for a list of Linux Commands. If you are new to Linux try them out for yourself at the command line.</p> Common Linux Commands <ul> <li>Creating, navigating and viewing directories:<ul> <li><code>pwd</code>: tells you the full path of the directory you are     currently in</li> <li><code>mkdir dirname</code>: creates a directory with the name \"dirname\"</li> <li><code>cd dirname</code>: change directory to directory \"dirname\"<ul> <li><code>cd ../</code>: takes you up one level</li> </ul> </li> <li><code>ls</code>: lists the files in the directory<ul> <li><code>ls -a</code>: lists all files including hidden files</li> <li><code>ls -l</code>: lists files in \"long format\" including ownership     and date of last update</li> <li><code>ls -t</code>: lists files by date stamp, most recently updated     file first</li> <li><code>ls -tr</code>: lists files by dates stamp in reverse order, most     recently updated file is listed last (this is useful if you     have a lot of files, you want to know which file you changed     last and the list of files results in a scrolling window)</li> <li><code>ls dirname</code>: lists the files in the directory \"dirname\"</li> </ul> </li> </ul> </li> <li>Viewing files<ul> <li><code>more filename</code>: shows the first part of a file, hitting the     space bar allows you to scroll through the rest of the file, q     will cause you to exit out of the file.</li> <li><code>less filename</code>: allows you to scroll through the file, forward     and backward, using the arrow keys.</li> <li><code>tail filename</code>: shows the last 10 lines of a file (useful when     you are monitoring a log file or output file to see that the     values are correct)<ul> <li><code>tail &lt;number&gt; filename</code>: show you the last &lt;number&gt;     lines of a file.</li> <li><code>tail -f filename</code>: shows you new lines as they are written     to the end of the file. Press CMD+C or Control+C to exit.     This is helpful to monitor the log file of a batch job.</li> </ul> </li> </ul> </li> <li>Copying, moving, renaming, and deleting files<ul> <li><code>mv filename dirname</code>: moves filename to directory dirname.<ul> <li><code>mv filename1 filename2</code>: moves filename1 to filename2, in     essence renames the file. The date and time are not changed     by the mv command.</li> </ul> </li> <li><code>cp filename dirname</code>: copies to directory dirname.<ul> <li><code>cp filename1 filename2</code>: copies filename1 to filename2. The     date stamp on filename2 will be the date/time that the file     was moved</li> <li><code>cp -r dirname1 dirname2</code>: copies directory dirname1 and its     contents to dirname2.</li> </ul> </li> <li><code>rm filename</code>: removes (deletes) the file</li> </ul> </li> </ul>","tags":["Getting Started","Linux"]},{"location":"getting-started/#transferring-files","title":"Transferring Files","text":"<p>One of the first tasks is to get your code, data, and any other files you need into your home directory on the system. If your code is in github you can use git commands on the system to clone your repository to your home directory. You can also transfer your files to your home directory from your computer by using the commands <code>scp</code> or <code>rsync</code>. Read the page on Transferring Files for the system you are using to learn how to use these commands and transfer what you need to your home directory.</p> <p>You can use <code>scp</code> or <code>rsync</code> from the command line on your local computer for any ORCD system. Both commands work similarly to the <code>cp</code> command, following the pattern <code>&lt;command&gt; &lt;source&gt; &lt;destination&gt;</code>, the only difference being that you will need to include the hostname of the system you are transferring to or from. For this reason you must run this command from the terminal on your computer before you've logged in.</p> <p>To transfer a file from your computer to the ORCD system:</p> EngagingSuperCloudOpenMind <pre><code>scp &lt;file-name&gt; USERNAME@orcd-login001.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above. Note that you will need to authenticate with Duo)</p> <pre><code>scp &lt;local-file-name&gt; USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <p>To transfer a file from an ORCD system to your computer:</p> EngagingSuperCloudOpenMind <pre><code>scp USERNAME@orcd-login001.mit.edu:&lt;path-to-engaging-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <pre><code>scp USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>Similar to <code>cp</code>, use the <code>-r</code> flag to copy over an entire directory and its contents. </p> EngagingSuperCloudOpenMind <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@orcd-login001.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <p>The <code>rsync</code> command can be used similarly and has some additional flags you can use. It also can be used to transfer only new or modified files to the destination, which makes it easy to keep a directory in \"sync\".</p> <p>For more information on transferring files and additional methods please see the Transferring Files page.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#running-your-first-job","title":"Running your First Job","text":"<p>At this point you may want to do a test-run of your code. You always want to start small in your test runs, so you should choose a small example that tests the functionality of what you would ultimately like to run on the system. If your test code is serial and runs okay on a moderate personal laptop or desktop you can request an interactive session to run your code in by executing the command:</p> EngagingSuperCloudOpenMind <pre><code># Requesting a single core for an interactive job for 1 hour\nsalloc -t 01:00:00 -p mit_normal\n</code></pre> <pre><code># Requesting a single core for an interactive job\nLLsub -i\n</code></pre> <pre><code># Requesting a single core for an interactive job for 1 hour\nsrun -n 1 -t 01:00:00  --pty bash  \n</code></pre> <p>After you run this command you will be on a compute node and you can do a test-run of your code. This command will allocate one core to your job. If your test code is multithreaded or parallel, uses a lot of memory, or requires a GPU you should request additional resources as needed. Not requesting the resources you will be using can negatively impact others on the system.</p> <p>Please see your system's documentation pages for more information on requesting more resources for running interactive jobs, and how to run batch jobs.</p> EngagingSuperCloudOpenMind <p>The Engaging documentation on running jobs is available under the \"Running Jobs\" section on this site (see the sidebar on the left). We recommend reading through both the Overview and Requesting Resources pages, and then select the additional pages most relevant to you.</p> <p>SuperCloud's Documentation for Running Jobs</p> <p>OpenMind's Documentation for Running Jobs</p>","tags":["Getting Started","Linux"]},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#apptainer-singularity","title":"Apptainer / Singularity","text":"<p>A container platform designed for high-performance computing (HPC) environments. It allows users to create and run containers that encapsulate their applications and dependencies, ensuring reproducibility and portability across different systems.</p>"},{"location":"glossary/#batch-job","title":"Batch Job","text":"<p>A type of job submitted to the scheduler to run in the background without direct interactive supervision. These are typically used for longer-running programs and are submitted using a script.</p>"},{"location":"glossary/#binary-executable-file","title":"Binary / Executable File","text":"<p>A pre-built program file that can be run directly without needing to be compiled from source code. These are often placed in the <code>bin</code> directory of a software installation.</p>"},{"location":"glossary/#cli-command-line-interface","title":"CLI (Command Line Interface)","text":"<p>A text-based user interface used to interact with software and operating systems by typing commands into a console or terminal.</p>"},{"location":"glossary/#conda-environment","title":"Conda Environment","text":"<p>A self-contained directory that includes a set of installed packages and libraries, managed by the Conda package manager. Conda is typically used to manage Python environments.</p>"},{"location":"glossary/#container","title":"Container","text":"<p>A lightweight, standalone, and executable software package that includes everything needed to run a piece of software, including the code, runtime, libraries, and system tools. Containers are isolated from each other and the host system, allowing for consistent and reproducible environments across different computing platforms.</p>"},{"location":"glossary/#container-image","title":"Container Image","text":"<p>A static file that contains the complete filesystem and configuration needed to run a containerized application. It serves as a blueprint for creating containers.</p>"},{"location":"glossary/#compute-node","title":"Compute Node","text":"<p>The primary nodes within an HPC cluster where the actual computational work of user jobs is performed.</p>"},{"location":"glossary/#computing-cluster","title":"Computing Cluster","text":"<p>A collection of interconnected computers (nodes) that work together to perform complex computations.</p>"},{"location":"glossary/#cpu-core","title":"CPU Core","text":"<p>The smallest unit of processing on a node that can be used by a job. In HPC, jobs can request a specific number of CPU cores to perform parallel tasks.</p>"},{"location":"glossary/#engaging","title":"Engaging","text":"<p>The main computing cluster managed by ORCD. This cluster is available to the entire MIT community and has a mixture of CPU and GPU resources.</p>"},{"location":"glossary/#environment","title":"Environment","text":"<p>The overall configuration and settings in which a program or job runs, including system variables, paths, and loaded modules.</p>"},{"location":"glossary/#environment-variable","title":"Environment Variable","text":"<p>Named values in your shell environment that store configuration information for the operating system and running programs. Examples include <code>$PATH</code> (which tells Linux where to look for executable files) or <code>$SLURM_ARRAY_JOB_ID</code> (set by the scheduler during job execution).</p>"},{"location":"glossary/#filesystem","title":"Filesystem","text":"<p>The method and structure by which files and directories are organized and stored on a computer. ORCD's clusters use a shared filesystem accessible by all nodes in the cluster.</p>"},{"location":"glossary/#globus","title":"Globus","text":"<p>A data management service that enables secure, reliable, and efficient transfer of files across multiple systems and locations.</p>"},{"location":"glossary/#gpu-graphics-processing-unit","title":"GPU (Graphics Processing Unit)","text":"<p>A specialized processor designed to accelerate graphics rendering and parallel processing tasks, commonly used in high-performance computing for tasks such as machine learning and simulations.</p>"},{"location":"glossary/#gui-graphical-user-interface","title":"GUI (Graphical User Interface)","text":"<p>A user interface that allows users to interact with electronic devices through graphical icons and visual indicators, as opposed to text-based interfaces, typed command labels, or text navigation.</p>"},{"location":"glossary/#hard-disk-drive-hdd","title":"Hard Disk Drive (HDD)","text":"<p>A type of storage device that uses spinning disks to read and write data. HDDs are commonly used for bulk storage in HPC environments, but they are slower than SSDs in terms of data access and transfer speeds.</p>"},{"location":"glossary/#home-directory","title":"Home Directory","text":"<p>A personal directory allocated to each user on the cluster, located at <code>/home/$USER</code>. This is the default working directory for users when they log in.</p>"},{"location":"glossary/#hpc-high-performance-computing","title":"HPC (High-Performance Computing)","text":"<p>The use of supercomputers and parallel processing techniques to solve complex computational problems that require significant processing power and memory.</p>"},{"location":"glossary/#interactive-job","title":"Interactive Job","text":"<p>A job that allows users to interact with the application or process while it is running, typically through a command line interface or graphical user interface.</p>"},{"location":"glossary/#io-inputoutput","title":"I/O (Input/Output)","text":"<p>The communication between a program and the filesystem, including reading from and writing to files. I/O performance can be a critical factor in the efficiency of HPC applications.</p>"},{"location":"glossary/#job","title":"Job","text":"<p>A unit of work submitted to the scheduler (e.g., Slurm) to be executed on the compute nodes of an HPC cluster</p>"},{"location":"glossary/#local-filesystem","title":"Local Filesystem","text":"<p>The storage system that is physically attached to a node, as opposed to shared or networked filesystems. Local filesystems typically offer faster access times but are limited to the storage capacity of the individual node.</p>"},{"location":"glossary/#login-node","title":"Login Node","text":"<p>A specialized node within an HPC cluster that users first connect to via SSH or a web portal. It is intended for tasks like file editing, package installation, data downloads, and submitting jobs to compute nodes, but not for heavy computations.</p>"},{"location":"glossary/#memory","title":"Memory","text":"<p>The hardware component (such as RAM or cache) where data and applications are temporarily stored for active use by the processor. Data in memory is volatile and is lost when the computer is powered off or restarted.</p>"},{"location":"glossary/#module","title":"Module","text":"<p>A software tool that allows users to easily manage different versions of software, libraries, and compilers in a shared computing environment. Users can load or unload modules to configure their environment for specific tasks.</p>"},{"location":"glossary/#mpi-message-passing-interface","title":"MPI (Message Passing Interface)","text":"<p>A standardized library of functions for parallel computing that enables data communication between processes running across multiple CPU cores or nodes. It's primarily used for distributed-memory parallelism, where each process has its own memory.</p>"},{"location":"glossary/#node","title":"Node","text":"<p>An individual computing unit within an HPC cluster, which can be a physical or virtual machine. Each node has its own CPU cores and memory.</p>"},{"location":"glossary/#openmp","title":"OpenMP","text":"<p>A shared-memory parallelism technique where different parts of a program run concurrently as \"threads\" within a single process, sharing the same memory space. OpenMP is a common standard for implementing multithreading.</p>"},{"location":"glossary/#partition","title":"Partition","text":"<p>A logical grouping of compute nodes within an HPC cluster, managed by the scheduler (Slurm). Different partitions may have varying CPU types, GPU availability, memory limits, or time limits, and jobs are submitted to specific partitions.</p>"},{"location":"glossary/#pool-storage","title":"Pool storage","text":"<p>A storage space on Engaging that is meant for handling large data sets but is not optimized for high I/O. Typically, pool storage is hosted on HDDs.</p>"},{"location":"glossary/#process","title":"Process","text":"<p>An instance of a running program, which can consist of one or more threads. Each process has its own memory space.</p>"},{"location":"glossary/#python-virtual-environment","title":"Python Virtual Environment","text":"<p>A self-contained directory that includes a set of installed Python packages. This allows users to manage dependencies and avoid conflicts between different projects.</p>"},{"location":"glossary/#scratch-storage","title":"Scratch Storage","text":"<p>Temporary, high-speed storage areas optimized for I/O-heavy jobs. Data stored here is typically not backed up and is meant for actively running jobs rather than long-term storage. Scratch storage is hosted on SSDs.</p>"},{"location":"glossary/#slurm-scheduler","title":"Slurm / Scheduler","text":"<p>An open-source workload manager designed for high-performance computing (HPC) clusters. It is used to allocate resources, schedule jobs, and manage job queues on the cluster.</p>"},{"location":"glossary/#solid-state-drive-ssd","title":"Solid State Drive (SSD)","text":"<p>A type of storage device that uses flash memory to provide faster data access and improved reliability compared to traditional hard disk drives (HDDs).</p>"},{"location":"glossary/#terminal-shell-command-line-console","title":"Terminal / Shell / Command Line / Console","text":"<p>A text-based interface used to interact with the operating system and run commands.</p>"},{"location":"glossary/#thread","title":"Thread","text":"<p>A smaller unit of a process that can run concurrently with other threads within the same process, sharing the same memory space.</p>"},{"location":"hosted-hardware/","title":"Hardware Purchases","text":"<p>There are often cases where the public partitions (<code>mit_preemptable</code>, <code>mit_normal</code> and <code>mit_normal_gpu</code>) are not sufficient for a lab's computational needs.  In these cases we often recommend a hardware purchase to be hosted on Engaging. </p> <p>Our recommendations are generally vendor agnostic. However, specific vendors give us very compelling pricing based  on negotiated volume discounts. To keep maintenance costs lower, we strive to standardize on hardware that  is easily installed into Engaging. This allows our operations team to manage far more hardware, more efficiently, than a cluster with heterogenous hardware. There are some variations of hardware for special edge cases, but they are exceedingly rare. </p> <p>ORCD staff maintains relationships with multiple hardware vendors and is consistently negotiating for better pricing that is generally not available to lower volume customers. </p>","tags":["Maintenance Schedule"]},{"location":"hosted-hardware/#process","title":"Process","text":"<p>We prefer to meet with the group and learn about the type of work that is being done on the nodes and make recommendations.  Next, we ask the group to provide a budget and a timeframe. We use this information to work with our vendors to get quotes. Once the quotes have been agreed on, we forward them to the lab's administrator for purchase. For tracking purposes, the administrator forwards us the purchase order number once the order has been placed. On arrival to MGHPCC in Holyoke, hardware is carefully checked and installed in the racks. Operating system and networking and monitoring are installed and tested by the ORCD operations staff. Once completed, the Research Computing Support team runs a series of tests to confirm all is working as expected and then the hardware is released for use. </p>","tags":["Maintenance Schedule"]},{"location":"hosted-hardware/#maintenance","title":"Maintenance","text":"<p>Once your hardware is in place, our staff will take care of all maintenance of the operating system, patches, security, access to the network,  and access to the shared storage resources. Our operations staff will coordinate any hardware repairs with the vendor and will work with the vendor to make repairs as appropriate with the warranty. A five year warranty is required. These nodes would be part of the regular monthly maintenance.</p>","tags":["Maintenance Schedule"]},{"location":"hosted-hardware/#slurm-partitions","title":"Slurm Partitions","text":"<p>Scientists may specify the configuration of their partitions within reason. Generally the partition time limit is constrained only by the engaging maintenance schedule, which is one day per month (the third Tuesday of each month). All machines are required to be in the mit_preemtable queue, which allows them to be used by non owners when the hardware is idle. We generally start with a standard popular slurm partition configuration and then work with the lab to modify as needed. </p> <p>Once the nodes have been tested, they are released for use. You may grant users access via MIT Moira. Your new nodes will have access controlled by your user group. To give users access to your nodes, the corresponding usergroup must be modified. For example: </p> <p>orcd_ug_pi_[pikerb]_all</p> <p>You can make the additions here on the WebMoira page.</p>","tags":["Maintenance Schedule"]},{"location":"hosted-hardware/#testing-and-burn-in","title":"Testing and burn in","text":"<p>ORCD staff maintains several testing suites to confirm that nodes are working as expected. We run these tests on all new hardware to confirm there are no problems before releasing the hardware for general use.</p>","tags":["Maintenance Schedule"]},{"location":"hosted-hardware/#retirement","title":"Retirement","text":"<p>Hardware is retired after 5 years when the warranty period is over. The nodes are removed from the racks and recycled responsibly. </p>","tags":["Maintenance Schedule"]},{"location":"orcd-systems/","title":"ORCD Systems","text":"<p>ORCD operates and provides support and training for a number of cluster computer systems available to all researchers. These systems all run with a Slurm scheduler and most have a web portal for interactive computing. These are Engaging, SuperCloud, and OpenMind.</p>","tags":["Engaging","SuperCloud"]},{"location":"orcd-systems/#maintenance-schedule","title":"Maintenance Schedule","text":"<p>With the exception of SuperCloud, the maintenance schedule for all ORCD systems is:</p> <ul> <li>Monthly downtimes on the 3rd Tuesday of the month lasting about a day.</li> <li>Weekly restarts of login nodes Monday mornings starting at 7am for about 15 minutes. If Monday is a holiday this restart will occur on Tuesday.</li> </ul> <p>SuperCloud has monthly downtimes on the 2nd Tuesday of each month.</p>","tags":["Engaging","SuperCloud"]},{"location":"orcd-systems/#engaging-cluster","title":"Engaging Cluster","text":"<p>The Engaging is the ORCD primary HPC resource and is a mixed CPU and GPU computing cluster that is openly available to all  research projects at MIT. It has around 50,000 x86 CPU cores and over 1000  GPU cards including A100, RTX6000, H100, H200 and L40S. New, modern hardware is consistently being added to the Engaging cluster. Engaging offers fast flash based storage for each user's scratch space. Hardware access is through the Slurm  resource scheduler that supports batch and interactive workloads and allows dedicated reservations. The cluster has a large shared file system for working datasets. Additional compute and storage  resources can be purchased by PIs. A wide range of standard software is available and the Docker  compatible Apptainer container tool is supported. User-level tools like Miniforge for Python,  R libraries, and Julia packages are all supported. A range of PI group maintained custom software  stacks are also available through the widely adopted environment modules toolkit. A standard,  open-source, web-based portal supporting Jupyter notebooks, R studio, Mathematica, and X graphics  is available at https://engaging-ood.mit.edu. Further information and support is available from orcd-help-engaging@mit.edu.</p>","tags":["Engaging","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-engaging","title":"How to Get an Account on Engaging","text":"<p>Accounts on the engaging cluster are connected to your main MIT institutional kerberos id.  Connecting to the cluster for the first time through its web portal automatically activates an account with basic access to resources. See this page for instructions on how to log in.</p>","tags":["Engaging","SuperCloud"]},{"location":"orcd-systems/#engaging-quick-links","title":"Engaging Quick Links","text":"<ul> <li>OnDemand web portal: https://engaging-ood.mit.edu</li> <li>Help: Send email to orcd-help-engaging@mit.edu</li> </ul>","tags":["Engaging","SuperCloud"]},{"location":"orcd-systems/#supercloud","title":"SuperCloud","text":"<p>The SuperCloud system is a collaboration with MIT Lincoln Laboratory on a shared facility that  is optimized for streamlining open research collaborations with Lincoln Laboratory (e.g.,  AIA, BW, CQE, Haystack, HPEC, ISN). The facility is open to those with a Lincoln Laboratory collaboration. The latest SuperCloud system has more than 16,000 x86 CPU cores  and more than 850 NVidia Volta GPUs in total. Hardware access is through the Slurm resource scheduler that supports batch and interactive workloads and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. User-level tools like Anaconda for Python, R libraries, and Julia packages are all supported. A custom, web-based portal supporting Jupyter notebooks is available at https://txe1-portal.mit.edu/. Further information and support is available at supercloud@mit.edu.</p>","tags":["Engaging","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-supercloud","title":"How to Get an Account on SuperCloud","text":"<p>To request a SuperCloud account follow the instructions on SuperCloud's Requesting an Account page.</p>","tags":["Engaging","SuperCloud"]},{"location":"orcd-systems/#supercloud-quick-links","title":"SuperCloud Quick Links","text":"<ul> <li>Documentation: https://supercloud.mit.edu/</li> <li>Online Course: https://learn.llx.edly.io/course/practical-hpc/</li> <li>Web portal: https://txe1-portal.mit.edu/</li> <li>Help: Send email to supercloud@mit.edu</li> </ul>","tags":["Engaging","SuperCloud"]},{"location":"orcd-systems/#openmind","title":"OpenMind","text":"<p>The OpenMind system is a collaboration with Department of Brain and Cognitive Sciences (BCS) and McGovern Institute. OpenMind is mainly a GPU computing cluster optimized for artificial intelligence (AI) research and data science. Totally there are around 70 compute nodes, 3500 CPU cores, 48 TB of RAM, and 340 GPUs, including 142 A100-80GB GPUs. It also provides around 2 PB of flash storage supporting fast read/write data speed. Hardware access is through the Slurm resource scheduler that supports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and Docker compatible Apptainer/Singularity container tool is supported. User-level tools like Anaconda for Python, R libraries, and Julia packages are all supported. OpenMind is slated for retirement, users are moving to the Engaging cluster.  Further information and support is available at orcd-help-openmind@mit.edu.</p>","tags":["Engaging","SuperCloud"]},{"location":"orcd-systems/#openmind-quick-links","title":"OpenMind Quick Links","text":"<ul> <li>Documentation: https://github.mit.edu/MGHPCC/OpenMind/wiki</li> <li>Home Page and Online Course: https://openmind.mit.edu/</li> <li>Help: Send email to orcd-help-openmind@mit.edu</li> <li>Slack: https://openmind-46.slack.com</li> </ul>","tags":["Engaging","SuperCloud"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#tag:apptainer","title":"Apptainer","text":"<ul> <li>            Containers          </li> </ul>"},{"location":"tags/#tag:best-practices","title":"Best Practices","text":"<ul> <li>            Jupyter          </li> <li>            PyCharm          </li> <li>            VSCode Remote SSH          </li> </ul>"},{"location":"tags/#tag:c","title":"C","text":"<ul> <li>            Intel compiler          </li> </ul>"},{"location":"tags/#tag:cc","title":"C/C++","text":"<ul> <li>            Compiling Codes          </li> </ul>"},{"location":"tags/#tag:engaging","title":"Engaging","text":"<ul> <li>            Advanced Job Array          </li> <li>            AlphaFold 3          </li> <li>            Compiling Codes          </li> <li>            Containers          </li> <li>            GROMACS          </li> <li>            Julia          </li> <li>            MPI Jobs          </li> <li>            Mujoco          </li> <li>            ORCA          </li> <li>            ORCD Systems          </li> <li>            PyTorch on GPUs - I          </li> <li>            PyTorch on GPUs - II          </li> <li>            RAG          </li> <li>            VASP          </li> </ul>"},{"location":"tags/#tag:fortran","title":"Fortran","text":"<ul> <li>            Intel compiler          </li> </ul>"},{"location":"tags/#tag:gpu","title":"GPU","text":"<ul> <li>            GROMACS          </li> <li>            NVHPC with CUDA aware MPI          </li> <li>            PyTorch on GPUs - I          </li> <li>            PyTorch on GPUs - II          </li> <li>            RAG          </li> </ul>"},{"location":"tags/#tag:gromacs","title":"GROMACS","text":"<ul> <li>            GROMACS          </li> </ul>"},{"location":"tags/#tag:getting-help","title":"Getting Help","text":"<ul> <li>            Getting Help          </li> </ul>"},{"location":"tags/#tag:getting-started","title":"Getting Started","text":"<ul> <li>            Getting Started Tutorial          </li> </ul>"},{"location":"tags/#tag:h100","title":"H100","text":"<ul> <li>            Getting started on 8-way H100 nodes on Satori          </li> </ul>"},{"location":"tags/#tag:howto-recipes","title":"Howto Recipes","text":"<ul> <li>            Advanced Job Array          </li> <li>            AlphaFold 3          </li> <li>            GROMACS          </li> <li>            Getting started on 8-way H100 nodes on Satori          </li> <li>            Intel compiler          </li> <li>            MPI Jobs          </li> <li>            MPI for Python          </li> <li>            Mujoco          </li> <li>            NVHPC with CUDA aware MPI          </li> <li>            PyCharm          </li> <li>            PyTorch on GPUs - I          </li> <li>            PyTorch on GPUs - II          </li> <li>            RAG          </li> <li>            Relion          </li> <li>            VASP          </li> <li>            VSCode Remote SSH          </li> </ul>"},{"location":"tags/#tag:install-recipe","title":"Install Recipe","text":"<ul> <li>            GROMACS          </li> <li>            Mujoco          </li> <li>            ORCA          </li> <li>            Relion          </li> <li>            VASP          </li> </ul>"},{"location":"tags/#tag:julia","title":"Julia","text":"<ul> <li>            Julia          </li> </ul>"},{"location":"tags/#tag:jupyter","title":"Jupyter","text":"<ul> <li>            Jupyter          </li> </ul>"},{"location":"tags/#tag:llm","title":"LLM","text":"<ul> <li>            RAG          </li> </ul>"},{"location":"tags/#tag:linux","title":"Linux","text":"<ul> <li>            Getting Started Tutorial          </li> </ul>"},{"location":"tags/#tag:logging-in","title":"Logging In","text":"<ul> <li>            Logging in with SSH via Terminal          </li> <li>            SSH Control Channels          </li> </ul>"},{"location":"tags/#tag:logging-in-with-ondemand","title":"Logging in with OnDemand","text":"<ul> <li>            Logging in with OnDemand          </li> </ul>"},{"location":"tags/#tag:mpi","title":"MPI","text":"<ul> <li>            GROMACS          </li> <li>            MPI Jobs          </li> <li>            MPI for Python          </li> <li>            NVHPC with CUDA aware MPI          </li> <li>            Relion          </li> <li>            VASP          </li> </ul>"},{"location":"tags/#tag:maintenance-schedule","title":"Maintenance Schedule","text":"<ul> <li>            Hardware Purchases          </li> </ul>"},{"location":"tags/#tag:modules","title":"Modules","text":"<ul> <li>            Modules          </li> </ul>"},{"location":"tags/#tag:nvhpc","title":"NVHPC","text":"<ul> <li>            NVHPC with CUDA aware MPI          </li> </ul>"},{"location":"tags/#tag:orca","title":"ORCA","text":"<ul> <li>            ORCA          </li> </ul>"},{"location":"tags/#tag:openmind","title":"OpenMind","text":"<ul> <li>            Compiling Codes          </li> <li>            Containers          </li> <li>            MPI for Python          </li> </ul>"},{"location":"tags/#tag:physics","title":"Physics","text":"<ul> <li>            Mujoco          </li> </ul>"},{"location":"tags/#tag:pytorch","title":"PyTorch","text":"<ul> <li>            PyTorch on GPUs - I          </li> <li>            PyTorch on GPUs - II          </li> </ul>"},{"location":"tags/#tag:python","title":"Python","text":"<ul> <li>            MPI for Python          </li> </ul>"},{"location":"tags/#tag:r","title":"R","text":"<ul> <li>            R          </li> </ul>"},{"location":"tags/#tag:relion","title":"RELION","text":"<ul> <li>            Relion          </li> </ul>"},{"location":"tags/#tag:rocky-linux","title":"Rocky Linux","text":"<ul> <li>            VASP          </li> </ul>"},{"location":"tags/#tag:ssh-key-setup","title":"SSH Key Setup","text":"<ul> <li>            SSH Key Setup          </li> </ul>"},{"location":"tags/#tag:singularity","title":"Singularity","text":"<ul> <li>            Containers          </li> </ul>"},{"location":"tags/#tag:slurm","title":"Slurm","text":"<ul> <li>            Advanced Job Array          </li> </ul>"},{"location":"tags/#tag:software","title":"Software","text":"<ul> <li>            Compiling Codes          </li> <li>            Containers          </li> <li>            Julia          </li> <li>            Modules          </li> <li>            Overview          </li> <li>            Python          </li> <li>            R          </li> </ul>"},{"location":"tags/#tag:supercloud","title":"SuperCloud","text":"<ul> <li>            GROMACS          </li> <li>            Julia          </li> <li>            Mujoco          </li> <li>            ORCD Systems          </li> </ul>"},{"location":"tags/#tag:vasp","title":"VASP","text":"<ul> <li>            VASP          </li> </ul>"},{"location":"tags/#tag:cuda","title":"cuda","text":"<ul> <li>            NVHPC with CUDA aware MPI          </li> </ul>"},{"location":"tags/#tag:cuda-aware-mpi","title":"cuda aware mpi","text":"<ul> <li>            NVHPC with CUDA aware MPI          </li> </ul>"},{"location":"tags/#tag:python","title":"python","text":"<ul> <li>            Python          </li> </ul>"},{"location":"tags/#tag:vscode","title":"vscode","text":"<ul> <li>            Julia          </li> </ul>"},{"location":"transition-guide-satori/","title":"Satori to Engaging Transition Guide","text":"<p>Satori will be retired at the end of October. Engaging has many GPUs available and we encourage all Satori users to migrate their work to Engaging. The ORCD team is here to help make that transition as smooth as possible.</p> <p>This page is maintained to answer questions and document how to migrate to ORCD's Engaging system.</p>"},{"location":"transition-guide-satori/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"transition-guide-satori/#what-is-the-timeline","title":"What is the timeline?","text":"<ul> <li>October 15: Home and nobackup storage on Satori made read-only</li> <li>October 31: Satori retired</li> </ul>"},{"location":"transition-guide-satori/#how-do-i-migrate-to-another-system","title":"How do I migrate to another system?","text":"<p>There are three main steps or milestones for moving to a new system. These will be similar to the steps you take when getting a new account on any system.</p> <ul> <li>Transfer data: Identify what data you need to keep and transfer it to the new system. See the Migrating Data from Satori section below for recommended ways to transfer large amounts of data.</li> <li>Build any missing software: Check the software stack of the new system and see what you may need to rebuild or request. Engaging maintains a software stack through modules that includes many of the software packages provided on Satori. Names may be different, so run <code>module avail</code> to check for differences.</li> <li>Run jobs: Run small tests before running larger production jobs. Both Satori and Engaging use Slurm, but have different partition names. Some <code>sbatch</code> flags may differ between the two systems as well. You can also consult the section on Running Jobs in this documentation.</li> </ul>"},{"location":"transition-guide-satori/#where-can-i-get-help","title":"Where can I get help?","text":"<p>The ORCD team can help with migrating data and workloads to Engaging. ORCD has regular office hours (see the Office Hours Schedule). You can also request help through orcd-help@mit.edu.</p>"},{"location":"transition-guide-satori/#how-can-i-get-an-account-on-engaging","title":"How can I get an account on Engaging?","text":"<p>It is fairly quick and simple to create your account Engaging. Accounts on the engaging cluster are connected to your MIT institutional Kerberos id. To get an account log into the Engaging OnDemand Web Portal. Connecting to Engaging OnDemand for the first time automatically activates an account with basic access to resources. See this page for instructions on how to log in. After you log in wait a few minutes for your account setup to complete before starting to run jobs.</p>"},{"location":"transition-guide-satori/#my-group-has-our-own-partition-on-satori-what-will-happen-to-those-nodes","title":"My group has our own partition on Satori, what will happen to those nodes?","text":"<p>There are three group partitions on Satori with newer nodes purchased by MIT PIs. These nodes will be migrated to Engaging. Any NESE storage will also be re-mounted on Engaging.</p>"},{"location":"transition-guide-satori/#how-can-i-use-my-python-virtual-environments-or-conda-environments-on-engaging","title":"How can I use my Python virtual environments or Conda environments on Engaging?","text":"<p>If you have Python virtual environments or Conda environments on Satori that you would like to use on Engaging, we recommend recreating them on Engaging. Simply copying over environment files is not expected to work due to setup differences between the two clusters.</p> <p>For both types of environments, the general process is to save the environment specifications to a yaml or requirements.txt file to be used to recreate the environment on Engaging.</p> <p>Note</p> <p>For more information on using Python on Engaging, see our Python documentation.</p>"},{"location":"transition-guide-satori/#python-virtual-environments","title":"Python virtual environments","text":"<p>Save contents of python venv to a <code>requirements.txt</code> file:</p> <pre><code>module load anaconda3\nsource /path/to/my_venv/bin/activate\npip freeze &gt; my_venv_requirements.txt\n</code></pre> <p>Transfer file to Engaging:</p> <pre><code>scp my_venv_requirements.txt &lt;username&gt;@orcd-login001.mit.edu:/path/to/dest\n</code></pre> <p>On Engaging, your <code>requirements.txt</code> file should now appear in the directory you specified. Recreate it with the following commands:</p> <pre><code>module load miniforge\npython -m venv my_venv\nsource my_venv/bin/activate\npip install -r my_venv_requirements.txt\n</code></pre> <p>Note</p> <p>The Python version in Satori's Anaconda module is different than the version provided by the Miniforge module on Engaging. If this is a problem, we recommend creating a Conda environment with your desired version of Python installed.</p>"},{"location":"transition-guide-satori/#conda-environments","title":"Conda environments","text":"<p>You can check all Conda environments you have created using <code>conda info --envs</code>. As Conda environments can take up lots of space, we recommend going through and choosing to transfer only the environments that you are still using.</p> <p>For each Conda environment you'd like to transfer, take the following steps:</p> <p>On Satori:</p> <pre><code>module load anaconda3\nconda activate my_env\nconda env export --no-builds | grep -v \"^prefix: \" &gt; my_env.yml\n</code></pre> <p>Transfer <code>.yml</code> file to Engaging:</p> <pre><code>scp my_env.yml &lt;username&gt;@orcd-login001.mit.edu:/path/to/dest\n</code></pre> <p>Recreate environment on Engaging:</p> <pre><code>module load miniforge\nconda env create -f my_env.yml\n</code></pre>"},{"location":"transition-guide-satori/#migrating-data-from-satori","title":"Migrating Data from Satori","text":"<p>This section describes some recipes for migrating data from Satori to Engaging, but some advice will apply to other systems.</p>"},{"location":"transition-guide-satori/#step-1-what-to-transfer","title":"Step 1: What to Transfer","text":"<p>First, this is a good opportunity to decide what you need and what you don't need. Take a look at your home and nobackup directories and decide what you need to keep. Transferring a lot of data (more than a few TB) or files (order of 1 million files) can take a long time. </p> <p>Remove files carefully</p> <p>Remove files you no longer need very carefully. Remember, <code>rm</code> on Linux is permanent and the Satori storage is not backed up!</p>"},{"location":"transition-guide-satori/#step-2-where-to-transfer","title":"Step 2: Where to Transfer","text":"<p>Next figure out where you are going to transfer the data. Engaging has some base storage described here, with additional storage available for purchase.</p> <p>For long-term archival storage of data that you need to keep, but will never or rarely need to access, consider purchasing archival storage such as AWS Glacier. For storage that you may need to access, but don't need to compute, MIT IS&amp;T provides some storage options for MIT students, staff, and faculty.</p>"},{"location":"transition-guide-satori/#step-3-transfer-your-data","title":"Step 3: Transfer Your Data","text":"<p>If you don't have a lot of data to transfer you can use <code>scp</code> or <code>rsync</code> to transfer files at the command line. Log into either system and run your <code>scp</code> or <code>rsync</code> command from there. The best option is to do this on the download partition on Satori as a batch job. You would use a batch script that looks something like this:</p> transfer.sh<pre><code>#!/bin/bash\n\nrsync -ruP path/to/source USERNAME@orcd-login001.mit.edu:/path/to/destination/satori-files/\n</code></pre> <p>Do not directly copy Satori home into Engaging home</p> <p>Be careful not to copy your Satori home directory directly into your Engaging home directory. Doing so may overwrite files in your Engaging home directory, including files such as <code>.bashrc</code> which can cause issues. Instead create a subdirectory and transfer your files there.</p> <p>If you have a lot of data and are having trouble with <code>rsync</code> or <code>scp</code> failing before the transfer is complete, Engaging and Satori have Globus collections that can make transfer easier. Globus will manage the file transfer for you to make sure everything transfers properly.</p>"},{"location":"transition-guide-satori/#using-globus-to-transfer-data","title":"Using Globus to Transfer Data","text":"<p>Both Engaging and Satori have Globus collections that you can use to migrate your data.</p> <p>To initiate the transfer go to https://www.globus.org/, log in, and click \"File Manager\".</p> <p>In the left pane search for the mithpc#satori collection. In the right pane search for the MIT ORCD Engaging Collection collection.</p> <p>If you need help finding the target directory on Satori or Engaging with Globus send an email to orcd-help@mit.edu or stop by office hours and we can help you.</p> <p>Once you have selected your target collection navigate to the directory where you want to transfer your data. Create a directory called \"Satori\" and select it.</p> <p>Select the items you want to transfer from Satori in the left pane, or \"Select all\" to transfer your entire home directory. Symlinks (to group directories) and their contents are not transferred.</p> <p>Deselect dot . files</p> <p>Click \"Show Hidden Items\" to deselect . files. Your .bashrc and any conda environments will not work on another system and could cause issues.</p> <p>Under Transfer and Timer options select:</p> <ul> <li>Skip files on source with errors</li> <li>Fail on quota errors</li> <li>Encrypt transfer</li> <li>Consider:<ul> <li>Excluding dot files (ex: .bashrc). Check \"Apply filter rules to the transfer\" and set to exclude all matching <code>.*</code> (see screenshot below)</li> <li>Sync, This is helpful if you've already transferred files and only want to transfer new files</li> </ul> </li> </ul> <p></p> <p>Once you have selected your source files, destination, and transfer settings click the \"Start\" button on the left pane (the Satori side). You can view the transfer progress on the \"Activity\" page, and Globus will send you an email when the transfer is done.</p>"},{"location":"transition-guide-supercloud/","title":"SuperCloud to Engaging Transition Guide","text":"<p>Updates to SuperCloud's access policies mean that starting May 1 access to SuperCloud will be limited to Lincoln Laboratory collaborators. As ORCD expands Engaging we welcome any MIT researchers who are not Lincoln collaborators and are looking for a place to run their computational workloads. The ORCD team is here to help make that transition as smooth as possible.</p> <p>This page is maintained to answer questions and document how to migrate to ORCD's Engaging system. We will continue to update this page with answers to more questions and documentation.</p>"},{"location":"transition-guide-supercloud/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"transition-guide-supercloud/#what-are-my-options","title":"What are my options?","text":"<p>There are other computing options available to the MIT community. ORCD runs the Engaging system which has both resources available to the entire MIT community and resources purchased by individual PIs and DLCs made available to their researchers.</p> <p>Some DLCIs maintain their own set of resources. See this page for a list.</p>"},{"location":"transition-guide-supercloud/#what-is-the-timeline","title":"What is the timeline?","text":"<ul> <li>Through April 30: All SuperCloud users can run jobs</li> <li>Starting May 1: Running jobs limited to Lincoln Collaborators</li> <li>Through May 31: Non-collaborators may continue to access SuperCloud for data migration</li> <li>June 1: SuperCloud access limited to Lincoln Collaborators</li> </ul>"},{"location":"transition-guide-supercloud/#how-do-i-migrate-to-another-system","title":"How do I migrate to another system?","text":"<p>There are three main steps or milestones for moving to a new system. These will be similar to the steps you take when getting a new account on any system.</p> <ul> <li>Transfer data: Identify what data you need to keep and transfer it to the new system. See the Migrating Data from SuperCloud section below for recommended ways to transfer large amounts of data.</li> <li>Build any missing software: Check the software stack of the new system and see what you may need to rebuild or request. Engaging maintains a software stack through modules that includes many of the software packages provided on SuperCloud. Names may be different, so run <code>module avail</code> to check for differences.</li> <li>Run jobs: Run small tests before running larger production jobs. Both SuperCloud and Engaging use Slurm, but Engaging does not have the <code>LL</code> commands such as <code>LLsub</code>, <code>LLfree</code>, and <code>LLstat</code>. Some <code>sbatch</code> flags may differ between the two systems as well. We will provide documentation on this page in the near future for the Engaging equivalent for common SuperCloud job workflows. You can also consult the section on Running Jobs in this documentation.</li> </ul>"},{"location":"transition-guide-supercloud/#where-can-i-get-help","title":"Where can I get help?","text":"<p>The ORCD team can help with migrating data and workloads to Engaging. ORCD has regular office hours (see the Office Hours Schedule). You can also request help through orcd-help@mit.edu.</p>"},{"location":"transition-guide-supercloud/#how-can-i-get-an-account-on-engaging","title":"How can I get an account on Engaging?","text":"<p>It is fairly quick and simple to create your account Engaging. Accounts on the engaging cluster are connected to your MIT institutional Kerberos id. To get an account log into the Engaging OnDemand Web Portal. Connecting to Engaging OnDemand for the first time automatically activates an account with basic access to resources. See this page for instructions on how to log in. After you log in wait a few minutes for your account setup to complete before starting to run jobs.</p>"},{"location":"transition-guide-supercloud/#how-do-i-know-whether-i-have-a-lincoln-collaboration","title":"How do I know whether I have a Lincoln Collaboration?","text":"<p>The SuperCloud documentation on Requesting an Account describes the ways to demonstration a Lincoln collaboration.</p>"},{"location":"transition-guide-supercloud/#if-i-have-lincoln-collaboration-how-do-i-update-my-account-to-reflect-my-collaboration","title":"If I have Lincoln Collaboration how do I update my account to reflect my collaboration?","text":"<p>Check your User Profile page on the SuperCloud Web portal. The \"Lincoln Laboratory Collaboration\" section is in the column on the right and should list any collaborations you or your advisor/PI might have. If any collaborations are missing you can follow the instructions at the top of the page to update your information. Please indicate your collaborator status before May 1 any interruption to your ability to run jobs.</p>"},{"location":"transition-guide-supercloud/#how-can-i-use-my-python-virtual-environments-or-conda-environments-on-engaging","title":"How can I use my Python virtual environments or Conda environments on Engaging?","text":"<p>If you have Python virtual environments or Conda environments on SuperCloud that you would like to use on Engaging, we recommend recreating them on Engaging. Simply copying over environment files is not expected to work due to setup differences between the two clusters.</p> <p>For both types of environments, the general process is to save the environment specifications to a file to be used for regeneration on Engaging.</p> <p>Note</p> <p>For more information on using Python on Engaging, see our Python documentation.</p>"},{"location":"transition-guide-supercloud/#python-virtual-environments","title":"Python virtual environments","text":"<p>Save contents of python venv to a <code>requirements.txt</code> file:</p> <pre><code>module load anaconda/2023b\nsource /path/to/my_venv/bin/activate\npip freeze &gt; my_venv_requirements.txt\n</code></pre> <p>Transfer file to Engaging:</p> <pre><code>scp my_venv_requirements.txt &lt;username&gt;@orcd-login001.mit.edu:/path/to/dest\n</code></pre> <p>Note</p> <p>Your SuperCloud username is most likely different from your Engaging username, which is your MIT kerberos.</p> <p>On Engaging, your <code>requirements.txt</code> file should now appear in the directory you specified. Recreate it with the following commands:</p> <pre><code>module load miniforge\npython -m venv my_venv\nsource my_venv/bin/activate\npip install -r my_venv_requirements.txt\n</code></pre> <p>Note</p> <p>The Python version in SuperCloud's Anaconda module is different than the version provided by the Miniforge module on Engaging. If this is a problem, we recommend creating a Conda environment with your desired version of Python installed.</p>"},{"location":"transition-guide-supercloud/#conda-environments","title":"Conda environments","text":"<p>You can check all Conda environments you have created using <code>conda info --envs</code>. As Conda environments can take up lots of space, we recommend going through and choosing to transfer only the environments that you are still using.</p> <p>For each Conda environment you'd like to transfer, take the following steps:</p> <p>On SuperCloud:</p> <pre><code>module load anaconda/2023b\nconda activate my_env\nconda env export --no-builds | grep -v \"^prefix: \" &gt; my_env.yml\n</code></pre> <p>Transfer <code>.yml</code> file to Engaging:</p> <pre><code>scp my_env.yml &lt;username&gt;@orcd-login001.mit.edu:/path/to/dest\n</code></pre> <p>Recreate environment on Engaging:</p> <pre><code>module load miniforge\nconda env create -f my_env.yml\n</code></pre>"},{"location":"transition-guide-supercloud/#differences-between-supercloud-and-engaging","title":"Differences Between SuperCloud and Engaging","text":"<p>SuperCloud and Engaging are both shared HPC systems that use Slurm. Their high-level architecture is the same, both have login nodes and compute nodes connected by a network and filesystems that can be accessed from each node. However, there are differences in the systems, practices, and policies between the two. This section describes some of those differences that are most helpful to know.</p>"},{"location":"transition-guide-supercloud/#general-differences","title":"General Differences","text":"<ul> <li>Engaging accounts can be created by anyone with an MIT kerberos, see above</li> <li>Engaging maintenance is on the 3rd Tuesday of each month</li> <li>Groups on Engaging are managed through Moira, once groups are created the group admins can add or remove people themselves</li> <li>Engaging can be accessed both inside and outside the United States</li> <li>Compute nodes on Engaging can access the internet</li> <li>Engaging uses an OnDemand Web Portal that provides similar functionality to the SuperCloud Web Portal</li> </ul>"},{"location":"transition-guide-supercloud/#running-jobs","title":"Running Jobs","text":"<ul> <li>Nodes on Engaging are not exclusive by user, one node can have multiple users running jobs.</li> <li>Engaging partitions have a different naming convention and can have multiple different types of nodes. You must specify a partition when you launch jobs. See the Partitions section for more information.</li> <li>The wrapper commands that start with <code>LL</code> are not available on Engaging, however the Slurm commands (start with <code>s</code>, such as <code>sbatch</code>) behave similarly. See the Running Jobs Overview page  and the Requesting Resources page.</li> <li>MIT PIs and DLCIs can purchase additional compute nodes to add to Engaging. Their groups have priority access on these nodes. The MIT community can run preemptable jobs on these nodes when they are idle through the <code>mit_preemptable</code> partition. These jobs are preempted, or stopped, when someone from the group that owns the nodes runs a job on them. If your group is interested in purchasing compute nodes reach out to orcd-help@mit.edu.</li> </ul>"},{"location":"transition-guide-supercloud/#storage","title":"Storage","text":"<ul> <li>While each user on SuperCloud has a home directory, on Engaging each user has three spaces: home, pool, and scratch. Quotas on Engaging are also smaller than on SuperCloud. See General Use Filesystems for a description of each, what they are meant for, and their quotas.</li> <li>Each PI can request 5TB of shared group storage on Engaging.</li> <li>Additional storage space can be rented. See Project Specific Filesystems for more information and email orcd-help@mit.edu if you are interested in purchasing storage for your group.</li> </ul>"},{"location":"transition-guide-supercloud/#migrating-data-from-supercloud","title":"Migrating Data from SuperCloud","text":"<p>This section describes some recipes for migrating data from SuperCloud to Engaging, but some advice will apply to other systems.</p>"},{"location":"transition-guide-supercloud/#step-1-what-to-transfer","title":"Step 1: What to Transfer","text":"<p>First, this is a good opportunity to decide what you need and what you don't need. Take a look at your home and group directories and decide what you need to keep. Transferring a lot of data (more than a few TB) or files (order of 1 million files) can take a long time. </p> <p>Remove files carefully</p> <p>Remove files you no longer need very carefully. Remember, <code>rm</code> on Linux is permanent and the SuperCloud storage is not backed up!</p>"},{"location":"transition-guide-supercloud/#step-2-where-to-transfer","title":"Step 2: Where to Transfer","text":"<p>Next figure out where you are going to transfer the data. ORCD has some base storage described here, with additional storage available for purchase.</p> <p>Check your SuperCloud storage utilization to see how much space you use. You can see both your home directory and your group storage on your User Profile Page. If this is more than what the Engaging quotas can support you may want to check what you can clean up. Your group may also want to purchase additional storage.</p> <p>For long-term archival storage of data that you need to keep, but will never or rarely need to access, consider purchasing archival storage such as AWS Glacier. For storage that you may need to access, but don't need to compute, MIT IS&amp;T provides some storage options for MIT students, staff, and faculty.</p>"},{"location":"transition-guide-supercloud/#step-3-transfer-your-data","title":"Step 3: Transfer Your Data","text":"<p>If you don't have a lot of data to transfer you can use <code>scp</code> or <code>rsync</code> to transfer files at the command line. Log into either system and run your <code>scp</code> or <code>rsync</code> command from there. The best option is to do this on the download partition on SuperCloud as a batch job. You would use a batch script that looks something like this:</p> transfer.sh<pre><code>#!/bin/bash\n\n#SBATCH --partition=download\n\nrsync -ruP path/to/source USERNAME@orcd-login001.mit.edu:/path/to/destination/supercloud-files/\n</code></pre> <p>Do not directly copy SuperCloud home into Engaging home</p> <p>Be careful not to copy your SuperCloud home directory directly into your Engaging home directory. Doing so may overwrite files in your Engaging home directory, including files such as <code>.bashrc</code> which can cause issues. Instead create a subdirectory and transfer your files there.</p> <p>If you have a lot of data and are having trouble with <code>rsync</code> or <code>scp</code> failing before the transfer is complete, Engaging has Globus collections that can make transfer easier. Globus will manage the file transfer for you to make sure everything transfers properly.</p>"},{"location":"transition-guide-supercloud/#using-globus-to-transfer-data","title":"Using Globus to Transfer Data","text":"<p>The first step is to install Globus Connect Personal on SuperCloud. Log into SuperCloud and run the following in your home directory:</p> <pre><code>wget https://downloads.globus.org/globus-connect-personal/linux/stable/globusconnectpersonal-latest.tgz\ntar xzf globusconnectpersonal-latest.tgz\ncd globusconnectpersonal-x.y.z #replace x.y.z with version\n./globusconnectpersonal\n</code></pre> <p>The <code>./globusconnectpersonal</code> command will walk you through setting up Globus on SuperCloud. During the process it will direct you to a link where you will be prompted to log in. Use your MIT credentials to log in. There will be a code for you to copy and enter at the command line where you ran <code>./globusconnectpersonal</code>. This will connect this installation to your account on Globus. It will also ask you for a name for your Collection, use something descriptive that you will remember (something like \"My SuperCloud Collection\" is sufficient). This collection will be private, other Globus users will not be able to find it.</p> <p>To run Globus Connect personal use the command <code>./globusconnectpersonal -start</code>. We recommend starting Globus Connect Personal in a job on the <code>download</code> partition. This can be done with the following script:</p> <pre><code>#!/bin/bash\n\n#SBATCH -p download\n\n~/globusconnectpersonal-3.2.6/globusconnectpersonal -start\n</code></pre> <p>This assumes version 3.2.6 installed in your home directory.</p> <p>Copying Files in Group Directories</p> <p>By default Globus Connect Personal will allow you to transfer files in your home directory. If you need to transfer files in a group directory you will have to specify that directory when you run <code>glogbusconnectpersonal -start</code> by adding the <code>-restrict-paths</code> flag. For example:</p> <pre><code>globusconnectpersonal -start -restrict-paths /home/gridsan/$USER,/home/gridsan/[groupname]\n</code></pre> <p>To initiate the transfer go to https://www.globus.org/, log in, and click \"File Manager\".</p> <p>In the left pane search for your SuperCloud Personal collection using the name you gave it during setup. In the right pane search for the MIT ORCD Engaging Collection collection.</p> <p>If you need help finding the target directory on Engaging with Globus send an email to orcd-help@mit.edu or stop by office hours and we can help you.</p> <p>Once you have selected your target collection navigate to the directory where you want to transfer your data. Create a directory called \"SuperCloud\" and select it.</p> <p>Select the items you want to transfer from SuperCloud in the left pane, or \"Select all\" to transfer your entire home directory. Symlinks (to group directories) and their contents are not transferred.</p> <p>Deselect dot . files</p> <p>Click \"Show Hidden Items\" to deselect . files. Your .bashrc and any conda environments will not work on another system and could cause issues.</p> <p>Under Transfer and Timer options select:</p> <ul> <li>Skip files on source with errors</li> <li>Fail on quota errors</li> <li>Encrypt transfer</li> <li>Consider:<ul> <li>Excluding dot files (ex: .bashrc). Check \"Apply filter rules to the transfer\" and set to exclude all matching <code>.*</code> (see screenshot below)</li> <li>Sync, This is helpful if you've already transferred files and only want to transfer new files</li> </ul> </li> </ul> <p></p> <p>Once you have selected your source files, destination, and transfer settings click the \"Start\" button on the left pane (the SuperCloud side). You can view the transfer progress on the \"Activity\" page, and Globus will send you an email when the transfer is done.</p> <p>If Globus Connect Personal stops on SuperCloud, for example if the job ends, restart it in the same way and Globus should continue the transfer where it left off.</p>"},{"location":"accessing-orcd/control-channels/","title":"Using SSH ControlChannel to Streamline Two-Factor SSH","text":"<p>Two-factor logins for SSH login sessions add security but they can be cumbersome to work with when you need to create multiple login sessions. The ControlChannel feature of OpenSSH can be used to create multiple SSH sessions tied to a single two-factor sign on. Using this feature means you can log in with two-factor authentication once, then subsequent ssh login sessions will use the first ssh connection and two-factor will not be needed. This will last until the initial connection is disconnected.</p>","tags":["Logging In"]},{"location":"accessing-orcd/control-channels/#use-of-ssh-controlchannel","title":"Use of SSH ControlChannel","text":"<p>The simplest way to use the ControlChannel option is to create a section in the file <code>~/.ssh/config</code> that activates the ControlChannel feature on the nodes you use to connect. An example section is show below: </p> ~/.ssh/config<pre><code>Host orcd-login\n    Hostname orcd-login.mit.edu\n    ControlMaster auto\n    ControlPath ~/.ssh/%r@%h:%p\n    ControlPersist 300s\n    User USERNAME\n</code></pre> <p>Replace <code>USERNAME</code> with the username you use on Engaging. In the  configuration file examples the <code>ControlPersist</code> option is not required, but can be used to keep the primary connection open for a set time after that login session is exited. <code>ControlPersist</code> only works if you remain connected to the internet.</p> <p>To use the ControlChannel setup ssh using the name listed in the \"Host\" entry, in this example <code>orcd-login</code>:</p> First ConnectionAdditional Connections <pre><code>username@mycomputer ~ % ssh orcd-login\n(USERNAME@orcd-login.mit.edu) Password: \n(USERNAME@orcd-login.mit.edu) Duo two-factor login for USERNAME\n\nEnter a passcode or select one of the following options:\n\n1. Duo Push to XXX-XXX-1078\n2. Phone call to XXX-XXX-1078\n3. SMS passcodes to XXX-XXX-1078\n\nPasscode or option (1-3): 1\n\nPushed a login request to your device...\nSuccess. Logging you in...\nLast login: Fri Oct  3 16:02:32 2025 from 146.115.151.5\n[USERNAME@login008 ~]$ \n</code></pre> <pre><code>username@mycomputer ~ % ssh orcd-login\nLast login: Fri Oct  3 16:35:34 2025 from 146.115.151.5\n[USERNAME@login008 ~]$ \n</code></pre> <p>In this case you don't need to include your username because it is included in the <code>~/.ssh/config</code> file.</p> <p>Your initial connection will prompt you for your Kerberos password and Duo two-factor authentication. Additional connections will prompt you for a Kerberos password, unless you have ssh keys setup. If you have ssh keys you will only be prompted for your Kerberos password at the first login as part of the two-factor authentication, and additional connections will not require a Kerberos password.</p> <p>Port forwarding and X session forwarding is bound to the initial ControlChannel ssh session.</p>","tags":["Logging In"]},{"location":"accessing-orcd/control-channels/#ssh-controlchannel-and-vscode","title":"SSH ControlChannel and VSCode","text":"<p>This setup is very helpful when using Visual Studio Code's Remote - SSH extension. When VS Code connects to a remote server, it doesn't just open one connection. It opens multiple, simultaneous SSH sessions in the background to handle different tasks: one for the file explorer, one for the integrated terminal, others for language servers, debuggers, and extensions.</p> <p>Without a control channel: If your server requires two-factor authentication , VS Code's attempt to open these many connections at once can trigger a \"storm\" of two-factor prompts. You get bombarded with notifications, and the editor may fail to connect properly.</p> <p>With a control channel: The experience is completely different.  VS Code establishes the initial \"master\" connection, and you approve a single two-factor prompt. Every other connection VS Code needs is then instantly multiplexed through the existing socket.</p> <p>There are some additional settings that make using VS Code with Engaging much easier with two-factor. See our VS Code Tips and Tricks, particularly the \"Connection Timeout\" and \"Max Reconnection Attempts\" settings.</p>","tags":["Logging In"]},{"location":"accessing-orcd/ondemand-login/","title":"Logging in with OnDemand","text":"<p>OnDemand is a web portal used to access Engaging. This page provides instructions on how to log into the OnDemand portal. For logging into SuperCloud and OpenMind, please refer to the respective documentation:  </p> <ul> <li>SuperCloud Documentation and Web Portal </li> <li>OpenMind Login Documentation</li> </ul>","tags":["Logging in with OnDemand"]},{"location":"accessing-orcd/ondemand-login/#accessing-the-web-portal","title":"Accessing the Web Portal","text":"<ol> <li>Navigate to engaging-ood.mit.edu on your web browser. On the Engaging OnDemand login page, select the green <code>Login to Engaging Open OnDemand</code> button to authenticate with Globus.  </li> <li>Under <code>Use your existing organization login</code>, select \"Massachusetts Institute of Technology\" and <code>Continue</code>.   </li> <li>You will be redirected to the MIT Kerberos Login page. Enter your user ID and password.  </li> <li>After successful authentication, you should see the Engaging OnDemand dashboard. </li> </ol>","tags":["Logging in with OnDemand"]},{"location":"accessing-orcd/ssh-login/","title":"Logging in with SSH via Terminal","text":"<p>You can log into our systems via SSH through your local terminal. Using SSH in a terminal or command line window on your desktop is the traditional way to access HPC Systems. This method offers the most flexibility, allowing you to start interactive and batch jobs to run your code, download data, and install packages.</p>","tags":["Logging In"]},{"location":"accessing-orcd/ssh-login/#terminal-by-operating-system","title":"Terminal by Operating System","text":"<p>A terminal window is a window with a command line interface. </p> <p>To log into our systems, we use the terminal to SSH into the system. SSH (Secure Shell) is the primary way to log into remote systems. Once you initiate the SSH command, the shell in your terminal will no longer run on your computer but on the remote system. Authentication is required, either using a password or SSH keys. To set up SSH keys, please refer to the SSH Key Setup Page.</p> <p>Follow the directions below based on your operating system:</p> macOSLinuxWindows <p>Open Terminal by searching for it in Spotlight or by navigating to Applications &gt; Utilities &gt; Terminal.</p> <p>Open Terminal from your Applications menu.</p> <p>Windows systems offer multiple terminal options: Windows Terminal, Command Prompt, and PowerShell. The best way to get a terminal depends on your version of Windows. Starting at Windows 10, the Windows Subsystem for Linux (WSL) is available, allowing you to run Linux as an application in Windows. You can also use the Windows Command Prompt if you have SSH enabled. For older versions of Windows, install a terminal program that supports bash.</p>","tags":["Logging In"]},{"location":"accessing-orcd/ssh-login/#windows","title":"Windows","text":"<p>You have two options:  </p> <ol> <li>Windows Command Prompt (CMD): Comes with all Windows computers and supports SSH with little to no setup. However, it lacks some tools for transferring files and uses different commands than Linux.  </li> <li>Windows Subsystem for Linux (WSL) with Ubuntu: A full Linux terminal that requires some setup but supports all commands used to interact with SuperCloud.</li> </ol> <p>To check if CMD has SSH enabled, run the command <code>ssh</code>. If SSH is not enabled, follow the instructions on this Page to set it up.</p> <p>If you want to use WSL, follow the instructions on this Page to enable WSL and install a Linux distribution of your choice. If you don't have a preference, Ubuntu is a good place to start. If you have any questions about WSL, there is a good chance they are answered in their FAQ.</p>","tags":["Logging In"]},{"location":"accessing-orcd/ssh-login/#older-windows-versions","title":"Older Windows Versions","text":"<p>For older Windows versions, install a terminal that supports bash and SSH, such as MobaXterm. Follow the instructions on this Page to install MobaXterm and create a local shell.</p>","tags":["Logging In"]},{"location":"accessing-orcd/ssh-login/#logging-in-via-ssh","title":"Logging in via SSH","text":"<p>Once you have your terminal set up for your specific operating system, you can use SSH to access our HPC systems. Follow the commands below for your desired system.</p> EngagingSuperCloudOpenMind <p>You can login to Engaging via the command line with the SSH command: <pre><code>ssh [username]@orcd-login.mit.edu\n</code></pre> Replace <code>[username]</code> with your MIT Kerberos username.</p> <p>Connecting requires Two-Factor Authentication, your MIT Kerberos password and Duo. You can reduce the number of times you need to do Two-Factor Authentication by using SSH Control Channels.</p> <p>If you are still using the older Centos 7 nodes you can use one of the following login nodes instead:</p> <ul> <li>orcd-vlogin001.mit.edu  </li> <li>orcd-vlogin002.mit.edu </li> <li>orcd-vlogin003.mit.edu</li> <li>orcd-vlogin004.mit.edu</li> </ul> <p>Note</p> <p>You will be prompted to enter your MIT Kerberos password if you have not set up SSH keys. To set them up, please refer to our SSH Key Setup Page.</p> <p>Accessing the SuperCloud system through SSH requires that your public ssh-key has been added to the <code>authorized_keys</code> file in your SuperCloud account. Follow the instructions on this page to create your keys and add them to your account.</p> <p>If you have any issues connecting to the system, please send an email to supercloud@mit.edu.</p> <p>To connect to SuperCloud once your SSH keys are setup, follow these steps:  </p> <ol> <li>Open a command line terminal window.</li> <li>Enter the following command, replacing <code>[username]</code> with your SuperCloud username: <pre><code>ssh [username]@txe1-login.mit.edu\n</code></pre></li> </ol> <p>To login to OpenMind, you must first be connected to the MIT Wi-Fi or MIT VPN. For further instructions, refer to this page.</p> <p>To login to OpenMind via the command line, run the SSH command: <pre><code>ssh [username]@openmind.mit.edu\n</code></pre> Replace <code>[username]</code> with your MIT Kerberos username. </p> <p>Note</p> <p>You will be prompted to enter your MIT Kerberos password if you have not set up SSH keys. To set them up, please refer to our SSH Key Setup Page.</p>","tags":["Logging In"]},{"location":"accessing-orcd/ssh-setup/","title":"SSH Key Setup","text":"<p>An SSH key is a secure access credential used in the SSH protocol and establishes a secure and encrypted connection to our HPC systems. This page is for those who wish to implement SSH key authentication on top of general MIT Kerberos authentication.</p> <p>SSH keys consist of a pair: a public key and a private key. </p> <ul> <li>Public Key: This key can be shared freely and is used to encrypt data that only the corresponding private key can decrypt.</li> <li>Private Key: This key must be kept secure and private. It is used to decrypt data encrypted with the corresponding public key and to prove the identity of the user during the SSH authentication process.</li> </ul> <p>When you attempt to connect to an HPC system using SSH key authentication, the system uses your public key to initiate a challenge that can only be answered correctly using your private key. If the correct response is received, the system verifies your identity and grants access. </p> <p>Using the key and lock analogy, the private key is like your key, and the public key is like a lock you might place on a gym locker, which would be like your account on an ORCD system. You can leave the lock locked on the locker at the gym, because no one can open the lock without the key, but you wouldn't want to share your key with anyone else, because then they could get into your locker.</p> <p>Do Not Share Your Private Key</p> <p>Your private key should never be shared with anyone. If someone else obtains your private key, they could potentially gain unauthorized access to any system your key is associated with.</p>","tags":["SSH Key Setup"]},{"location":"accessing-orcd/ssh-setup/#checking-for-existing-ssh-keys","title":"Checking for Existing SSH Keys","text":"<p>Before you generate an SSH key, you should check for existing SSH keys.  </p> <ol> <li>Open your local terminal.  </li> <li>Run the following command to view all existing SSH keys: <pre><code>ls -al ~/.ssh\n</code></pre></li> <li>If you see a list of files, you have existing SSH keys. If you receive an error that ~/.ssh doesn't exist, you do not have an existing SSH key pair in the default location. You can create a new SSH key pair in the next step.</li> </ol>","tags":["SSH Key Setup"]},{"location":"accessing-orcd/ssh-setup/#generating-ssh-keys","title":"Generating SSH Keys","text":"<p>If you do not have an existing SSH key, follow these steps. </p> <ol> <li>Open your local Terminal.  </li> <li>Run the following command to generate an RSA key: <pre><code>ssh-keygen -t rsa\n</code></pre></li> <li> <p>Save the key pair: You will be prompted to enter a file path to save the key. Press <code>Enter</code> to accept the default location: <pre><code>Enter a file in which to save the key (/home/your_username/.ssh/id_rsa):\n</code></pre></p> </li> <li> <p>Passphrase:  You will be asked to enter a passphrase for additional security. You can either enter a passphrase or leave it empty and press <code>Enter</code>: <pre><code>Enter passphrase (empty for no passphrase):\n</code></pre></p> </li> </ol> <p>Note</p> <p>On ORCD systems, we recommend setting a passphrase as it provides extra security for your account by helping to prevent someone else from using your SSH keys. When you create a passphrase, your private key can only authenticate into your account when the correct passphrase is provided during login. Since you set your passphrase on your system, ORCD staff cannot help you remember or reset the passphrase for your SSH keys. You must create new SSH keys if you cannot remember your passphrase.</p>","tags":["SSH Key Setup"]},{"location":"accessing-orcd/ssh-setup/#uploading-ssh-key-on-our-systems","title":"Uploading SSH Key on Our Systems","text":"<p>To upload your SSH key on our systems, you must update the <code>authorized_keys</code> file in the respective system via terminal. Alternatively, for the Engaging System, you have the option to use OnDemand, and for SuperCloud, you can fill out an SSH key addition form.</p> TerminalEngaging OnDemandSuperCloud <p>To add your SSH key via Terminal, please follow the steps outlined below:</p> <ol> <li>Login to an HPC system login-node using MIT Kerberos Login.</li> <li>On your local machine, copy the contents of your public key (<code>~/.ssh/id_rsa.pub</code>): <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <p>Make sure to copy the entire line starting with ssh-rsa and ending with your email address or comment. </p> </li> <li>On the head-node, append the copied contents to the authorized_keys file located at <code>/home/[username]/.ssh/authorized_keys</code>. You can use any text editor of your choice, such as nano and vim. For example, if you're using <code>nano</code>, the command would be: <pre><code>nano /home/[username]/.ssh/authorized_keys\n</code></pre></li> </ol> <p>Note</p> <p>Do not remove anything already present in the authorized_keys file. Be careful to append your key to the end of the file rather than replacing its contents.</p> <p>To add your SSH key to Engaging via OnDemand, please follow the steps outlined below:</p> <ol> <li>Login to Engaging OnDemand through the portal.</li> <li>Once logged in, navigate to <code>Files</code> and <code>Home Directory</code>.</li> <li>On the top right corner, check <code>Show Dotfiles</code>. </li> <li>Click on the <code>.ssh</code> folder. </li> <li>Locate and edit the <code>authorized_keys</code> file to add your new key. </li> </ol> <p>Note</p> <p>Do not remove anything already present in the authorized_keys file. Be careful to append your key to the end of the file rather than replacing its contents.</p> <p>To add your SSH key to SuperCloud, please follow the steps outlined below:</p> <ol> <li>Navigate to the SuperCloud SSH Key Addition Form and follow the instructions to add your SSH key. </li> <li>If you encounter any problems during the SSH key submission process, refer to the SSH Troubleshooting Checklist for guidance. </li> </ol>","tags":["SSH Key Setup"]},{"location":"accessing-orcd/ssh-setup/#testing-your-ssh-key-setup","title":"Testing your SSH Key Setup","text":"<p>To ensure that your SSH key is correctly configured, follow these steps:</p> <ol> <li>Attempt to login on your terminal: <code>ssh your_username@cluster_address</code>. For more details, you can reference the Getting Started Tutorial page.</li> <li>If prompted for a password, the SSH key setup did not work. Recheck the steps and correct any issues.</li> </ol>","tags":["SSH Key Setup"]},{"location":"accessing-orcd/ssh-setup/#troubleshooting-ssh-key-issues","title":"Troubleshooting SSH Key Issues","text":"<p>If you encounter SSH key issues, you can reference the SSH Troubleshooting Checklist. While this guide is for SuperCloud, it should be helpful for other systems as well. If you are still having problems, please email us at orcd-help@mit.edu and visit the Getting Help page. In your help email, please include the output of the following command: <pre><code>ssh -vvv USERNAME@cluster_address\n</code></pre></p>","tags":["SSH Key Setup"]},{"location":"filesystems-file-transfer/filesystems/","title":"General Use Filesystems","text":"<p>Large HPC systems often have different filesystems for different purposes. ORCD systems are no different, and each have their own approach. This page documents these.</p>"},{"location":"filesystems-file-transfer/filesystems/#engaging","title":"Engaging","text":"<p>Users each get a Home Directory that is backed up and meant for important files. An additional larger Pool space is provided for storing larger datasets longer term. Larger Scratch space is not backed up. Additional storage can be purchased, and PIs can request an additional 5TB of shared Pool storage for their lab. The Scratch spaces are meant for data used in actively running jobs. It will be faster to access Scratch during your job for the majority of workloads, but it is not backed up and should not be used for long term storage. Any files that cannot be easily replaced should be stored in Home or Pool storage, or backed up outside of Engaging.</p> <p>See the table below for a description of each storage space. If your account was created before January 2025 and has not yet been migrated to the new storage, select the second tab.</p> Account Created or Migrated After Jan 2025Account Created Before Jan 2025 or Not Yet Migrated Storage Type Path Quota Backed up Purpose/Notes Home Directory  Flash <code>/home/&lt;username&gt;</code> 200 GB Backed up with snapshots Use for important files and software Pool  Hard Disk <code>/home/&lt;username&gt;/orcd/pool</code> 1 TB Not backed up Storing larger datasets Scratch  Flash <code>/home/&lt;username&gt;/orcd/scratch</code> 1 TB Not backed up Scratch space for I/O heavy jobs <p>Scratch is Not Backed Up</p> <p>Scratch is meant for temporary storage while running compute jobs. It is not meant for long term storage and is not backed up. If you have not logged in for 6 months files in scratch will be deleted. Any files that you would like to keep long-term should be copied onto another storage location with backup.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory  NFS <code>/home/&lt;username&gt;</code> 100 GB Backed up Use for important files NFS <code>/pool001/&lt;username&gt;</code> 1 TB Not backed up Scratch space Lustre <code>/nobackup1/&lt;username&gt;</code> 1 TB Not backed up Scratch space  Heavy I/O jobs with few, large files <p>Lustre /nobackup1 is Not Backed Up</p> <p>/nobackup1 is meant for temporary storage while running compute jobs. It is not meant for long term storage and is not backed up. Any files that you would like to keep long-term should be copied onto another storage location with backup.</p> <p>Lustre /nobackup1 performs best with fewer, larger files</p> <p>Having large numbers of small files will make Lustre slower than NFS and can slow down the filesystem overall, so it is important to follow the Lustre Best Practices.</p>"},{"location":"filesystems-file-transfer/filesystems/#supercloud","title":"SuperCloud","text":"<p>SuperCloud uses Lustre for all central/shared storage (accessible to all nodes in the system). This storage is not backed up. See the SuperCloud Best Practices and Performance Tips page for best practices using the Lustre filesystem. Quotas or limits are set on the storage as guardrails. Individual and group storage use and quotas can been viewed on the User Profile Page on the SuperCloud Web Portal (only accessible if you have an account). Additional storage may be granted on a case by case basis. Local disk spaces will be faster than the Lustre shared filesystem, but all are temporary and can only be accessed on the node where they are created.</p> Storage Type Path Access Backed up Limits Home Directory  Lustre <code>/home/gridsan/&lt;username&gt;</code> User only Not backed up See User Profile Page Group Directories  Lustre <code>/home/gridsan/groups/&lt;groupname&gt;</code> Files shared within a group Not backed up See User Profile Page Job-specific Temporary Storage  Local Disk Access using the <code>$TMPDIR</code> environment variable User or Group Not backed up   Temporary directory created at the start of a job and cleaned up at the end of the job NA Local Disk Space Create the directory <code>/state/partition1/user/$USER</code> as needed User or Group Not backed up  Cleaned up monthly during downtimes NA"},{"location":"filesystems-file-transfer/filesystems/#openmind","title":"OpenMind","text":"<p>OpenMind provides a number of different storage options. See the OpenMind Documentation page on Storage for more information, best practices, and recommendations.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory <code>/home/&lt;username&gt;</code> 20 GB Backed up Use for very important files. Physically located on Flash 2. Flash 1 <code>/om/user/&lt;username&gt;</code> (individual users) and <code>/om/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Flash 1 Scratch <code>/om/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 3 weeks after creation Scratch space Flash 2 <code>/om2/user/&lt;username&gt;</code> (individual users) and <code>/om2/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Flash 2 Scratch <code>/om2/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 2 weeks after creation Scratch space NFS <code>/om3</code>, <code>/om4</code>, <code>/om5</code> Per group Backed up Slow internal long-term storage NESE <code>/nese</code> Per group Backed up Slow internal long-term storage"},{"location":"filesystems-file-transfer/lustre-best-practices/","title":"Lustre Best Practices","text":"<p>Lustre will be Retired</p> <p>We are in the process of migrating all lustre storage to a new flash Scratch filesystem. This page applies if you still have storage on <code>/nobackup1</code>. While these are all good general best practices, not everything on this page is relevant to the new Scratch storage.</p> <p>Lustre is a type of file system technology used on the Engaging cluster, known to most users as the <code>/nobackup1b</code> or <code>/nobackup1c</code> file systems. The Lustre storage system is built to manage extremely high rates of input and output, and can read and write large files faster than traditional storage like NFS, the home directory.</p> <p>Since it is designed to read and write large files, it has difficulties running jobs that read and write lots of small files. When Lustre is faced with many small file input and output, it can get overloaded and in return responds very slowly across the entire filesystem, affecting more than just the user who is running the job with small file input and output.</p> <p>Along with avoiding small file input and output, there are a few other things users should know to avoid when using the Lustre file system (<code>nobackup1b</code>/<code>nobackup1c</code>).</p>"},{"location":"filesystems-file-transfer/lustre-best-practices/#avoid-using-the-command-ls-l","title":"Avoid Using the Command <code>ls -l</code>","text":"<p>The <code>ls -l</code> command displays a lot of information such as ownership, permissions, and the size of files and directories. Some of this information, such as the file size, is only stored in part of the Lustre technology, and so this information must be queried for all files and directories on the file system. This can be very resource consuming and take a long time to complete since many files are stored on <code>/nobackup1b</code> and <code>/nobackup1c</code> by many users.</p> <p>Instead of using <code>ls -l</code>, you should:</p> <ul> <li>Use <code>ls</code> by itself if you just want to see if a file exists</li> <li>Use <code>ls -l &lt;filename&gt;</code> if you want the long listing of a specific file</li> </ul>"},{"location":"filesystems-file-transfer/lustre-best-practices/#avoid-having-a-large-number-of-files-in-a-single-directory","title":"Avoid Having a Large Number of Files in a Single Directory","text":"<p>Opening a file keeps a lock on the parent directory. When many files in the same directory are to be opened, it creates contention. A better practice is to split a large number of files (in the thousands or more) into multiple subdirectories to minimize contention.</p>"},{"location":"filesystems-file-transfer/lustre-best-practices/#avoid-storing-and-accessing-small-files-on-lustre","title":"Avoid Storing and Accessing Small Files on Lustre","text":"<p>Accessing small files on the Lustre filesystem is not efficient. There are 2 other locations users can store smaller sized files that are much better suited to handle them. These locations are Pool and the user\u2019s Home Directory. For more information on these storage locations, please see the page on General Use Filesystems.</p> <p>To limit users storing lots of small files on <code>/nobackup1b</code> or <code>/nobackup1c</code>, there is a inode quota of 50k on these locations. An inode is a file, directory, or symlink, since all *unix systems treat these inodes the same in terms of permissions and storage.</p> <p>To check how much of your quota you have used, you can use the command:</p> <pre><code>lfs quota -h -u &lt;username&gt; /nobackup1\n</code></pre>"},{"location":"filesystems-file-transfer/lustre-best-practices/#avoid-accessing-and-running-executable-files-from-lustre","title":"Avoid Accessing and Running Executable Files from Lustre","text":"<p>Executables run slower when they are run from <code>/nobackup1</code>, and shouldn\u2019t be run from login or head nodes, regardless if they are run from <code>/nobackup1</code>, Pool, or a user\u2019s Home Directory. To learn how to run on Engaging, see the section on Running Jobs.</p> <p>Executable files for jobs are best stored in a user\u2019s home directory. Storing executable files in Pool is also acceptable, but less ideal. Input data, such as datasets or input files, can be stored on <code>/nobackup1</code> and your job can generate any large data output to <code>/nobackup1</code>. Any files you need to keep long term should also be stored in Home, Pool, or another backed up location. Anything stored in a location that is not backed up, such as <code>/nobackup1</code>, has some risk of being lost.</p>"},{"location":"filesystems-file-transfer/lustre-best-practices/#avoid-having-multiple-processes-open-the-same-files-at-the-same-time","title":"Avoid Having Multiple Processes Open the Same File(s) at the Same Time","text":"<p>On Lustre filesystems, if multiple processes try to open the same file(s), some processes will not able to find the file(s) and your job will fail.</p>"},{"location":"filesystems-file-transfer/project-filesystems/","title":"Project Specific Filesystems","text":""},{"location":"filesystems-file-transfer/project-filesystems/#purchasing-storage","title":"Purchasing Storage","text":"<p>Additional project and lab storage can be purchased on ORCD shared clusters by individual PI groups. This storage is mounted on the cluster and access to the storage is managed  by the group through MIT Web Moira (see below for details).</p> <p>The options for storage are:</p> Storage Type Description Encryption at Rest Backup Namespace Notes Data Frequent data access Optional No Limited Day to day research storage, active projects, instrument data buffers, etc. Compute (Coming Soon) Very frequent data access Optional No Limited Very fast access, special needs, high IO <p>Please note that all types of storage are not backed up by default.</p> <p>Storage is charged at the start of each month. The first month is prorated by the number of days left in the current month. A purchase must be a minimum of 20 TiB and in increments of 20 TiB.</p> <p>If you anticipate needing more than a few 100 TiB let us know when you request your storage. We may suggest purchasing a dedicated server for your lab.</p> <p>For more information, including pricing, and to purchase storage please send an email to orcd-help@mit.edu. If you are purchasing storage please include the following in your request:</p> <ul> <li>The storage type (compute, data, or archival)</li> <li>Amount in TiB (20 TiB increments)</li> <li>Cost object</li> <li>The lab PI</li> </ul>"},{"location":"filesystems-file-transfer/project-filesystems/#managing-access-using-mit-web-moira","title":"Managing access using MIT Web Moira","text":"<p>Individual group storage is configured so that access is limited to a set of accounts belonging to a web moira list that is defined for the group storage. The owner and administrators of group storage can manage access themselves, by modifying the membership of an associated moira list under https://groups.mit.edu/webmoira/list/[list_name]. The name of the list corresponds to a UNIX group name associated with the ORCD shared  cluster storage. The naming scheme we use means this list name won't exactly match the UNIX group name you see on the cluster storage, but it will be similar.</p> <p>If you are an admin of an ORCD group you will see that group listed under \"Lists I Can Administer\" when you log into Web Moira. Click on the group name to go to the group management page. We will also send you a direct link to your group management page when we create your group.</p>"},{"location":"filesystems-file-transfer/project-filesystems/#moira-web-interface-example","title":"Moira Web Interface Example","text":"<p>The figure below shows a screenshot of the web moira management page at https://groups.mit.edu/webmoira/list/cnh_research_computing for a hypothetical storage group named <code>cnh_research_computing</code>. The interface provides a  self-service mechanism for controlling access to any storage belonging to this group. MIT account IDs can be added and  removed as needed from this list by the storage access administrators.</p> <p></p>"},{"location":"filesystems-file-transfer/transferring-files/","title":"Transferring Files","text":"<p>There are a few different ways to transfer files depending on your goals, the data you are transferring, and what you are comfortable with. On this page we cover the different methods of transferring files, as well as touch on how to transfer files between systems.</p> <p>We recommend using OnDemand for every-day file transfer and Globus for transferring large files or large numbers of files. For those who prefer to use the command line you can use scp or rsync.</p> <p>For system-specific information on transferring files for SuperCloud or OpenMind, click on one of the following tabs. This page discusses transferring files for Engaging.</p> SuperCloudOpenMind <p>SuperCloud Transferring Files Documentation Page</p> <p>OpenMind Transferring Files Documentation Page</p>"},{"location":"filesystems-file-transfer/transferring-files/#ondemand","title":"OnDemand","text":"<p>Engaging has an OnDemand web portal. SuperCloud has its own custom portal.</p> EngagingSuperCloud <p>Engaging OnDemand Portal</p> <p>SuperCloud Web Portal (Documentation)</p> <p>With the Engaging OnDemand portal you can do the following using the File Browser:</p> <ul> <li>Upload and download files and directories</li> <li>Rename, move, and delete files and directories</li> <li>View and edit files</li> </ul> <p>Once you are logged into OnDemand, you can use the File Browser by selecting Files -&gt; Home Directory in the menu bar at the top left of the page.</p> <p>To upload and download you can drag and drop files and directories between your File Browser window and your desktop Finder/Explorer windows. You can also use the \"Upload\" and \"Download\" buttons. Select multiple files by holding the Control (or Command) key and clicking on the files you'd like to select. Those files can then be downloaded with the \"Download\" button.</p>"},{"location":"filesystems-file-transfer/transferring-files/#viewing-and-editing-files","title":"Viewing and Editing Files","text":"<p>The File Browser is also the easiest way to view and edit files in place. Click on the file that you would like to view or edit. Then click either the \"View\" or \"Edit\" buttons directly above the list of files.</p> <p>The editor has some nice features like line numbers and syntax highlighting for most languages. You can also change the display colors, the font size, and whether you'd like the words to wrap. Click \"Save\" to save any edits you made.</p>"},{"location":"filesystems-file-transfer/transferring-files/#globus","title":"Globus","text":"<p>If you are moving more than a few files, or your files are particularly large, we recommend using Globus. Globus is a tool that helps transfer large amounts of data between systems. We have Globus collections set up on each ORCD system. Collections are the mechanism Globus uses for accessing data.</p> <p>Some advantages of using Globus are:</p> <ul> <li>It is easy to initiate transfers through the Globus webpage</li> <li>You don't need to stay logged into Globus through the entire transfer</li> <li>If your transfer is interrupted it will continue automatically where it left off once the connection is re-established</li> </ul> <p>To transfer data:</p> <ol> <li>Log in: Log into Globus with your MIT credentials.</li> <li>Select your source and destination collections: In the \"File Manager\" tab in each of the two \"Collection\" boxes search for the collections for the systems you want to transfer data between (MIT ORCD Engaging Collection for Engaging, or see below for a full list of ORCD  Collections). To transfer data to or from your own computer you will need to set up Globus Connect Personal. Follow the instructions on the page for your system listed here.</li> <li>Navigate to your source and destination directories: On the source side navigate to the source directory and select the files and/or directories you'd like to transfer. On the destination side navigate to the location where you'd like to copy your files</li> <li>Select any additional settings: Click on \"Transfer and Timer Options\" for additional settings, such as syncing new or changed files and scheduling recurring transfers.</li> <li>Initiate the transfer: Once you have selected the files and options you want, press the \"Start\" button on the source column.</li> <li>Monitor your transfer (optional): Click on \"Activity\" to view the status of your active transfers. You will also get an email when you transfer is complete, or if Globus runs into any issues with the transfer.</li> </ol> <p>The Globus Documentation has a tutorial with screenshots to demonstrate this process.</p> <p>Here is a list of Globus Collections on ORCD systems:</p> System Globus Collection Engaging MIT ORCD Engaging Collection OpenMind mithpc#openmind <p>More documentation on transferring files through Globus can be found on the Globus Documentation Pages. Globus also has an FAQ that is helpful for answering any questions you might have.</p>"},{"location":"filesystems-file-transfer/transferring-files/#command-line","title":"Command Line","text":"<p>The most common commands used to transfer files are <code>scp</code> and <code>rsync</code>. You will need to run both of these commands from your local computer, before logging into any ORCD system. In order to use these two commands you will need:</p> <ul> <li>The hostname of the remote machine you are transferring files to or from, these are listed below</li> <li>The full path on the remote machine where you are copying the file to or from</li> <li>The ability to ssh to the remote machine where you are transferring files to or from</li> </ul> <p>The hostname of the node where you will be transferring files is often a login node, but may also be a dedicated data transfer node. Select the system you are using to see options for the hostname here:</p> EngagingOpenMindSuperCloud <ul> <li><code>orcd-login001.mit.edu</code></li> <li><code>orcd-login002.mit.edu</code></li> <li><code>orcd-login003.mit.edu</code></li> <li><code>orcd-login004.mit.edu</code></li> </ul> <ul> <li><code>openmind-dtn.mit.edu</code></li> </ul> <ul> <li><code>txe1-login.mit.edu</code></li> </ul> <p>Both <code>scp</code> and <code>rsync</code> work similar to <code>cp</code>, in that you specify a source (where the file is coming from) and destination (where the file is going to). The main difference is that you will need to specify the hostname of the remote system.</p> <pre><code># using cp to copy files within the same system\ncp /path/to/source /path/to/destination\n\n# using scp to copy files from the local system to &lt;hostname&gt;\nscp /path/to/source &lt;hostname&gt;:/path/to/destination\n\n# using scp to copy files from &lt;hostname&gt; to the local system\nscp &lt;hostname&gt;:/path/to/source /path/to/destination\n</code></pre> <p>Unless you have your paths memorized, the easiest way to do this is to have two terminals open. The first terminal is logged into Engaging or other remote system, the second is on your local computer. In each navigate to the respective source and destination directories. In the Engaging tab you can run the <code>pwd</code> command to print out the path to your current location and copy the output to use in the <code>scp</code> or <code>rsync</code> command.</p>"},{"location":"filesystems-file-transfer/transferring-files/#scp","title":"scp","text":"<p>First, open a terminal on your computer (not logged into any ORCD system).</p> <p>To transfer a file from your local computer to an ORCD system you would use the command:</p> EngagingOpenMindSuperCloud <pre><code>scp &lt;local-file-name&gt; USERNAME@orcd-login001.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; USERNAME@txe1-login.mit.edu:&lt;path-to-supercloud-dir&gt;\n</code></pre> <p>For example, let's say you have the local file <code>myscript.py</code> and you want to transfer it to the directory <code>mycode</code> in your home directory. The command would be:</p> EngagingOpenMindSuperCloud <pre><code>scp myscript.py USERNAME@orcd-login001.mit.edu:/home/USERNAME/mycode/\n</code></pre> <pre><code>scp  myscript.py USERNAME@openmind-dtn.mit.edu:/home/USERNAME/mycode/\n</code></pre> <pre><code>scp myscript.py USERNAME@txe1-login.mit.edu:/home/gridsan/USERNAME/mycode/\n</code></pre> <p>To transfer the other direction (from an ORCD system to your local computer) switch the order:</p> EngagingOpenMindSuperCloud <pre><code>scp USERNAME@orcd-login001.mit.edu:&lt;path-to-engaging-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <pre><code>scp USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <pre><code>scp USERNAME@txe1-login.mit.edu:&lt;path-to-supercloud-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <p>If you were to have the file <code>results.csv</code> that you want to copy from the <code>output</code> directory in your remote home directory to the current directory on your computer the command would be:</p> EngagingOpenMindSuperCloud <pre><code>scp USERNAME@orcd-login001.mit.edu:/home/USERNAME/output/results.csv .\n</code></pre> <pre><code>scp USERNAME@openmind-dtn.mit.edu:/home/USERNAME/output/results.csv .\n</code></pre> <pre><code>scp USERNAME@txe1-login.mit.edu:/home/gridsan/USERNAME/output/results.csv .\n</code></pre> <p>Note the <code>.</code> in the command above means the current directory.</p> <p>Similar to the <code>cp</code> command, if you want to transfer an entire directory and all of its subdirectories, use the <code>-r</code> (recursive) flag for either direction:</p> EngagingOpenMindSuperCloud <pre><code>scp -r &lt;local-directory-name&gt; USERNAME@orcd-login001.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-directory-name&gt; USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-directory-name&gt; USERNAME@txe1-login.mit.edu:&lt;path-to-supercloud-dir&gt;\n</code></pre> <p>Note</p> <p>To <code>scp</code> files to/from the login nodes on Engaging, you will need to authenticate with Duo. You may get a Duo push without any indication from the command line.</p>"},{"location":"filesystems-file-transfer/transferring-files/#rsync","title":"rsync","text":"<p>The use of <code>rsync</code> is very similar to <code>scp</code>, but the behavior is different. By default <code>rsync</code> will not transfer files that are identical at both the source and destination. There are additional flags you can use to specify what <code>rsync</code> should do when files differ. The <code>rsync</code> command can be very useful when you want to \"sync\" updates to a directory or when transferring large directories. If a transfer fails during <code>rsync</code> you can re-run the command and it will pick up where it left off, rather than re-transfer everything.</p> <p>For general use, the example commands above for <code>scp</code> apply, use the same command but replace <code>scp</code> with <code>rsync</code>.</p> <p>Some useful flags include:</p> <ul> <li><code>-r</code>, <code>--recursive</code> to recursively copy files in all sub-directories</li> <li><code>-l</code>, <code>--links</code> to copy and retain symbolic links</li> <li><code>-u</code>, <code>--update</code> skips any files for which the destination file already exists and has a date later than the source file</li> <li><code>-v</code>, <code>--verbose</code> prints out more information during the file transfer, add more <code>v</code>s for more information</li> <li><code>--partial</code> keeps partially transferred files, useful when transferring large files so rsync can continue where it left off if the transfer fails</li> <li><code>--progress</code> prints information about the progress of the transfer</li> <li><code>-n</code>, <code>--dry-run</code> does not run the transfer but prints out what actions it would be taken, useful to avoid unintended file overwrites</li> </ul> <p>You can run <code>rsync --help</code> to print out a full list of flags that can be used with the <code>rsync</code> command.</p> <p>Note</p> <p>To <code>rsync</code> files to/from the login nodes on Engaging, you will need to authenticate with Duo. You may get a Duo push without any indication from the command line.</p>"},{"location":"filesystems-file-transfer/transferring-files/#moving-files-between-orcd-systems","title":"Moving files between ORCD Systems","text":"<p>If you need to move files between ORCD systems you ssh to one of the ORCD systems and initiate the transfer from that system to the other. Once you are logged into one system the process is the same as if you were to transfer files to or from your own computer. For example to move a file from SuperCloud to Engaging using <code>scp</code> you would first log into SuperCloud and then use <code>scp</code> to transfer the file: Transferring files from SuperCloud to Engaging<pre><code>ssh USERNAME@txe1-login.mit.edu\nscp &lt;path-to-SuperCloud-file&gt; USERNAME@orcd-login001.mit.edu:&lt;path-to-engaging-directory&gt;\n</code></pre> You can also <code>ssh</code> into Engaging and initiate the transfer from there using a similar command.</p>"},{"location":"filesystems-file-transfer/transferring-files/#graphical-applications-for-file-transfer","title":"Graphical Applications for File Transfer","text":"<p>There are a few applications you can download that will allow you to transfer files with  drag-and-drop, similar to how you would move files around on your own computer.</p> <p>Some of the most common options are:</p> <ul> <li>Cyberduck (Mac and Windows)</li> <li>FileZilla (Mac, Windows, and Linux)</li> <li>WinSCP (Windows only)</li> </ul> <p>To use these you will need to know the hostname of the ORCD system you are accessing, either one of the login nodes or a dedicated data transfer node. See the list of hostnames at the top of the Command Line section to see which you should use for the system you are transferring files to.</p>"},{"location":"images/","title":"Index","text":""},{"location":"images/#directory-of-static-images","title":"Directory of static images","text":""},{"location":"recipes/X11/","title":"Using X11 with Engaging Cluster","text":"<p>While using the command line is often fast and efficient, we often just need to use a Graphical User Interface (GUI).</p> <p>This process allows you to use the cluster and present a GUI to your desktop machine using X11.</p>"},{"location":"recipes/X11/#mac-and-linux","title":"Mac and Linux","text":"<p>On Linux or Mac, Open the terminal program of your choice. On the Mac, you will have to make sure you have XQuartz installed and you will need to use either XQuartz or iTerm2 as your terminal.</p> <p>ssh to the cluster log in node using your MIT kerberos ID.  <pre><code>ssh -X &lt;your_mit_kerb&gt;@orcd-login001.mit.edu\n</code></pre> You will get a DUO push 2 factor authentication notification asking you to log in. </p> <p>Once you are logged in, create your allocation in slurm using X11 option: <pre><code>salloc -N 1 -n 4 --mem-per-cpu=4G -p mit_normal --x11\n</code></pre> Load the module you want to use a GUI with. In this example, we're going run MATLAB on the cluster and have it display on our desktop or laptop: </p> <p><pre><code>module load matlab\n\nmatlab &amp; \n</code></pre> The \"&amp;\" in the above command puts the matlab program in the background and frees your terminal for other activities. </p> <p>Note</p> <p>If you are on a wireless network, the GUI may not show up on your desktop for several seconds. If you are connected via ethernet cable, it will be more responsive. For example, working remotely at the Rotch library on wireless it could take 20 seconds for the GUI to display on your system. This is not likely something you would want to do for your regular workflow, but it can be useful for small tasks or if you do not have another option.</p>"},{"location":"recipes/af3/","title":"Running AlphaFold 3 on Engaging","text":"<p>AlphaFold is an AI system developed by Google that is used for predicting protein structures. Here we provide a brief description of how to run this model on the Engaging computing cluster.</p> <p>Note</p> <p>These instructions assume that you have access to a partition with a GPU on Engaging. If you do not have such access, then you may be able to run this on a CPU, but this would require editing the code distribution provided by Google DeepMind and rebuilding the Apptainer image.</p>","tags":["Engaging","Howto Recipes"]},{"location":"recipes/af3/#getting-started","title":"Getting Started","text":"<p>For simplicity, in this example, we will create a folder in our home directory to use as our working directory:</p> <pre><code>mkdir ~/af3\nexport WORKDIR=~/af3\n</code></pre> <p>Model weights</p> <p>Each user needs to obtain the model weights individually due to licensing restrictions. These can be obtained by submitting a request to Google DeepMind. Usually, requests are granted within a few days. To make a request, follow the instructions on the AlphaFold 3 GitHub Repository.</p> <p>When you get access, you will receive a link to download the parameters. After you download them, you can upload them to Engaging using <code>scp</code> on your local machine (you will receive a Duo push notification - see Transferring Files):</p> <pre><code>scp /path/to/source/af3.bin.zst $USER@orcd-login001.mit.edu:~/af3\n</code></pre> <p>On Engaging, decompress the file and move to a <code>models</code> directory:</p> <pre><code>cd $WORKDIR\nzstd -d af3.bin.zst\nmkdir models\nmv af3.bin models\nrm af3.bin.zst\n</code></pre> <p>Container image</p> <p>Google DeepMind provides instructions in their repository on running AlphaFold 3 with Docker. Docker is not compatible with most HPC environments, so we need to run a pre-built container using Apptainer. We have an image saved globally on Engaging located at:</p> <pre><code>/orcd/software/community/001/container_images/alphafold3/20250321/alphafold3.sif\n</code></pre> <p>Note</p> <p>The image that we have built uses a specific version of the AlphaFold 3 GitHub repository. As such, it does not change as their code base gets updated for bug fixes. If you would like us to update the image to reflect recent changes in the repository, let us know at orcd-help-engaging@mit.edu.</p>","tags":["Engaging","Howto Recipes"]},{"location":"recipes/af3/#running-alphafold-3","title":"Running AlphaFold 3","text":"<p>The last thing you will need to run AlphaFold 3 is the AlphaFold dataset. Because it is quite large, we have saved it globally on Engaging for all users at <code>/orcd/datasets/001/alphafold3</code>.</p> <p>Once you have everything you need, you will be ready to run AlphaFold 3. We will now go through a test case adapted from the AlphaFold 3 GitHub Repository. From the working directory, create an output directory and a test input file:</p> <pre><code>mkdir af_output\nmkdir af_input\ntouch af_input/fold_input.json\n</code></pre> <p>Copy the following into <code>af_input/fold_input.json</code> (using <code>vim</code>, <code>emacs</code>, or <code>nano</code>):</p> fold_input.json<pre><code>{\n  \"name\": \"2PV7\",\n  \"sequences\": [\n    {\n      \"protein\": {\n        \"id\": [\"A\", \"B\"],\n        \"sequence\": \"GMRESYANENQFGFKTINSDIHKIVIVGGYGKLGGLFARYLRASGYPISILDREDWAVAESILANADVVIVSVPINLTLETIERLKPYLTENMLLADLTSVKREPLAKMLEVHTGAVLGLHPMFGADIASMAKQVVVRCDGRFPERYEWLLEQIQIWGAKIYQTNATEHDHNMTYIQALRHFSTFANGLHLSKQPINLANLLALSSPIYRLELAMIGRLFAQDAELYADIIMDKSENLAVIETLKQTYDEALTFFENNDRQGFIDAFHKVRDWFGDYSEQFLKESRQLLQQANDLKQG\"\n      }\n    }\n  ],\n  \"modelSeeds\": [1],\n  \"dialect\": \"alphafold3\",\n  \"version\": 1\n}\n</code></pre> <p>You can either run this in an interactive session or in a batch job. If you have access to a partition with a GPU, replace the partition name below as necessary:</p> InteractiveBatch <p>Request an interactive session with a GPU:</p> <pre><code>salloc -N 1 -n 16 -p mit_normal_gpu --gres=gpu:1\n</code></pre> <p>Run this script (<code>sh run_alphafold.sh</code>):</p> run_alphafold.sh<pre><code>#!/bin/bash\n\nmodule load apptainer\n\n# Enter the path to the AF3 dataset and container image:\nDATABASES_DIR=/orcd/datasets/001/alphafold3\nIMAGE_PATH=/orcd/software/community/001/container_images/alphafold3/20250321/alphafold3.sif\n\n# Enter the directory of the AF3 material:\nWORKDIR=~/af3\n\napptainer exec \\\n    --bind $WORKDIR/af_input:/root/af_input \\\n    --bind $WORKDIR/af_output:/root/af_output \\\n    --bind $WORKDIR/models:/root/models \\\n    --bind $DATABASES_DIR:/root/public_databases \\\n    --nv \\\n    $IMAGE_PATH \\\n    python /alphafold3/run_alphafold.py \\\n    --json_path=/root/af_input/fold_input.json \\\n    --model_dir=/root/models \\\n    --output_dir=/root/af_output \\\n    --db_dir=/root/public_databases\n</code></pre> <p>Create your batch job script:</p> run_alphafold.sbatch<pre><code>#!/bin/bash\n\n#SBATCH -N 1\n#SBATCH -n 16\n#SBATCH -p mit_normal_gpu\n#SBATCH --gres=gpu:1\n\nmodule load apptainer\n\n# Enter the path to the AF3 dataset and container image:\nDATABASES_DIR=/orcd/datasets/001/alphafold3\nIMAGE_PATH=/orcd/software/community/001/container_images/alphafold3/20250321/alphafold3.sif\n\n# Enter the directory of the AF3 material:\nWORKDIR=~/af3\n\napptainer exec \\\n    --bind $WORKDIR/af_input:/root/af_input \\\n    --bind $WORKDIR/af_output:/root/af_output \\\n    --bind $WORKDIR/models:/root/models \\\n    --bind $DATABASES_DIR:/root/public_databases \\\n    --nv \\\n    $IMAGE_PATH \\\n    python /alphafold3/run_alphafold.py \\\n    --json_path=/root/af_input/fold_input.json \\\n    --model_dir=/root/models \\\n    --output_dir=/root/af_output \\\n    --db_dir=/root/public_databases\n</code></pre> <p>Submit the batch job:</p> <pre><code>sbatch run_alphafold.sbatch\n</code></pre> <p>Output is saved to the <code>af_output</code> directory.</p>","tags":["Engaging","Howto Recipes"]},{"location":"recipes/build-vasp-gcc-cpu/","title":"Example Build of the VASP Software","text":"","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#about-vasp","title":"About VASP","text":"<p>VASP is a first principles simulation tool for electronic structure and quantum mechanical molecular dynamics computations. The name VASP is an acronym of Vienna Ab-initio Simulation Package. The VASP software is used in quantum chemistry to simulate the properties and structure of atomic scale materials. VASP can compute detailed atomic structure of molecules, finding terms such as bond lengths and vibration frequencies.</p>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#building-vasp-software","title":"Building VASP software","text":"<p>VASP is distributed as Fortran source code that must be compiled by end-users to create an executable program. This recipe describes how to compile VASP using the GNU compiler stack. The recipe shows commands for a Rocky Linux system.</p> <p>Prerequisites</p> <ul> <li>To use VASP a research group must obtain a license from the VASP team as described here here.</li> <li>This example assumes you are working with a Rocky Linux environment.</li> </ul>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#1-extract-vasp-source-code-files","title":"1. Extract VASP source code files","text":"<p>Once a licensed copy of VASP has been obtained the source code files must be extracted from the tar file that can be downloaded by license holders from the VASP portal site. The command</p> <pre><code>tar -xzvf vasp.6.4.3.tgz\n</code></pre> <p>will extract the source files and their directory tree. This command should be executed in a sub-directory where you will store the compiled VASP programs. </p> <p>Once the code has been extracted, switch to use the VASP directory for the remaining steps</p> <pre><code>cd vasp.6.4.3\n</code></pre> Tip <p>For different versions of VASP, the download file name and directory name will be different. In that case, remember to adjust the example commands above accordingly.</p>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#2-configure-the-compiler-options-file","title":"2. Configure the compiler options file","text":"<p>The VASP software is distributed with multiple example compiler options files.  These are in the sub-directory <code>arch/</code>.  For this example we will use the GNU compiler options file <code>makefile.include.gnu_omp</code>.  To activate the chosen options, copy the options file into the top-level VASP directory.</p> <pre><code>cp arch/makefile.include.gnu_omp makefile.include\n</code></pre>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#3-activate-the-relevant-modules","title":"3. Activate the relevant modules","text":"<p>To build the VASP program from the licensed source code several tools and libraries are needed.  The modules below add the needed software.  The <code>gcc</code> and <code>openmpi</code> modules provide compilers (gcc) and computational tools (openmpi)  needed for parallel computing with VASP.  The <code>lapack</code>, <code>scalapack</code>, <code>fftw</code> and <code>openblas</code> toos are numerical libraries that VASP uses.</p> <pre><code>module load gcc/12.2.0\nmodule load openmpi/4.1.4\nmodule load netlib-lapack/3.10.1\nmodule load netlib-scalapack/2.2.0\nmodule load fftw/3.3.10\nmodule load openblas/0.3.26\n</code></pre>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#4-set-environment-variables-that-are-needed-for-compilation","title":"4. Set environment variables that are needed for compilation","text":"<p>The compilation scripts that come with VASP include variables that must be set to the cluster's local values. Here we set environment variables to hold those settings.</p> <pre><code>SCALAPACK_ROOT=`module -t show  netlib-scalapack 2&gt;&amp;1 | grep CMAKE_PREFIX_PATH | awk -F, '{print $2}'  | awk -F\\\" '{print $2}'`\nFFTW_ROOT=`pkgconf --variable=prefix fftw3`\nOPENBLAS_ROOT=$(dirname `pkgconf --variable=libdir openblas`)\n</code></pre>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#5-compile-the-vasp-code","title":"5. Compile the VASP code","text":"<p>To compile the VASP code use the <code>make</code> program, passing it the environment variable settings as shown. The settings shown will also build the Fortran 90 modules that VASP includes.</p> <pre><code>make -j OPENBLAS_ROOT=$OPENBLAS_ROOT FFTW_ROOT=$FFTW_ROOT SCALAPACK_ROOT=$SCALAPACK_ROOT MODS=1 DEPS=1\n</code></pre>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#6-check-the-vasp-executables","title":"6. Check the VASP executables","text":"<p>The above commands should generate VASP executable programs <code>bin/vasp_std</code>, <code>bin/vasp_gam</code> and <code>bin/vasp_ncl</code>. To test that these programs can execute the following commands can be used:</p> <pre><code>export LD_LIBRARY_PATH=${OPENBLAS_ROOT}/lib:${FFTW_ROOT}/lib:${SCALAPACK_ROOT}/lib\nbin/vasp_std\n</code></pre> <p>If the code has compiled successfully the follow output should be generated. This output shows that the  VASP program can be run. The output shows an error because no input files have been configured.</p> <pre><code> -----------------------------------------------------------------------------\n|                                                                             |\n|     EEEEEEE  RRRRRR   RRRRRR   OOOOOOO  RRRRRR      ###     ###     ###     |\n|     E        R     R  R     R  O     O  R     R     ###     ###     ###     |\n|     E        R     R  R     R  O     O  R     R     ###     ###     ###     |\n|     EEEEE    RRRRRR   RRRRRR   O     O  RRRRRR       #       #       #      |\n|     E        R   R    R   R    O     O  R   R                               |\n|     E        R    R   R    R   O     O  R    R      ###     ###     ###     |\n|     EEEEEEE  R     R  R     R  OOOOOOO  R     R     ###     ###     ###     |\n|                                                                             |\n|     No INCAR found, STOPPING                                                |\n|                                                                             |\n|       ----&gt;  I REFUSE TO CONTINUE WITH THIS SICK JOB ... BYE!!! &lt;----       |\n|                                                                             |\n -----------------------------------------------------------------------------\n\n -----------------------------------------------------------------------------\n|                                                                             |\n|     EEEEEEE  RRRRRR   RRRRRR   OOOOOOO  RRRRRR      ###     ###     ###     |\n|     E        R     R  R     R  O     O  R     R     ###     ###     ###     |\n|     E        R     R  R     R  O     O  R     R     ###     ###     ###     |\n|     EEEEE    RRRRRR   RRRRRR   O     O  RRRRRR       #       #       #      |\n|     E        R   R    R   R    O     O  R   R                               |\n|     E        R    R   R    R   O     O  R    R      ###     ###     ###     |\n|     EEEEEEE  R     R  R     R  OOOOOOO  R     R     ###     ###     ###     |\n|                                                                             |\n|     No INCAR found, STOPPING                                                |\n|                                                                             |\n|       ----&gt;  I REFUSE TO CONTINUE WITH THIS SICK JOB ... BYE!!! &lt;----       |\n|                                                                             |\n -----------------------------------------------------------------------------\n\nSTOP 1\n</code></pre>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#7-example-scripts-to-compile-and-test","title":"7. Example scripts to compile and test","text":"<p>The commands above can be combined into scripts as shown below. This example scripts that can either be run from the command line or submitted to Slurm  as a batch job.</p> <p>The following script shows compiling VASP and testing that the build completed successfully. Place and run this script from the directory where you put the VASP source .tgz file.</p> <p>The call to <code>vasp_std</code> is expected to produce an error as in 6. Check the VASP executables above. To run a full VASP experiment problem specific inputs and parameters must be added to the script for running (see Running VASP below).</p> compile_and_test.sh<pre><code>#!/bin/bash\n#SBATCH --time=2:00:00\n#SBATCH --partition=mit_normal\n#SBATCH -c 8\n\ntar -xzvf vasp.6.4.3.tgz\ncd vasp.6.4.3\ncp arch/makefile.include.gnu_omp makefile.include\n\nmodule load gcc/12.2.0\nmodule load openmpi/4.1.4\nmodule load netlib-lapack/3.10.1\nmodule load netlib-scalapack/2.2.0\nmodule load fftw/3.3.10\nmodule load openblas/0.3.26\n\nSCALAPACK_ROOT=`module -t show  netlib-scalapack 2&gt;&amp;1 | grep CMAKE_PREFIX_PATH | awk -F, '{print $2}'  | awk -F\\\" '{print $2}'`\nFFTW_ROOT=`pkgconf --variable=prefix fftw3`\nOPENBLAS_ROOT=$(dirname `pkgconf --variable=libdir openblas`)\n\nmake -j OPENBLAS_ROOT=$OPENBLAS_ROOT FFTW_ROOT=$FFTW_ROOT SCALAPACK_ROOT=$SCALAPACK_ROOT MODS=1 DEPS=1\n\nexport LD_LIBRARY_PATH=${OPENBLAS_ROOT}/lib:${FFTW_ROOT}/lib:${SCALAPACK_ROOT}/lib\n\nsrun ./bin/vasp_std\n</code></pre>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#creating-a-vasp-module","title":"Creating a VASP Module","text":"<p>It can be convenient to create a module for VASP since it does have several dependencies. Below is an example modulefile. This modulefile assumes you have installed VASP 6.4.3, placed it in <code>$HOME/software/VASP</code>, and used the same dependency modules to build VASP as described in Step 3 above. If you have installed a different version of VASP, placed it in a different location, or used different dependency modules you will need to adjust the modulefile accordingly.</p> $HOME/software/modulefiles/vasp/6.4.3.lua<pre><code>-- -*- lua -*-\n--\n\nwhatis([[Name : vasp]])\nwhatis([[Version : 6.4.3]])\nwhatis([[Target : x86_64]])\nwhatis([[Short description : The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.]])\n\nlocal base = pathJoin(os.getenv(\"HOME\"),\"software/VASP/vasp.6.4.3\") \n\ndepends_on(\"gcc/12.2.0\")\ndepends_on(\"openmpi/4.1.4\")\n\nprepend_path(\"LD_LIBRARY_PATH\",\"/orcd/software/core/001/spack/pkg/openblas/0.3.26/ro5tivv/lib:/orcd/software/core/001/spack/pkg/fftw/3.3.10/dg7y4ph/lib:/orcd/software/core/001/spack/pkg/netlib-scalapack/2.2.0/ff5iskg/./lib\")\nprepend_path(\"PATH\", pathJoin(base,\"bin/\"), \":\")\n</code></pre>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/build-vasp-gcc-cpu/#running-vasp","title":"Running VASP","text":"<p>To run VASP create a job script like the one below in the same directory as your input files. You may need to increase <code>ntasks</code> or <code>cpus-per-task</code> or allocation additional resources depending on the size of the problem. This script assumes you have created a module and placed the modulefile in in <code>$HOME/software/modulefiles</code>. Update the location of your VASP module as needed. VASP has a page of examples in their documentation that can be used for testing.</p> run_vasp.sh<pre><code>#!/bin/bash\n#SBATCH --time=2:00:00\n#SBATCH --partition=mit_normal\n#SBATCH --ntasks=4 # Number of VASP processes\n#SBATCH --cpus-per=task=2 # Number of threads per VASP process\n\n# Limit the number of threads to the number of cpus requested\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n# Load the VASP module\nmodule use $HOME/software/modulefiles\nmodule load vasp/6.4.3\n\n# Run VASP\nsrun vasp_std\n</code></pre> <p>Note</p> <p>During testing we found that VASP has a tendency to create a very large number of threads that can slow down the calculations and cause them to hang. To prevent that we've set the <code>$OMP_NUM_THREADS</code> environment variable to the number of <code>cpus-per-task</code> requested (<code>$SLURM_CPUS_PER_TASK</code>).</p>","tags":["Engaging","Howto Recipes","MPI","VASP","Rocky Linux","Install Recipe"]},{"location":"recipes/gromacs/","title":"Installing and Using GROMACS","text":"<p>GROMACS is a free and open-source software suite for high-performance molecular dynamics and output analysis.</p> <p>You can learn about GROMACS here: https://www.gromacs.org/.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes","Install Recipe"]},{"location":"recipes/gromacs/#install-gromacs-with-mpi","title":"Install GROMACS with MPI","text":"EngagingSuperCloud <p>Select a version on the GROMACS website, then download and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget --no-check-certificate http://ftp.gromacs.org/pub/gromacs/gromacs-2019.6.tar.gz\ntar xvfz gromacs-2019.6.tar.gz\n</code></pre></p> <p>Load MPI and Cmake modules, <pre><code>module load engaging/openmpi/2.0.3 cmake/3.17.3\n</code></pre></p> <p>Create build and install directories, <pre><code>mkdir -p 2019.6/build\nmkdir 2019.6/install\ncd 2019.6/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2019.6 -DGMX_MPI=ON -DCMAKE_INSTALL_PREFIX=~/gromacs/2019.6/install\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage environment, <pre><code>source ~/gromacs/2019.6/install/bin/GMXRC\n</code></pre></p> <p>Select a version on the GROMACS website, then download and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget http://ftp.gromacs.org/pub/gromacs/gromacs-2023.2.tar.gz\ntar xvfz gromacs-2023.2.tar.gz\n</code></pre></p> <p>Load CUDA, Anaconda and MPI modules, <pre><code>module load cuda/11.8 anaconda/2023a\nmodule load mpi/openmpi-4.1.5\n</code></pre></p> <p>Create build and install directories, <pre><code>mkdir -p 2023.2/build\nmkdir 2023.2/install\ncd 2023.2/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2023.2 -DGMX_MPI=ON -DGMX_GPU=CUDA -DCMAKE_INSTALL_PREFIX=~/gromacs/2023.2-gpu\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage environment before running GROMACS programs, <pre><code>source ~/gromacs/2023.2/install/bin/GMXRC\n</code></pre></p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes","Install Recipe"]},{"location":"recipes/gromacs/#run-gromacs","title":"Run GROMACS","text":"<p>Firstly, prepare for an input file. Refer to file formats. Here shows an example with an input file named <code>benchPEP-h.tpr</code> downloaded from this page. In these examples we have saved the input files in the <code>~/gromacs/bench/</code> directory.</p> <p>Secondly, create a batch job script, for example, named <code>job.sh</code>.</p> EngagingSuperCloud <p>This job script requests 2 nodes with a total of 8 CPU cores and 50GB of memory.</p> job.sh<pre><code>#!/bin/bash\n#SBATCH --job-name=\"production run\"\n#SBATCH --partition=sched_mit_hill\n#SBATCH --constraint=centos7\n#SBATCH --mem=50G\n#SBATCH -N 2\n#SBATCH --ntasks 8\n#SBATCH --time=12:00:00\n\n\nmodule purge\nmodule load gromacs/2018.4\n\ngmx_mpi=/home/software/gromacs/2018.4/bin/gmx_mpi\n\nif [ -n \"$SLURM_CPUS_PER_TASK\" ]; then\n    ntomp=\"$SLURM_CPUS_PER_TASK\"\nelse\n    ntomp=\"1\"\nfi\n\n\n# setting OMP_NUM_THREADS to the value used for ntomp to avoid complaints from gromacs\nexport OMP_NUM_THREADS=$ntomp\n\nmpirun -np $SLURM_NTASKS $gmx_mpi mdrun -ntomp $ntomp -deffnm ~/gromacs/bench/benchPEP-h -v\n</code></pre> <p>This job requests 2 nodes with 4 CPU cores and 2 GPUs per node.</p> job.sh<pre><code>#!/bin/bash\n#SBATCH --nodes=2              # 2 nodes\n#SBATCH --ntasks-per-node=2    # 2 MPI tasks per node\n#SBATCH --cpus-per-task=2      # 2 CPU cores per task\n#SBATCH --gres=gpu:volta:2     # 2 GPUs per node\n#SBATCH --time=01:00:00        # 1 hour\n\n\n# Load required modules\nmodule load cuda/11.8 mpi/openmpi-4.1.5\n\n# Enable direct GPU to GPU communications\nexport GMX_ENABLE_DIRECT_GPU_COMM=true\n\n# Activate user install of GROMACS\nsource ~/gromacs/2023.2-gpu/bin/GMXRC\n\n# Check MPI, GPU and GROMACS\nmpirun hostname\nnvidia-smi\nwhich gmx_mpi\n\n# Run GROMACS\nmpirun gmx_mpi mdrun -s ~/gromacs/bench/benchPEP-h.tpr -ntomp ${SLURM_CPUS_PER_TASK} -pme gpu -update gpu -bonded gpu -npme 1\n</code></pre> <p>Finally, submit the job, <pre><code>sbatch job.sh\n</code></pre></p> <p>Refer to GROMACS user guide for more info.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes","Install Recipe"]},{"location":"recipes/h100_getting_started/","title":"Getting started on 8-way H100 nodes on Satori","text":"<p>This page provides information on how to run a first job on the IBM Watson AI Lab H100 GPU nodes on Satori. The page describes how to request an access to the Slurm partition associated  with the H100 nodes and how to run a first example pytorch script on the systems. </p> <p>A first set of H100 GPU systems has been added to Satori. These are for priority use by IBM Watson AI Lab research collaborators. They are also available for general opportunistic use when they are idle.</p> <p>Currently ( 2023-06-19 ) there are 4 H100 systems installed.  Each system has 8 H100 GPU cards, two Intel 8468 CPUs each with 48 physical cores and 1TiB of main memory.</p> <p>Below are some instructions for getting started with these systems. </p>","tags":["Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#access-to-the-nodes","title":"Access to the nodes","text":"<p>To access the nodes in the priority group you need your satori login id to be listed in the WebMoira  group https://groups.mit.edu/webmoira/list/sched_oliva.  Either Alex Andonian and Vincent Sitzmann are able to add accounts to the <code>sched_oliva</code> Moira list.</p>","tags":["Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#interactive-access-through-slurm","title":"Interactive access through Slurm","text":"<p>To access an entire node through Slurm, the command below can be used from the satori login node</p> <pre><code>srun -p sched_oliva --gres=gpu:8 -N 1 --mem=0 -c 192 --time 1:00:00 --pty /bin/bash\n</code></pre> <p>this command will launch an interactive shell on one of the nodes (when a full node becomes available).  From this shell the NVidia status command  <pre><code>nvidia-smi\n</code></pre> should list 8 H100 GPUs as available. </p> <p>Single node, multi-gpu training examples (for example https://github.com/artidoro/qlora ) should run  on all 8 GPUs. </p> <p>To use a single GPU interactively the following command can be used <pre><code>srun -p sched_oliva --gres=gpu:1 --mem=128 -c 24 --time 1:00:00 --pty /bin/bash\n</code></pre></p> <p>this will request a single GPU. This request will allow other Slurm sessions to run on other GPUs  simultaneously with this session.</p>","tags":["Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-nightly-build-pytorch-example-with-a-fresh-miniconda-and-pytorch","title":"Running a nightly build pytorch example with a fresh miniconda and pytorch","text":"<p>A miniconda environment can be used to run the latest nightly build pytorch code on these  systems. To do this, first create a software install directory and install the needed pytorch software</p> <pre><code>mkdir -p /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre> <p>and then switch your shell to that directory. <pre><code>cd /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre></p> <p>now install miniconda and create an environment with the needed software <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \nchmod +x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh -b -p minic\n. ./minic/bin/activate \nconda create -y -n pytorch_test python=3.10\nconda activate pytorch_test                          \nconda install -y -c conda-forge cupy\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n</code></pre></p> <p>Once the software is installed, the following script can be used to test the installation. test.py<pre><code>import torch\ndevice_id = torch.cuda.current_device()\ngpu_properties = torch.cuda.get_device_properties(device_id)\nprint(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n          \"%.1fGb total memory.\\n\" % \n          (torch.cuda.device_count(),\n          device_id,\n          gpu_properties.name,\n          gpu_properties.major,\n          gpu_properties.minor,\n          gpu_properties.total_memory / 1e9))\n</code></pre> Run this script with <pre><code>python test.py\n</code></pre></p> <p>To exit the Slurm srun session enter the command <pre><code>exit\n</code></pre></p>","tags":["Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-simple-batch-script-using-an-installed-miniconda-environment","title":"Running a simple batch script using an installed miniconda environment","text":"<p>To run a batch script on one of the H100 nodes in partition <code>sched_oliva</code> first paste the content in the  box below into a slurm script file called, for example, <code>test_script.slurm</code> ( change the <code>RUNDIR</code> setting to assign the  path to a directory where you have already installed a conda environment in a sub-directory called <code>minic</code> ).</p> <p>First create the <code>mytest.py</code> script with the contents above.</p> <pre><code>#!/bin/bash\n#\n#SBATCH --gres=gpu:8\n#SBATCH --partition=sched_oliva\n#SBATCH --time=1:00:00\n#SBATCH --mem=0\n#\n\nnvidia-smi\n\nRUNDIR=/nobackup/users/${USER}/h100-testing/minic\ncd ${RUNDIR}\n\n. ./minic/bin/activate\n\nconda activate pytorch_test\n\npython mytest.py\n</code></pre> <p>This script can then be submitted to Slurm to run in a background batch node using the command.</p> <pre><code>sbatch &lt; test_script.slurm\n</code></pre>","tags":["Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#getting-help","title":"Getting help","text":"<p>As always, please feel welcome to email orcd-help@mit.edu with questions, comments or suggestions. We would be happy to hear from you!</p>","tags":["Howto Recipes","H100"]},{"location":"recipes/intel/","title":"Intel compiler","text":"<p>The Intel compiler is optimized for Intel CPUs. It provides the Math Kernel Library (MKL) in which linear algebra computations are optimized. The performance of C and Fortran codes can be improved on Intel CPUs if compiled with the Intel compiler. It provides an MPI implemetation for MPI programs that run on multipe nodes. Users should choose the Intel compiler for Intel CPUs when possible. </p>","tags":["C","Fortran","Howto Recipes"]},{"location":"recipes/intel/#set-up-environment","title":"Set up environment","text":"Rocky 8 nodesCentOS 7 nodes <p>If you use Rocky 8 nodes, log in to an appropriate head node first:</p> <pre><code>ssh &lt;user&gt;@orcd-login003.mit.edu\n</code></pre> <p>Load an Intel module:</p> <pre><code>module load intel/2024.2.1\n</code></pre> <p>Check commands for the Intel compiler and MPI and environment variables for MKL are ready for use:</p> <pre><code>$ which icx\n/orcd/software/community/001/rocky8/intel/2024.2.1/compiler/2024.2/bin/icx\n$ which ifort\n/orcd/software/community/001/rocky8/intel/2024.2.1/compiler/2024.2/bin/ifort\n$ which mpiicx\n/orcd/software/community/001/rocky8/intel/2024.2.1/mpi/2021.13/bin/mpiicx\n$ which mpiifort\n/orcd/software/community/001/rocky8/intel/2024.2.1/mpi/2021.13/bin/mpiifort\n$ echo $MKLROOT\n/orcd/software/community/001/rocky8/intel/2024.2.1/mkl/2024.2\n</code></pre> <p>If you use CentOS 7 nodes, log in to an appropriate head node first:</p> <pre><code>ssh &lt;user&gt;@orcd-vlogin003.mit.edu\n</code></pre> <p>Load the modules for the Intel compiler, Intel MPI and MKL:</p> <pre><code>module load intel/2018-01\nmodule load impi/2018-01\nmodule load mkl/2018-01 \n</code></pre> <p>Check commands for the Intel compiler and MPI and environment variables for MKL are ready for use:</p> <pre><code>$ which icc\n/home/software/intel/2018-01/bin/icc\n$ which ifort\n/home/software/intel/2018-01/bin/ifort\n$ which mpiicc\n/home/software/intel/2018-01/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/mpiicc\n$ which mpiifort\n/home/software/intel/2018-01/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/mpiifort\n$ echo $MKLROOT\n/home/software/intel/2018-01/compilers_and_libraries_2018.1.163/linux/mkl/\n</code></pre>","tags":["C","Fortran","Howto Recipes"]},{"location":"recipes/intel/#compile-and-run-programs-with-intel-compiler","title":"Compile and run programs with Intel compiler","text":"Rocky 8 nodesCentOS 7 nodes <p>Once the environment is set up, you can compile your C or Fortran codes like this:</p> <pre><code>icx -O3 name.c -o name\nifort -O3 name.f90 -o name\n</code></pre> <p>or MPI codes:</p> <pre><code>mpiicx -O3 name.c -o name\nmpiifort -O3 name.f90 -o name\n</code></pre> <p>If you use GNU Make to build your program, set up the varialbes in the Makefile:</p> <pre><code>CC=icx\nFC=ifort\nMPICC=mpiicx\nMPIFC=mpiifort\n</code></pre> <p>Use the variable <code>MKLROOT</code> in the Makefile when needed.</p> <p>Finally, submit a job script specifying a partition with <code>-p &lt;partition-name&gt;</code> and loading the Intel module:</p> <pre><code>module load intel/2024.2.1\n</code></pre> <p>Once the environment is set up, you can compile your C or Fortran codes like this:</p> <pre><code>icc -O3 name.c -o name\nifort -O3 name.f90 -o name\n</code></pre> <p>or MPI codes:</p> <pre><code>mpiicc -O3 name.c -o name\nmpiifort -O3 name.f90 -o name\n</code></pre> <p>If you use GNU Make to build your program, set up the varialbes in the Makefile:</p> <pre><code>CC=icc\nFC=ifort\nMPICC=mpiicc\nMPIFC=mpiifort\n</code></pre> <p>Use the variable <code>MKLROOT</code> in the Makefile when needed.</p> <p>Finally submit a job script specifying a partition with <code>-p &lt;partition-name&gt;</code> and loading the Intel module:</p> <pre><code>module load intel/2018-01\nmodule load impi/2018-01\nmodule load mkl/2018-01 \n</code></pre>","tags":["C","Fortran","Howto Recipes"]},{"location":"recipes/intel/#references","title":"References","text":"<p>Refer to the following references for more details on logging in, compiling C/Fortran codes, using GNU make, and using partitions in Slurm job scheduler:</p> <p>Log in to the system. </p> <p>Compile C/Fortran Codes and Use GNU Make. </p> <p>Use Slurm to submit jobs. </p>","tags":["C","Fortran","Howto Recipes"]},{"location":"recipes/julia_install/","title":"Julia install","text":"<p>This page has moved to the Julia software page.</p>"},{"location":"recipes/jupyter/","title":"Jupyter Notebooks","text":"<p>Jupyter notebooks provide a way to run code in an interactive environment. While most prominently used for Python, Jupyter supports a range of languages, such as Julia and R.</p>","tags":["Jupyter","Best Practices"]},{"location":"recipes/jupyter/#choosing-an-approach","title":"Choosing an Approach","text":"<p>There are multiple ways to run Jupyter notebooks on the computing clusters available through ORCD. The route you choose depends on your needs and level of familiarity with high performance computing environments.</p>","tags":["Jupyter","Best Practices"]},{"location":"recipes/jupyter/#web-portal","title":"Web Portal","text":"<p>The most straightforward way to run a Jupyter notebook on one of our computing clusters is to use the cluster's web portal. While this route is the easiest to set up, it can be limiting if you want more control over your environment or the resources allocated to your notebook.</p> EngagingSuperCloud <ul> <li> <p>Link to web portal: https://engaging-ood.mit.edu/</p> </li> <li> <p>Select \"Interactive Apps\" --&gt; \"Jupyter Notebook\"</p> </li> <li> <p>Follow the on-screen instructions to start a session. You are able to use a custom Conda environment provided it has <code>jupyterlab</code> installed.</p> </li> <li> <p>If you'd like to run Julia, enter the name of the Julia module you're using (e.g., <code>julia/1.8.5</code>). Note that you need to have <code>IJulia</code> installed in your environment for this version of Julia.</p> </li> <li> <p>If you'd like to run R, enter the name of your custom Conda environment that has <code>r-irkernel</code> installed.</p> </li> <li> <p>When the session is ready, click \"Connect to Jupyter.\" From here you can create a Jupyter notebook and select the language you would like to use.</p> </li> </ul> <p>Note</p> <p>The Engaging web portal is currently running on CentOS 7, which has a different set of modules from Rocky 8 nodes. If you would like to run a Jupyter notebook on Rocky 8, you will need to follow either the VS Code or port forwarding method.</p> <ul> <li> <p>Link to web portal: https://txe1-portal.mit.edu/</p> </li> <li> <p>Select \"jupyter\" and follow the on-screen instructions to create a Jupyter notebook. When you open a notebook, select the kernel for your desired language.</p> </li> </ul> <p>See the SuperCloud documentation on Jupyter for more information.</p>","tags":["Jupyter","Best Practices"]},{"location":"recipes/jupyter/#vs-code","title":"VS Code","text":"<p>First, follow these instructions to set up VS Code to run on a compute node.</p> <p>Open a Jupyter notebook and click the top right button to select a kernel. You can select \"Python Environments\" for any Conda environments or \"Jupyter Kernel\" to find Julia or R environments. If you have installed R with Conda, you can find your Conda environment under \"Jupyter Kernel.\" <code>jupyterlab</code> must be installed to your Conda environment.</p>","tags":["Jupyter","Best Practices"]},{"location":"recipes/jupyter/#port-forwarding","title":"Port Forwarding","text":"<p>Port forwarding offers the most flexibility in setting up your Jupyter notebook but the setup is slightly more involved. With port forwarding, the rendering is handled through your internet browser while computation is done on the cluster. This method is more lightweight than VS Code and can be more reliable.</p> <p>Port forwarding consists of running the notebook on a compute node and then accessing the notebook on your local machine by SSH tunnelling through a login node.</p> <p>First, request a compute node with the resources you'll need for your Jupyter session (here we are requesting 1 node with 4 CPU cores):</p> EngagingSuperCloud <pre><code>salloc -N 1 -c 4 -p mit_normal\n</code></pre> <pre><code>LLsub -i -s 4\n</code></pre> <p>Note</p> <p>See Requesting Resources for more information.</p> <p>Make a note of the node that your job is running on by running <code>hostname</code> from the command line.</p> <p>Even if you are using a different language with Jupyter, Jupyter is tightly linked to Python, so you will need to use a Conda environment with <code>jupyterlab</code> installed:</p> EngagingSuperCloud <pre><code>module load miniforge\nconda create -n jupyter_env jupyterlab\nconda activate jupyter_env\n</code></pre> <pre><code>module load anaconda\nconda create -n jupyter_env jupyterlab\nconda activate jupyter_env\n</code></pre> <p>Now, we can run the notebook. To be able to access it on our local machine, we need to add a few arguments:</p> <pre><code>port=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\njupyter-lab --ip=0.0.0.0 --port=$port\n</code></pre> <p>The port can be any number between 1024 and 65535, and the first command above ensures getting an open port.</p> <p>When you run the notebook, the output will contain a link with a token that allows you to access the notebook, which will look like the following:</p> <p></p> <p>Make sure to select the second URL that is provided as outlined in yellow above.  We cannot use this link right away because that node is not available from our local machine. Through \"tunneling,\" however, we can access this node through a login node, which is accessble from our local machine.</p> <p>In a second terminal window on your local machine, set up an SSH tunnel to your Jupyter notebook that's running on the compute node, filling in the node name, port number, and username as necessary. We will keep the local port and the remote port the same for simplicity.</p> EngagingSuperCloud <pre><code>ssh -L &lt;port&gt;:&lt;node&gt;:&lt;port&gt; &lt;USER&gt;@orcd-login001.mit.edu\n</code></pre> <pre><code>ssh -L &lt;port&gt;:&lt;node&gt;:&lt;port&gt; &lt;USER&gt;@txe1-login.mit.edu\n</code></pre> <p>Now you can access Jupyter in an internet browser using the link we received above.</p> <p>Now you can open a Jupyter notebook and select your kernel from the top right corner. The Python environment is the same environment you used to run the notebook.</p>","tags":["Jupyter","Best Practices"]},{"location":"recipes/jupyter/#language-specific-instructions","title":"Language-Specific Instructions","text":"","tags":["Jupyter","Best Practices"]},{"location":"recipes/jupyter/#julia","title":"Julia","text":"<p>You will need to add the <code>IJulia</code> package to your environment for Jupyter to recognize the Julia kernel. You can do so from the command line:</p> <p>Bash<pre><code>module load julia\njulia\n</code></pre> Julia<pre><code>using Pkg\nPkg.add(\"IJulia\")\nPkg.build(\"IJulia\")\n</code></pre></p> <p>Unlike R, Julia environments are separate from Conda. However, if the <code>IJulia</code> package is installed, then the Julia kernel should be visible regardless of the Conda environment from which you run your Jupyter notebook.</p> <p>See our Julia documentation for more information.</p>","tags":["Jupyter","Best Practices"]},{"location":"recipes/jupyter/#r","title":"R","text":"<p>To run R in a Jupyter notebook, you need to create a Conda environment with both <code>r-irkernel</code> and <code>jupyterlab</code> installed:</p> EngagingSuperCloud <pre><code>module load miniforge\nconda create -n r_jupyter_env jupyterlab r-irkernel\nconda activate r_jupyter_env\n</code></pre> <pre><code>module load anaconda\nconda create -n r_jupyter_env jupyterlab r-irkernel\nconda activate r_jupyter_env\n</code></pre> <p>Most R packages are available through Conda, so feel free to install other packages you need to this environment.</p> <p>See our R documentation for more information.</p>","tags":["Jupyter","Best Practices"]},{"location":"recipes/jupyter/#python","title":"Python","text":"<p>To run Python Jupyter notebooks, install <code>jupyterlab</code> to whatever Conda environment that contains the packages you need.</p> <p>See our Python documentation for more information.</p>","tags":["Jupyter","Best Practices"]},{"location":"recipes/jupyter/#faqs","title":"FAQs","text":"<p>How do I run a Jupyter notebook with a GPU?</p> <p>The cluster web portals offer an option to allocate a GPU to your Jupyter session. If you would like to use a different partition, however, then follow the instructions for VS Code or port forwarding and request a GPU in your Slurm job. See our documentation on requesting resources for more information.</p> <p>Jupyter does not recognize the kernel for my environment. What do I do?</p> <p>First, make sure you have <code>r-irkernel</code> installed if you're using R, <code>IJulia</code> installed (and built) if you're using Julia, and <code>jupyterlab</code> installed to your Conda environment.</p> <p>On VS Code, you may need to specify the path to the <code>conda</code> binary of the Conda installation you're using. This can be done by editing the \"Python: Conda Path\" setting. For example, if you're using the <code>miniforge/24.3.0-0</code> module on Engaging, the path would be:</p> <pre><code>/orcd/software/core/001/pkg/miniforge/24.3.0-0/condabin/conda\n</code></pre> <p>To see all kernels that Jupyter recognizes, activate a Conda environment with <code>jupyterlab</code> installed and run <code>jupyter kernelspec list</code>.</p> <p>I tried to install <code>jupyterlab</code> to my Conda environment, but the installation failed. How can I run a Jupyter notebook with the dependencies I need?</p> <p>It's best to install the packages you need when you create a Conda environment rather than one-by-one after the environemnt has been created. This will make Conda more likely to solve your environment succesfully. For example:</p> <pre><code>conda create -n jupyter_env jupyterlab pandas pytorch\n</code></pre> <p>See Conda Environments for more information.</p>","tags":["Jupyter","Best Practices"]},{"location":"recipes/many-jobs/","title":"Advanced Job Array","text":"<p>Users often need to submit many jobs to run a program many times with different input parameters or files.  </p> <p>It is straightforward to execute the command <code>sbatch</code> in a loop, but this approach is inefficient for job scheduling. When the iteration number is large, it will slow down the Slurm scheduler and affect all users.</p> <p>A good practice is to use a job array, which is much more efficient. Refer to this page for details of job array. </p> <p>However, if a user submits too many jobs in a short time, even with a job array, it will still slow down the Slurm scheduler. The maximum number of jobs per user is set to be 500, so that there are not too many jobs queuing in a given time period. A user can submit up to 500 jobs with a job array.</p> Terminology: job and job array <p>On this page, the word job means either an individual job or a task in a job array. For example, submitting a job array with 100 tasks means submitting 100 jobs. </p> <p>When a user needs to run a program more than 500 times, it is recommended to combine serial execution and/or parallel execution with a job array.</p> <p>On this page, we will introduce serial execution and parallel execution, and how to integrate them with a job array. A simple Python code named mycode.py is used for all examples. </p>","tags":["Engaging","Slurm","Howto Recipes"]},{"location":"recipes/many-jobs/#serial-execution","title":"Serial execution","text":"<p>Serial execution means executing multiple programs serially within a job. Here is an example job script that runs 10 programs serially. </p> <pre><code>#!/bin/bash\n#SBATCH -t 02:00:00    # tow hours\n#SBATCH -N 1           # 1 node\n#SBATCH -n 2           # 2 CPU cores\n#SBATCH --mem=2GB      # 2 GB of memory\n\nmodule load miniforge/24.3.0-0\n\nN_PROGRAMS=10\nfor i in `seq 1 $N_PROGRAMS`   # Loop for serial execution\ndo\n   python mycode.py  $i    # Run the program serially\ndone\n</code></pre> <p>Each program uses 2 CPU cores and 2 GB of memory.</p> <p>The program is executed 10 times with different input parameters (i.e. the loop index ) each time. The next execution will start after the current execution is completed. </p> <p>Note that the serial execution increases the total run time. Request a wall time that is long enough for all the programs to complete. </p> <p>Note</p> <p>This approach is good for programs with short run times. If each program requires a long run time, the total run time may exceed the maximum wall time limit.    </p>","tags":["Engaging","Slurm","Howto Recipes"]},{"location":"recipes/many-jobs/#integrate-serial-execution-and-job-array","title":"Integrate serial execution and job array","text":"<p>To scale up the number of programs, use a job array together with serial execution. Here is an example job script.</p> <pre><code>#!/bin/bash\n#SBATCH -t 02:00:00             # Two hours\n#SBATCH -N 1                    # 1 node\n#SBATCH -n 2                    # 2 CPU cores\n#SBATCH --mem=2GB               # 2 GB of memory\n#SBATCH --array=0-99            # Job array \n\nmodule load miniforge/24.3.0-0\n\nnmax=$SLURM_ARRAY_TASK_COUNT    # Num of tasks per array\nid=$SLURM_ARRAY_TASK_ID         # Task ID\n\nN_PROGRAMS=10\nfor i in `seq 1 $N_PROGRAMS`             # Loop for serial execution\ndo\n    index=$(( $nmax * ($i -1) + $id ))   # Calculate the global index\n    python mycode.py $index     # Run the program serially, using the global index as input\ndone\n</code></pre> <p>The number of jobs (tasks) of the array is 100 and each job runs 10 programs serially, so <code>10 * 100 = 1,000</code> programs are submitted.</p> <p>The array task ID (<code>$SLURM_ARRAY_TASK_ID</code>) and total number of tasks in the array (<code>$SLURM_ARRAY_TASK_COUNT</code>) are used to calculate the global index. Use the global index as the input parameter for the program.</p> <p>Note</p> <p>This approach is useful for submitting a large number of short-run-time programs.</p> <p>Also refer to this page for more examples of integrating serial execution and job array.</p>","tags":["Engaging","Slurm","Howto Recipes"]},{"location":"recipes/many-jobs/#parallel-execution","title":"Parallel execution","text":"<p>Parallel execution means executing multiple programs in parallel within a job. Here is an example job script that runs 10 programs in parallel.  <pre><code>#!/bin/bash\n#SBATCH -t 00:30:00    # 30 minutes\n#SBATCH -N 1           # 1 node\n#SBATCH -n 20          # 20 CPU cores\n#SBATCH --mem=20GB     # 20 GB of memory\n\nmodule load miniforge/24.3.0-0\n\nN_PROGRAMS=10\nfor i in `seq 1 $N_PROGRAMS` # Loop from 1 to number of programs\ndo\n   python mycode.py $i &amp;    # Run a program parallely in background\ndone\nwait   # Wait for all programs to be completed, then exit the batch job. \n</code></pre></p> <p>Each program uses 2 CPU cores and 2 GB of memory, so the job requests 20 CPU cores and 20 GB of memory for 10 programs. </p> <p>Here the main difference from the serial execution is that an <code>&amp;</code> mark is added at the end of the execution command, which puts the program in the background, and all 10 programs start to run almost simultaneously.</p> <p>The <code>wait</code> command in the last line ensures that the batch job will not be terminated until all background programs are completed.  </p> <p>Note</p> <p>This approach is good when each program requires a small number of CPU cores and a small amount of memory. If each program requires many CPU cores or large memory, executing multiple programs in parallel would require too many CPUs or too much memory, which may not fit within one node. </p>","tags":["Engaging","Slurm","Howto Recipes"]},{"location":"recipes/many-jobs/#integrate-parallel-execution-and-job-array","title":"Integrate parallel execution and job array","text":"<p>To scale up the number of programs, use a job array together with parallel execution. Here is an example job script.</p> <pre><code>#!/bin/bash\n#SBATCH -t 02:00:00             # Two hours\n#SBATCH -N 1                    # 1 node\n#SBATCH -n 2                    # 2 CPU cores\n#SBATCH --mem=2GB               # 2 GB of memory\n#SBATCH --array=0-99            # Job array \n\nmodule load miniforge/24.3.0-0\n\nnmax=$SLURM_ARRAY_TASK_COUNT    # Num of tasks per array\nid=$SLURM_ARRAY_TASK_ID         # Task ID\n\nN_PROGRAMS=10\nfor i in `seq 1 $N_PROGRAMS`    # Loop for serial execution\ndo\n    index=$(( $nmax * ($i - 1) + $id ))   # Calculate the global index\n    python mycode.py $index  &amp;   # Run a program parallely in background, using the global index as input\ndone\nwait     # Wait for all programs to be completed, then exit the batch job. \n</code></pre> <p>The number of jobs (tasks) of the array is 100 and each job runs 10 programs in parallel, so <code>10 * 100 = 1,000</code> programs are submitted.</p> <p>Note</p> <p>This approach is useful to submit a large number of programs, when each program requires small resources (CPUs and memory).</p>","tags":["Engaging","Slurm","Howto Recipes"]},{"location":"recipes/many-jobs/#integrate-sequential-execution-parallel-execution-and-job-array","title":"Integrate sequential execution, parallel execution, and job array","text":"<p>To further scale up the number of programs, one may consider integrating sequential execution, parallel execution, and a job array. Here is an example job script. </p> <pre><code>#!/bin/bash\n#SBATCH -t 02:00:00       # Two hours\n#SBATCH -N 1              # 1 node\n#SBATCH -n 20             # 20 CPU cores\n#SBATCH --mem=20GB        # 20 GB of memory\n#SBATCH --array=0-99      # Job array \n\nmodule load miniforge/24.3.0-0\n\nnmax=$SLURM_ARRAY_TASK_COUNT     # Num of tasks per array\nid=$SLURM_ARRAY_TASK_ID          # Task ID\n\nN_SERIAL=10\nN_PARALLEL=10\nfor i in `seq 1 $N_SERIAL`        # Loop for serial executions\ndo\n   for j in `seq 1 $N_PARALLEL`   # Loop for parallel executions\n   do\n     index=$(( $nmax * ( $N_PARALLEL * ($i-1) + ($j-1) ) + $id ))  # Global index\n     python mycode.py $index &amp;    # Run a program in the background, using the global index as input. \n   done \n   wait    # Wait for all parallel executions to complete, then go to the next iteration in the loop of serial executions.\ndone \n</code></pre> <p>The number of jobs (tasks) of the array is 100 and each job runs <code>10 * 10 = 100</code> programs in the nested loops of serial and parallel executions, so <code>100 * 100 = 10,000</code> programs are submitted.</p> <p>Note</p> <p>This approach is useful for submitting a large number of programs, when each program requires a short run time and small resources (CPUs and memory).</p>","tags":["Engaging","Slurm","Howto Recipes"]},{"location":"recipes/mpi/","title":"Message Passing Interface (MPI)","text":"<p>Message Passing Interface (MPI) is a standard designed for data communication in parallel computing. The MPI standard defines useful library functions/routines in C, C++, and Fortran. Python interface is available for MPI.   </p> <p>There are several MPI implementationos, such as <code>OpenMPI</code>, <code>MPICH</code>, <code>MVAPICH</code>, and <code>Intel MPI</code>, which work with Infiniband network for high-bandwidth data communication.</p> Engaging","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#mpi-modules","title":"MPI modules","text":"<p>There are OpenMPI modules available on the cluster. Before building or runrning your MPI programs, load the modules of a <code>gcc</code> compiler and an <code>openmpi</code> libraries to set up environment varialbes.</p> <p>There are two different operations systems (OS) on the cluster: CentOS 7 and Rocky 8. For CentOS 7 nodes, load these modules, <pre><code>module load gcc/6.2.0 openmpi/3.0.4\n</code></pre> or <pre><code>module load gcc/9.3.0 openmpi/4.0.5\n</code></pre> For Rocky 8 nodes, load these modules, <pre><code>module use /orcd/software/community/001/modulefiles\nmodule load gcc/12.2.0 openmpi/4.1.4-pmi-ucx-x86_64\n</code></pre> If you need to run MPI programs with GPUs, load these modules instead, <pre><code>module load gcc/12.2.0 openmpi/4.1.4-pmi-cuda-ucx-x86_64\n</code></pre></p> <p>Note</p> <p>Load a <code>gcc</code> module first, then the openmpi modules built with this <code>gcc</code> will be shown in the output of <code>module avail</code> and can be loaded. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#build-mpi-programs","title":"Build MPI programs","text":"<p>This section will be focused on building MPI programs in C or Fortran. Python users can refer to this page for using the <code>mpi4py</code> package.</p> <p>Most MPI software should be built from source codes. First, download the package from the internet. Load one of the OpenMPI modules mentioned above. A typical building process is like this, <pre><code>./configure CC=mpicc CXX=mpicxx --prefix=&lt;/path/to/your/installation&gt;\nmake\nmake install\n</code></pre> Create an install directory and assign its full path to the flag <code>--prefix</code>. This is where the binaries will be saved.</p> <p>Widely-used MPI software includes <code>Gromacs</code>, <code>Lammps</code>, <code>NWchem</code>, <code>OpenFOAM</code> and many others. The building process of every software is different. Refer to its official installation guide for details.</p> Side note: MPI binaries <p>Some MPI software are provided with prebuilt binaries only. In this case, download the binaries compatible with the <code>linux</code> OS and the <code>x86_64</code> CPU architecture. If possible, try to choose an OpenMPI version, that the binary was built with, as close as possible to that of a module on the cluster. This type of MPI software includes <code>ORCA</code>. </p> <p>Spack is a popular tool to build many software packages systematically on clusters. It makes building processes convenient in many cases. If you want to use Spack to build your software package on the cluster, refer to the recipe page for Spack.</p> <p>If you develop your MPI codes, the codes can be compiled and linked like this <pre><code>mpicc -O3 name.c -o my_program\n</code></pre> or <pre><code>mpif90 -O3 name.f90 -o my_program\n</code></pre> This will create an executable file named <code>my_program</code>. Prepare a GNU Makefile to build programs with multiple source files. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#mpi-jobs","title":"MPI jobs","text":"<p>MPI programs are suitable to run on multiple CPU cores of a single node or on multiple nodes. </p> <p>Here is an example script (e.g. named <code>job.sh</code>) to run an MPI job using multiple cores on a single node.  <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 8\n#SBATCH --mem=10GB   \n\nsrun hostname\nmodule load gcc/6.2.0 openmpi/3.0.4\nmpirun -n $SLURM_NTASKS my_program\n</code></pre></p> <p>This job requests 8 cores (with <code>-n</code>) and 10 GB of memory (with <code>--mem</code>) on 1 node (with <code>-N</code>) for 30 minutes (with <code>-t 30</code>). The <code>-n</code> flag is the same as <code>--ntasks</code>. The specified value is saved to the variable <code>SLURM_NTASKS</code>. In this case, the requested number of cores is equal to <code>SLURM_NTASKS</code>. The command <code>mpirun -n $SLURM_NTASKS</code> ensures that each MPI task runs on one core. </p> <p>The command <code>srun hostname</code> is to check if the correct number of cores and nodes are assigned to the job. It is not needed in production runs. </p> Side note: partitions and modules <p>The modules used in the above example is for the CentOS 7 OS, which works for these partitions: <code>sched_mit_hill</code>, <code>newnodes</code>, and <code>sched_any</code>. If using a partition with the Rocky 8 OS, such as <code>sched_mit_orcd</code>, change the modules accrodingly (see the first session). These are public partitions that are avaiable to most users. Many labs have partitions for their purchased nodes. </p> <p>Submit the job with the <code>sbatch</code> command, <pre><code>sbatch job.sh\n</code></pre></p> <p>To run an MPI job on multiple nodes, refer to the following exmaple script. <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=8\n#SBATCH --mem=10GB\n\nsrun hostname\nmodule load gcc/6.2.0 openmpi/3.0.4\nmpirun -n $SLURM_NTASKS my_program\n</code></pre></p> <p>This job requests 2 nodes with 8 cores and 10 GB of memory per node. In this case, the total number of cores (saved to <code>SLURM_NTASKS</code>) is equal to the number of nodes (saved to <code>SLURM_NNODES</code>) times the number of cores per node (saved to <code>SLURM_NTASKS_PER_NODE</code>). The command <code>mpirun -n $SLURM_NTASKS</code> ensures that MPI tasks are distributed to both nodes and each MPI task runs on one core. </p> <p>Alternatively, users can specify the number of cores per node using an OpenMPI option like this <code>mpirun -npernode $SLURM_NTASKS_PER_NODE my_program</code>.</p> <p>If replacing <code>--ntasks-per-node=8</code> with <code>-n 16</code> in the above script, the job will request 16 cores on 2 nodes, but it is not always the case that there are 8 cores per node. For example, there may be 7 cores on one node and 9 cores on another, or 1 core on one node and 15 cores on another, etc, depending on the current available resources on the cluster. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#computing-resources-for-mpi-jobs","title":"Computing resources for MPI jobs","text":"<p>To get a better idea on how many nodes, cores and memory should be requested, users need to consider the following two questions. </p> <p>First, what resources are available on the cluster? Use this command to check node and job info on the cluster, including the constraint associated with OS (<code>%f</code>), the nubmer of CPU cores (<code>%c</code>), the memory size (<code>%m</code>), the wall time limit (<code>%l</code>), and the current usage status (<code>%t</code>).  <pre><code> sinfo -N -p sched_mit_hill,newnodes,sched_any,sched_mit_orcd -o %f,%c,%m,%l,%t |grep -v drain\n</code></pre> On typical nodes of the cluster, the number of cores per node varies from 16 to 128, and the memory per node varies from 63 GB to 515 GB.</p> <p>To obtain a good performance of MPI programs, it is recommended to request all physical CPU cores and memory on each node. For example, request two nodes with 16 physical cores per node and all of the memory like this, <pre><code>#SBATCH -N 2\n#SBATCH --ntasks-per-node=16\n#SBATCH --mem=0\n</code></pre></p> <p>As MPI is a distributed-memory parallelism, sometimes it is good to use the <code>--mem-per-core</code> flag assigning a certain amount of memory to each core. The total memory is increased with the number of cores in this case. Double check that the total memory fits in the maximum memory of a node to avoid failed jobs.</p> <p>Second, what is the speedup of your MPI program? According to Amdahl's law, well-performing MPI programs are usually speeded up almost linearly as the number of cores is increased until it is saturated at some point. In practice, try to run testing cases investigating the speedup of your program, and then decide how many cores are needed to speed it up efficiently. Do not increase the number of cores when speedup is poor. </p> <p>Note</p> <p>After a job starts to run, execute the command <code>squeue -u $USER</code> to check which node the job is running on, and then log in to the node with <code>ssh &lt;hostname&gt;</code> and execute the <code>top</code> command to check how many CPU cores are being used by the program and what the CPU efficiency is. The efficiency may vary with the number of CPU cores. Try to keep your jobs at a high efficiency. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#hybrid-mpi-and-multithreading-jobs","title":"Hybrid MPI and multithreading jobs","text":"<p>MPI programs are based on distributed-memory parallelism, that says, each MPI task owns a faction of data, such as arrays, matrices, or tensors. In contrast to MPI, the multithreading technique is based on a shared-memory parallelism, in which data is shared by multiple threads. A common implementation of the multithreading technique is OpenMP. For Python users, the <code>numpy</code> package is based on C libraries, such as Openblas, which are usually built with OpenMP. </p> Side note: OpenMP <p>OpenMP is an abbreviation of Open Multi-Processing. It is not related to OpenMPI.</p> <p>Some programs are designed in a hybrid scheme such that MPI and OpenMP are combined to enable two-level parallelization. Set MPI tasks and OpenMP threads in hybrid programs based on the following equation, <pre><code>(Number of MPI Tasks) * (Number of Threads) = Total Number of Cores          (1)\n</code></pre></p> Side note: hyperthreads <p>There are two or multiple hyperthreads on each CPU core in modern CPU architecture. The hyperthread technique is turned off for most nodes of this cluster, but it may be turned on for some nodes as requested by the owner labs. In the case that there are two hyerthreads per physical core, the right side of the equation should be <code>2 * (Total Number of Cores)</code> instead.</p> <p>One way to run hybrid progmrams in Slurm jobs is to use the <code>-n</code> flag for the number of MPI tasks and the <code>-c</code> flag for the number of threads. The follwing is a job script that runs a program with 2 MPI tasks and 8 threads per task on a node with 16 cores. <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 2\n#SBATCH -c 8\n#SBATCH --mem=10GB\n\nmodule load gcc/6.2.0 openmpi/3.0.4\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nmpirun -n $SLURM_NTASKS my_program\n</code></pre> The <code>-c</code> flag is the same as <code>--cpus-per-task</code>. The specified value is saved in the variable <code>SLURM_CPUS_PER_TASK</code>. In this case, the total number of cores equals <code>SLURM_NTASKS * SLURM_CPUS_PER_TASK</code>, that is 16. </p> <p>The built-in environment variable <code>OMP_NUM_THREADS</code> is used to set the number of threads for an OpenMP program. Here it is equal to <code>SLURM_CPUS_PER_TASK</code>. The number of MPI tasks is set to be <code>SLURM_NTASK</code> in the <code>mpirun</code> command line, therefore, the nubmer of MPI tasks times the number of threads equals the total number of CPU cores. </p> <p>Users only need to specify the numbers following Slurm flags <code>-n</code> and <code>-c</code>, for example, <code>-n 4 -c 4</code> or <code>-n 8 -c 2</code>, keeping the product unchanged, then the MPI tasks and threads are all set automatically.  </p> <p>Similarly, here is an exmple script to request two nodes,  <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=2\n#SBATCH -c 8\n#SBATCH --mem=0\n\nmodule load gcc/6.2.0 openmpi/3.0.4\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nmpirun -n $SLURM_NTASKS my_program\n</code></pre> In this case, the total number of cores is equal to <code>SLURM_NNODES * SLURM_NTASKS_PER_NODE * SLURM_CPUS_PER_TASK</code>, that is <code>2 * 2 * 8 = 32</code>. The job will run 4 MPI tasks (i.e. 2 tasks per node) and 8 threads per task. Equation (1) is satisfied as <code>4 * 8 = 32</code>. </p> <p>As mentioned in the previous section, it is recommended to run testing cases to determine the values for the flags <code>-N</code>, <code>-n</code> and <code>-c</code> to obtain a better performance.</p> <p>There is another way to submit jobs for hybrid programs, in which the <code>-c</code> flag is not used at all. For example, it also works like this, <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 16\n#SBATCH --mem=10GB\n\nmodule load gcc/6.2.0 openmpi/3.0.4\nexport OMP_NUM_THREADS=8\nMPI_NTASKS=$((SLURM_NTASK / $OMP_NUM_THREADS))\nmpirun -n $MPI_NTASKS my_program\n</code></pre> This job requests 16 CPU cores on 1 node and runs 2 MPI tasks with 8 threads per task, so equation (1) is satisfied as <code>2 * 8 = 16</code>. In this case, users need to set the values for the Slurm flag <code>-n</code> and the variable <code>OMP_NUM_THREADS</code>.</p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi4py/","title":"MPI for Python","text":"<p>MPI for Python (<code>mpi4py</code>) provides Python bindings for the Message Passing Interface (MPI) standard, allowing Python applications to exploit multiple processors on workstations, clusters and supercomputers.</p> <p>You can learn about <code>mpi4py</code> here: https://mpi4py.readthedocs.io/en/stable/.</p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#installation","title":"Installation","text":"OpenMindEngaging <p>The support team has installed <code>mpi4py</code> in an Anaconda module. You can load the module and do not need to install anything:</p> <pre><code>module load openmind/anaconda/3-2022.05\n</code></pre> <p>If you want to use Anaconda in your directory, refer to section 3 on this page to set it up, then install <code>mpi4py</code> and <code>numpy</code>:</p> <pre><code>conda install -c conda-forge mpi4py numpy\n</code></pre> <p>If you are using a Rocky 8 node (such as orcd-login001 or orcd-login002), then <code>mpi4py</code> is already installed in the <code>miniforge/24.3.0-0</code> module.</p> <p>If you are using CentOS 7 (such as orcd-vlogin001 or orcd-vlogin002), then you need to create a new Conda environment using the <code>miniforge</code> module:</p> <pre><code>module load miniforge\n</code></pre> <p>And install <code>mpi4py</code>:</p> <pre><code>conda create -n mpi mpi4py\n</code></pre>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#example-codes","title":"Example codes","text":"<p>Prepare your Python codes. </p> <p>Example 1: The following code is for sending and receiving a dictionary. Save it in a file named <code>p2p-send-recv.py</code>:</p> p2p-send-recv.py<pre><code>from mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\nif rank == 0:\n    data = {'a': 7, 'b': 3.14}\n    comm.send(data, dest=1, tag=11)\n    print(rank,data)\nelif rank == 1:\n    data = comm.recv(source=0, tag=11)\n    print(rank,data)\n</code></pre> <p>Example 2: The following code is for sending and receiving an array. Save it in a file named <code>p2p-array.py</code>:</p> p2p-array.py<pre><code>from mpi4py import MPI\nimport numpy\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\n# passing MPI datatypes explicitly\nif rank == 0:\n    data = numpy.arange(1000, dtype='i')\n    comm.Send([data, MPI.INT], dest=1, tag=77)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(1000, dtype='i')\n    comm.Recv([data, MPI.INT], source=0, tag=77)\n    print(rank,data)\n\n# automatic MPI datatype discovery\nif rank == 0:\n    data = numpy.arange(100, dtype=numpy.float64)\n    comm.Send(data, dest=1, tag=13)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(100, dtype=numpy.float64)\n    comm.Recv(data, source=0, tag=13)\n    print(rank,data)\n</code></pre>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#submitting-jobs","title":"Submitting jobs","text":"OpenMindEngaging <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>:</p> p2p-job.sh<pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n\nmodule load openmind/anaconda/3-2022.05\n\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre> <p>Note</p> <p>If you use Anaconda in your directory, do not load the Anaconda module. </p> <p>Finally submit the job:</p> <pre><code>sbatch p2p-job.sh\n</code></pre> <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>:</p> p2p-job.sh<pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n#SBATCH -p mit_normal\n\nmodule load miniforge\nmodule load openmpi/4.1.4\n\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\n\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre> <p>Finally, submit the job:</p> <pre><code>sbatch p2p-job.sh\n</code></pre>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mujoco/","title":"Mujoco","text":"<p>Warning</p> <p>This page has been archived. The information present is not updated and may no longer be accurate.</p>","tags":["Engaging","SuperCloud","Physics","Howto Recipes","Install Recipe"]},{"location":"recipes/mujoco/#installing-and-using-mujoco","title":"Installing and Using MuJoCo","text":"<p>MuJoCo is a free and open source physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed.</p> <p>You can learn about MuJoCo here: https://mujoco.org.</p> <p>Whether you are installing on Engaging or SuperCloud, you\u2019ll first have to install the MuJoCo binaries. This process is the same on all systems.</p>","tags":["Engaging","SuperCloud","Physics","Howto Recipes","Install Recipe"]},{"location":"recipes/mujoco/#install-the-mujoco-binaries","title":"Install the MuJoCo Binaries","text":"<p>First, install MuJoCo itself somewhere in your home directory. This is as simple as downloading the MuJoCo binaries, which can be found on their web page. For the release that you want, select the file that ends with \u201clinux-x86_64.tar.gz\u201d, for example for 2.3.0 select mujoco-2.3.0-linux-x86_64.tar.gz. Right click, and copy the link address. Then you can download on one of the login nodes with the \u201cwget\u201d command, and untar:</p> <pre><code>wget https://github.com/deepmind/mujoco/releases/download/2.3.0/mujoco-2.3.0-linux-x86_64.tar.gz\ntar -xzf mujoco-2.3.0-linux-x86_64.tar.gz\n</code></pre> <p>In order for mujoco-py to find the MuJoCo binaries, set the following paths:</p> <pre><code>export MUJOCO_PY_MUJOCO_PATH=$HOME/path/to/mujoco-2.3.0/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MUJOCO_PY_MUJOCO_PATH/bin\n</code></pre>","tags":["Engaging","SuperCloud","Physics","Howto Recipes","Install Recipe"]},{"location":"recipes/mujoco/#install-mujoco-py","title":"Install Mujoco-Py","text":"<p>First, make sure the <code>MUJOCO_PY_MUJOCO_PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables are set pointing to your mujoco installation. You can use the \u201cecho\u201d command to do this:</p> <pre><code>echo MUJOCO_PY_MUJOCO_PATH\necho LD_LIBRARY_PATH\n</code></pre> <p>If any of these are not set properly you can set them as described above (see here for MUJOCO_PY_MUJOCO_PATH and LD_LIBRARY_PATH).</p> EngagingSuperCloud <p>Next load either a Python or Anaconda module. In this example I loaded the <code>miniforge</code> module (run <code>module avail miniforge</code> to see the current list of available Anaconda modules):</p> <pre><code>module load miniforge\n</code></pre> <p>From here on you can follow the standard instructions to install mujoco-py, using the <code>--user</code> flag where appropriate to install in your home directory, or install in an anaconda or virtual environment (do not use the <code>--user</code> flag if you want to install in a conda or virtual environment). Here I am installing in my home directory with <code>--user</code>:</p> <pre><code>pip install --user 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Start up python and import <code>mujoco_py</code> to complete the build process:</p> <pre><code>import mujoco_py\n</code></pre> <p>MuJoCo, particularly mujoco-py, can be tricky to install on SuperCloud as it uses file locking during the install and whenever the package is loaded. File locking is disabled on the SuperCloud shared filesystem performance reasons, but is available on the local disk of each node. Therefore, one workaround is to install mujoco-py on the local disk of one of the login nodes and then copy the install to your home directory. To load the package, the install then needs to be copied to the local disk.</p> <p>We\u2019ve found the most success by doing this with a python virtual environment. By using a python virtual environment you can install any additional packages you need with mujoco-py, and they can be used along with packages in our anaconda module, unlike conda environments.</p> <p>Create the virtual environment on the local disk of the login node and install mujoco-py (install the version you would like to use):</p> <pre><code>module load anaconda/2023a\nmkdir /state/partition1/user/$USER\npython -m venv /state/partition1/user/$USER/mujoco_env\nsource /state/partition1/user/$USER/mujoco_env/bin/activate\npip install 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Now install any other packages you need to run your MuJoCo jobs. With virtual environments you won\u2019t see any of the packages you\u2019ve previously installed with <code>pip install --user</code> or what you may have installed in another environment. You should still be able to use any of the packages in the anaconda module you\u2019ve loaded, so no need to install any of those.</p> <pre><code>pip install pkgname1\npip install pkgname2\n</code></pre> <p>Since you are installing into a virtual environment, do not use the <code>--user</code> flag.</p> <p>Once you\u2019ve installed the packages you need, start Python and import <code>mujoco_py</code> to finish the build:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>Now that your environment is created, copy it to your home directory for permanent storage.</p> <pre><code>cp -r /state/partition1/user/$USER/mujoco_env $/software/mujoco/\n</code></pre> <p>If you\u2019d like you can run the few example lines listed on install section of the mujoco-py github page to verify the install went through properly:</p> <pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre>","tags":["Engaging","SuperCloud","Physics","Howto Recipes","Install Recipe"]},{"location":"recipes/mujoco/#using-mujoco-in-a-job","title":"Using MuJoCo in a Job","text":"<p>To use MuJoCo you\u2019ll need to first load the same Python or Anaconda module you used to install mujoco-py. If you installed it into a conda environment or python virtual environment, load that environment as well. We recommend you do this in your job submission script rather than in your <code>.bashrc</code> or at the command line before you submit the job. This way you know your job is configured properly every time it runs.</p> <p>You can use the following test scripts to test your MuJoCo setup in a job environment, and as a starting point for your own job:</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> EngagingSuperCloud submit_test.sh<pre><code>#!/bin/bash\n\n# Load the same python/anaconda module you used to install mujoco-py\nmodule load python/3.8.3\n\n# Run the script\npython mujoco_test.py\n</code></pre> <p>Now whenever you use mujoco-py the installation will need to be on the local disk of the node(s) where you are running. In your job script you can add a few lines of code that will check whether the environment exists on the local disk, and if not copy it. You can run these lines during an interactive job as well.</p> submit_test.sh<pre><code>#!/bin/bash``\n\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\nif [ ! -d \"$MUJOCO_ENV\" ]; then\n    echo \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\n    mkdir -p /state/partition1/user/$USER\n    cp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\n\nmodule load anaconda/2022a\nsource $MUJOCO_ENV/bin/activate\n\npython mujoco_test.py\n</code></pre>","tags":["Engaging","SuperCloud","Physics","Howto Recipes","Install Recipe"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/","title":"Example of a Minimal Program Using the NVHPC Stack with CUDA-aware MPI","text":"","tags":["Howto Recipes","NVHPC","MPI","cuda","cuda aware mpi","GPU"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#about-nvhpc","title":"About NVHPC","text":"<p>NVHPC is an integrated collection of software tools and libraries distributed by Nvidia. An overview document describing NVHPC can be found here. The aim of the NVHPC team is to provide an up-to-date and preconfigured suites of compilers, libraries, and tools that are specifically optimized for Nvidia GPU hardware. It supports both single and multi-GPU execution.</p>","tags":["Howto Recipes","NVHPC","MPI","cuda","cuda aware mpi","GPU"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#basic-usage-example","title":"Basic Usage Example","text":"<p>This example shows steps for using NVHPC to run a simple test MPI program, written in C, that communicates between two GPUs across two MPI tasks. MPI requires that the communication between GPUs goes through CPUs. For the much faster communication directly between GPUs, you can use NCCL. The detailed steps that can be executed in an interactive Slurm session are explained below. A complete Slurm job example is shown at the end.</p>","tags":["Howto Recipes","NVHPC","MPI","cuda","cuda aware mpi","GPU"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#1-activate-the-relevant-nvhpc-module","title":"1. Activate the Relevant NVHPC Module","text":"<p>The NVHPC environment is installed as a module and can be made visible in a session using the command</p> <pre><code>module load nvhpc/24.5\n</code></pre> <p>This will add a specific version of the NVHPC software (version 24.5 on Engaging was released in 2024) to a shell or batch script. The software added includes compilers for C, C++ and Fortran; base GPU optimized numerical libraries for linear algebra, Fourier transforms and others; and GPU optimized communication libraries supporting MPI, SHMEM and NCCL APIs.</p> <p>An environment variable, <code>NVHPC_ROOT</code>, is also set. This can be used in scripts to reference the locations of libraries when needed.</p>","tags":["Howto Recipes","NVHPC","MPI","cuda","cuda aware mpi","GPU"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#2-set-paths-needed-for-compile-step","title":"2. Set paths needed for compile step","text":"<p>Here we use the module environment variable, <code>NVHPC_ROOT</code>, to set environment variables that have paths needed for compilation and linking of code.</p> <pre><code>culibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n</code></pre>","tags":["Howto Recipes","NVHPC","MPI","cuda","cuda aware mpi","GPU"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#3-create-a-c-program-that-executes-some-simple-multi-task-multi-gpu-test-code","title":"3. Create a C program that executes some simple multi-task, multi-GPU test code","text":"<p>The next step is to create a file holding C code that uses MPI to send information between two GPUs running in different processes. Paste the C code below into a file called <code>test.c</code>.</p> test.c<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;cuda_runtime.h&gt;\nint main(int argc, char *argv[])\n{\n  int myrank, mpi_nranks; \n  int LBUF=1000000;\n  float *sBuf_h, *rBuf_h;\n  float *sBuf_d, *rBuf_d;\n  int bSize;\n  int i;\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);                  // my MPI rank\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;mpi_nranks);\n\n  if ( mpi_nranks != 2 ) { printf(\"Program requires exactly 2 ranks\\n\");exit(-1); }\n\n  int deviceCount;\n  cudaGetDeviceCount(&amp;deviceCount);               // How many GPUs?\n  printf(\"Number of GPUs found = %d\\n\",deviceCount);\n  int device_id = myrank % deviceCount;\n  cudaSetDevice(device_id);                       // Map MPI-process to a GPU\n  printf(\"Assigned GPU %d to MPI rank %d of %d.\\n\",device_id, myrank, mpi_nranks);\n\n  // Allocate buffers on each host and device\n  bSize = sizeof(float)*LBUF;\n  sBuf_h = malloc(bSize);\n  rBuf_h = malloc(bSize);\n  for (i=0;i&lt;LBUF;++i){\n    sBuf_h[i]=(float)myrank;\n    rBuf_h[i]=-1.;\n  }\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  cudaMalloc((void **)&amp;sBuf_d,bSize);\n  cudaMalloc((void **)&amp;rBuf_d,bSize);\n\n  cudaMemcpy(sBuf_d,sBuf_h,bSize,cudaMemcpyHostToDevice);\n\n  if ( myrank == 0 ) {\n   MPI_Recv(rBuf_d,LBUF,MPI_REAL,1,0,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n  } \n  else if ( myrank == 1 ) {\n   MPI_Send(sBuf_d,LBUF,MPI_REAL,0,0,MPI_COMM_WORLD);\n  }\n  else\n  {\n   printf(\"Unexpected myrank value %d\\n\",myrank);\n   exit(-1);\n  }\n\n  cudaMemcpy(rBuf_h,rBuf_d,bSize,cudaMemcpyDeviceToHost);\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}\n</code></pre>","tags":["Howto Recipes","NVHPC","MPI","cuda","cuda aware mpi","GPU"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#4-compile-program","title":"4. Compile program","text":"<p>Here we use the NVHPC MPI wrapper to compile. The two environment variables we set earlier (<code>cuincdir</code> and <code>culibdir</code>) are used to let the compile step know where to find the relevant CUDA header and library files. The CUDA runtime library (<code>cudart</code>) is added as a location for finding CUDA functions the code utilizes.</p> <pre><code>mpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n</code></pre>","tags":["Howto Recipes","NVHPC","MPI","cuda","cuda aware mpi","GPU"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#5-execute-program","title":"5. Execute program","text":"<p>Once the code has been compiled, the <code>mpiexec</code> command that is part of the <code>nvhpc</code> module can be used to run the test program. The <code>nvhpc</code> module defaults to using its builtin version of OpenMPI. The OpenMPI option <code>btl_openib_warn_no_device_params_found</code> is passed into the OpenMPI runtime library. This option suppresses a warning that OpenMPI can generate when it encounters a network device card that is not present in a built-in list that OpenMPI has historically included.</p> <pre><code>salloc -n 2 --gres=gpu:2\nmpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre> <p>Note the <code>salloc</code> command is only needed to run interactively from the login node. If you are running in a batch job or are already in an interactive job on a compute node you will not need to first run <code>salloc</code>.</p> <p>Running this program using the command above should produce the following output, although some of the lines may be in different orders due to the asynchronous nature of the tasks.</p> <pre><code>Number of GPUs found = 1\nNumber of GPUs found = 1\nAssigned GPU 0 to MPI rank 0 of 2.\nrBuf_h[0] = -1.000000\nAssigned GPU 0 to MPI rank 1 of 2.\nrBuf_h[0] = 1.000000\n</code></pre>","tags":["Howto Recipes","NVHPC","MPI","cuda","cuda aware mpi","GPU"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#example-of-slurm-job-file-for-executing-this-example","title":"Example of Slurm job file for executing this example","text":"<p>First create a file called \"test.c\" containing the example C program above. The job script file below will run all the steps described above for \"test.c\". It can be submitted to Slurm using the command <code>sbatch</code> followed by the filename holding the job script. The output will be written to a file named myjob.log-jobID where jobID is the ID of the specific job you ran.</p> test_cuda_and_mpi.sbatch<pre><code>#!/bin/bash                                                                                          \n#SBATCH -n 2                                                                                         \n#SBATCH --gres=gpu:2                                                                                 \n#SBATCH -o myjob.log-%A                                                                              \n#                                                                                                    \n# Basic slurm job that tests GPU aware MPI in the NVHPC tool stack.                                  \n#                                                                                                    \n#                                                                                                    \n#   To submit through Slurm use:                                                                     \n#                                                                                                    \n#   $ sbatch test_cuda_and_mpi.sbatch                                                                \n#                                                                                                    \n#   in terminal.                                                                                     \n\n# Write a little log info                                                                            \necho \"## Start time \\\"\"`date`\"\\\"\"\necho \"## Slurm job running on nodes \\\"${SLURM_JOB_NODELIST}\\\"\"\necho \"## Slurm submit directory \\\"${SLURM_SUBMIT_DIR}\\\"\"\necho \"## Slurm submit host \\\"${SLURM_SUBMIT_HOST}\\\"\"\necho \" \"\n\n\nmodule load nvhpc/24.5\nculibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n\nmpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n\nmpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out\n</code></pre>","tags":["Howto Recipes","NVHPC","MPI","cuda","cuda aware mpi","GPU"]},{"location":"recipes/orca/","title":"Installing ORCA for Personal Use","text":"<p>ORCA is a quantum chemistry software package designed for computational chemistry, featuring a wide range of methods including electronic structure theory.</p> <p>ORCA is a licensed software that is free for academic use, but it cannot be transferred to third parties (per the ORCA EULA). So, we cannot install newer versions system-wide. Users must create an account and install it personally. Here are the steps to do so:</p> <ul> <li>Navigate to the ORCA website</li> <li>Register for an account and login</li> <li>Click \"Downloads\" in the top bar</li> </ul> <p></p> <ul> <li>Select the version of ORCA you want</li> </ul> <p></p> <ul> <li>Select the \"Linux, x86-64, .tar.xz Archive\u201d version</li> </ul> <p></p> <ul> <li>Click \"Download\"</li> </ul> <p></p> <ul> <li>Upload the <code>.tar.xz</code> file to Engaging using <code>scp</code>:</li> </ul> <pre><code>scp /path/to/source/orca_6_0_1_linux_x86-64_shared_openmpi416.tar.xz $USER@orcd-login001.mit.edu:/path/to/destination\n</code></pre> <ul> <li>On Engaging, extract the <code>tar.xz</code> file:</li> </ul> <pre><code>tar -xf orca_6_0_1_linux_x86-64_shared_openmpi416.tar.xz\n</code></pre> <ul> <li>Add this version of ORCA to your path:</li> </ul> <pre><code>export PATH=/path/to/orca_6_0_1_linux_x86-64_shared_openmpi416:$PATH\n</code></pre>","tags":["ORCA","Engaging","Install Recipe"]},{"location":"recipes/orca/#running-a-test-case","title":"Running a Test Case","text":"<p>To see if our installation was successful, we can run a test case adapted from the ORCA 6.0 Tutorials.</p> <p>First, create an empty directory:</p> <pre><code>mkdir ~/orca_test\ncd ~/orca_test\n</code></pre> <p>Next, create a test file:</p> water.inp<pre><code>!HF DEF2-SVP\n* xyz 0 1\nO   0.0000   0.0000   0.0626\nH  -0.7920   0.0000  -0.4973\nH   0.7920   0.0000  -0.4973\n*\n</code></pre> <p>Run <code>orca</code> on this file and save the output to another file:</p> <pre><code>orca water.inp &gt; water.out\n</code></pre>","tags":["ORCA","Engaging","Install Recipe"]},{"location":"recipes/orca/#running-orca-with-multiple-processes","title":"Running ORCA with Multiple Processes","text":"<p>To truly take advantage of the resources available to you on a high performance computing cluster, you can run ORCA in parallel. The version of ORCA we've downloaded uses MPI to handle parallel computation. Since we already have MPI installed on the cluster as a module, using it is pretty straightforward.</p> <p>First, you will need to request adequate resources. Make sure the resources you request match what you specify in your ORCA input file:</p> <pre><code>salloc -N 1 -n 4 -p mit_normal\n</code></pre> <p>Note</p> <p>While this example is using an interactive job, we recommend using a batch job for longer-running programs. See here for more information on running jobs.</p> <p>When using ORCA with MPI, ORCA recommends that you add MPI to your path and also add the paths to the ORCA and MPI libraries to your <code>LD_LIBRARY_PATH</code> environment variable:</p> <pre><code>module load openmpi # Adds openmpi to $PATH\nexport LD_LIBRARY_PATH=/path/to/orca_6_0_1_linux_x86-64_shared_openmpi416/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/orcd/software/core/001/spack/pkg/openmpi/4.1.4/zahpnmk/lib:$LD_LIBRARY_PATH\n</code></pre> <p>Note</p> <p>This example is for Engaging Rocky 8 nodes. For Engaging CentOS 7 nodes or other clusters, you'll need to change the path to your OpenMPI library.</p> <p>We will also need to edit our input file to specify the number of processes:</p> water.inp<pre><code>!HF DEF2-SVP PAL4 # For 4 processes\n* xyz 0 1\nO   0.0000   0.0000   0.0626\nH  -0.7920   0.0000  -0.4973\nH   0.7920   0.0000  -0.4973\n*\n</code></pre> <p>When we run ORCA with multiple processes, we need to use the full path to the ORCA binary:</p> <pre><code>/path/to/orca_6_0_1_linux_x86-64_shared_openmpi416/orca water.inp &gt; water.out\n</code></pre>","tags":["ORCA","Engaging","Install Recipe"]},{"location":"recipes/pycharm/","title":"Using PyCharm on an ORCD System","text":"<p>PyCharm provides an integrated development environment for users to edit their Python code and has support for remote development via SSH. While we generally recommend using VS Code due to its much broader set of features while being free and open-source, some prefer the simplicity of PyCharm. Furthermore, PyCharm has a free license for students and teachers.</p> <p>To use PyCharm on the cluster, the setup is similar to VS Code. However, PyCharm uses a lot more memory and compute power to run, so it is essential that you run it on a compute node.</p> <p>Note</p> <p>To use PyCharm on a compute node, an SSH key is necessary. If you haven't set up SSH keys yet, refer to the SSH Key Setup guide.</p> <p>Note</p> <p>PyCharm is not available on SuperCloud because SuperCloud does not support file locking.</p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/pycharm/#download-pycharm","title":"Download PyCharm","text":"<p>Follow this link to download and install PyCharm on your local computer. Make sure you select the version that matches the architecture of your machine.</p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/pycharm/#requesting-a-compute-node","title":"Requesting a Compute Node","text":"<p>To run PyCharm on a compute node, you first need to request an interactive job with at least 4 cores on the cluster. PyCharm recommends using 4 cores so that the application can run more quickly. Request more resources as required by your code.</p> <pre><code>salloc -N 1 -n 4 --mem-per-cpu=4G -p mit_normal\n</code></pre> <p>Note</p> <p>PyCharm is not supported on Centos7 nodes.</p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/pycharm/#editing-your-ssh-config-file","title":"Editing your SSH Config File","text":"<p>Once you are in the interactive session, make a note of the node you are running on. We now want to edit our local SSH <code>config</code> file so that PyCharm can run on that node. To do this, open the command line and locate your <code>config</code> file. It is usually located in <code>~/.ssh/config</code>. Using your favorite editor, paste the following (enter your username and the correct node number):</p> config<pre><code>Host engaging-compute\n  User USERNAME\n  HostName nodeXXXX\n  ProxyJump orcd-login001.mit.edu\n</code></pre> <p>Note</p> <p>If you don't want to edit your <code>config</code> file every time you start up a PyCharm session, you can request a specific node each time you start an interactive session with the flag <code>--nodelist=nodeXXXX</code>. Just make sure that the node in your config file reflects the node that you're requesting. However, the node you're requesting may be unavailable, in which case you'll have to choose a different node and edit your <code>config</code> file anyway.</p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/pycharm/#starting-pycharm","title":"Starting PyCharm","text":"<p>PyCharm can be finnicky with Duo authentication. To get around this, connect to the MIT VPN so that Duo is not required.</p> <p>Open PyCharm and click Remote Development &gt; SSH on the left-hand side:</p> <p></p> <p>Create a new project and connect to SSH. Enter your username and host name (in this case it's <code>engaging-compute</code>), then click \"Check Connection and Continue\":</p> <p></p> <p>This will open a new page where you will enter your project directory. Enter the path to the directory on Engaging that you'd like to work in. You are likely to get the most success if you point PyCharm to a blank directory (I've named mine <code>pycharm</code> for now). Click \"Download IDE and Connect\":</p> <p></p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/pycharm/#troubleshooting","title":"Troubleshooting","text":"<p>If you are still running into issues, try deleting the JetBrains cache in your home directory on the cluster via the command line:</p> <pre><code>rm -r ~/.cache/JetBrains\n</code></pre>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/rag/","title":"Running Your Own Retrieval-Augmented Generation (RAG) Model","text":"<p>RAG models harness the power of Large Language Models (LLMs) to query and summarize a set of documents. Through RAG, one can combine the strengths of retrieval-based and generative models to provide more accurate and contextually relevant responses.</p> <p>RAG also provides an interesting test case to make use of our resources on the cluster. Here, we provide instructions on how to run a RAG model to query and answer questions about our ORCD documentation.</p> <p>The code for developing this model can be found in this GitHub repository. Feel free to use this repository as a guide to develop your own RAG model on separate documents.</p>","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/rag/#getting-started","title":"Getting Started","text":"","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/rag/#working-on-a-compute-node","title":"Working on a Compute Node","text":"<p>You will need to request an interactive session with a GPU:</p> <pre><code>salloc -N 1 -n 8 --mem-per-cpu=4G -p mit_normal_gpu -G l40s:1\n</code></pre> <p>I have specified an L40S GPU, which has 48GB of memory. You will need a GPU with at least 40GB of memory to use the 8B model.</p>","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/rag/#getting-access-to-huggingface","title":"Getting Access to HuggingFace","text":"<p>The LLMs used in this pipeline are from HuggingFace. By default, we use a model from Mistral, which is gated and requires users to request access. You can follow this process for doing so:</p> <ol> <li>Create a HuggingFace account</li> <li>Request access to mistralai/Ministral-8B-Instruct-2410</li> <li> <p>Create a user access token</p> <p>You will need to adjust the settings of your user access token so that you can download and run the model. To do so, navigate to your HuggingFace profile, then click \"Edit Profile\" &gt; \"Access Tokens\" and edit the permissions for your access token:</p> <p></p> <p>Edit your token permissions to match the following:</p> <p></p> </li> <li> <p>Export your access token as an environment variable on Engaging and add to your <code>.bash_profile</code> so it can be saved for future uses:</p> <pre><code>export HF_TOKEN=\"your_user_access_token\"\necho 'export HF_TOKEN=\"your_user_access_token\"' &gt;&gt; ~/.bash_profile\n</code></pre> <p>Note</p> <p>You will not be able to copy your HF token again from the HF website. If you do not save it somewhere, you will need to generate a new one every time you run this.</p> </li> </ol>","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/rag/#running-the-model","title":"Running the Model","text":"","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/rag/#running-in-a-container","title":"Running in a Container","text":"<p>You can run the RAG model on our documentation using the Apptainer image we have saved to Engaging. We have the commands for doing so saved in a shell script. To run the container, you can simply run the following:</p> <pre><code>sh /orcd/software/community/001/pkg/orcd-rag/container/run_rag_with_container.sh\n</code></pre> <p>The first time you run this, the model will be downloaded from HuggingFace and cached, so it may take a while to get running. Subsequent times will be much quicker because the model has already been downloaded.</p> <p>The 8B model takes about 15GB of space. The default cache location for HuggingFace models is <code>$HOME/.cache/huggingface</code>. If you do not have enough space in your home directory to store the model, you can set the <code>HF_HOME</code> environment variable to point to another directory. For example, to save models to your scratch directory (depending on your storage setup), that would look something like this:</p> <pre><code>export HF_HOME=/home/$USER/orcd/scratch\n</code></pre> <p>or:</p> <pre><code>export HF_HOME=/nobackup1/$USER\n</code></pre>","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/rag/#running-via-a-python-environment","title":"Running via a Python Environment","text":"<p>You can avoid the container route and run this using a Python environment. This method is recommended if you would like to make any advanced changes to the pipeline. The steps to do so can be found on this GitHub repository.</p>","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/rag/#customization","title":"Customization","text":"<p>To customize this pipeline to fit your needs, aside from editing the code base itself, you can use the provided optional flags when you call the script. To see what customizations are available, use the <code>--help</code> flag:</p> <pre><code>sh /orcd/software/community/001/pkg/orcd-rag/container/run_rag_with_container.sh --help\n</code></pre> <p>This will allow you to adjust model temperature, change the path to use a different set of documents, use a different LLM (doing this successfully will likely take some editing of the code base), or pass a set of queries to run in a batch setting.</p>","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/rag/#using-your-own-documents","title":"Using Your Own Documents","text":"<p>Be default, the pipeline is set up to run RAG on the ORCD documentation. However, this is designed to be easily adaptable to any set of documents you choose.</p> <p>If you have created a vector store of documents, you can specify the path to those documents when you run the pipeline using the <code>--vector_store_path</code> flag:</p> <pre><code>sh /orcd/software/community/001/pkg/orcd-rag/container/run_rag_with_container.sh --vector_store_path /path/to/vector/store\n</code></pre>","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/rag/#creating-a-vector-store","title":"Creating a Vector Store","text":"<p>The RAG pipeline requires that documents be stored in a vector database (\"vector store\"), so that relevant information can be queried efficiently. We have included code for creating a vector store based on <code>.md</code> or <code>.pdf</code> files. First, you will need to consolidate your documents into a single directory and upload them to Engaging. Then, run this command, specifying the path to your directory of documents:</p> <pre><code>sh /orcd/software/community/001/pkg/orcd-rag/container/create_vector_store_with_container.sh --docs_path /path/to/documents\n</code></pre> <p>This will create a vector database located at <code>~/.cache/orcd_rag/vector_stores/&lt;docs name&gt;_vector_store</code>.</p> <p>Note</p> <p>Large files (especially PDFs) may need to be split into smaller files to avoid exceeding memory limits on the GPU.</p>","tags":["LLM","GPU","Howto Recipes","Engaging"]},{"location":"recipes/relion/","title":"Relion","text":"<p>Warning</p> <p>This page has been archived. The information present is not updated and may no longer be accurate.</p>","tags":["MPI","RELION","Howto Recipes","Install Recipe"]},{"location":"recipes/relion/#installing-and-using-relion","title":"Installing and Using RELION","text":"<p>RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a software package that employs an empirical Bayesian approach for electron cryo-microscopy (cryo-EM) structure determination. </p>","tags":["MPI","RELION","Howto Recipes","Install Recipe"]},{"location":"recipes/relion/#relion-on-satori","title":"RELION on Satori","text":"<p>This recipe is for building and using RELION on x86 nodes on Satori. It is different from working on IBM power9 nodes on Satori.</p> <p>Note</p> <p>The x86 nodes are available to some labs only. </p>","tags":["MPI","RELION","Howto Recipes","Install Recipe"]},{"location":"recipes/relion/#install-relion","title":"Install RELION","text":"<p>Go to your directory and download RELION, <pre><code>cd /nobackup/users/$USER\ngit clone https://github.com/3dem/relion.git\n</code></pre></p> <p>Get an interactive session on x86 nodes of Satori, <pre><code>srun -p sched_mit_mbathe -c 2 -t 60 --pty bash\n</code></pre></p> <p>Load modules for the GCC compiler and Openmpi implementation, <pre><code>module use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64  \nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64 \n</code></pre></p> <p>Note: These modules are installed for the x86 nodes only. </p> <p>Build RELION with CUDA and FFTW features, <pre><code>cd ~\nmkdir relion\ncd relion\ngit checkout master \ncd ..\nmkdir 4.0.1\ncd 4.0.1\nmkdir install\nmkdir build\ncd build\n\ncmake -DCMAKE_INSTALL_PREFIX=/home/$USER/relion/4.0.1/install -DFORCE_OWN_FFTW=ON -DAMDFFTW=ON -DCUDA_ARCH=80 ../../relion\nmake\nmake install\n</code></pre></p> <p>It is all set for the installation.</p>","tags":["MPI","RELION","Howto Recipes","Install Recipe"]},{"location":"recipes/relion/#use-relion","title":"Use RELION","text":"<p>There is a nice Graphical User Interface (GUI) for RELION. To use the GUI, first log in Satori with x-forwarding support, <pre><code>ssh -Y &lt;user&gt;@satori-login-002.mit.edu\n</code></pre></p> <p>Get an interactive session with GPU and x-forwarding support on x86 nodes of Satori, <pre><code>srun --x11 -p sched_mit_mbathe --gres=gpu:1 -c 6 -t 60 --pty bash\n</code></pre></p> <p>Set up environment for compilers, mpi implementation, FFTw, and RELION, <pre><code>module use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64  \nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64 \nmodule load fftw/3.3.10-x86_64\nexport LD_LIBRARY_PATH=/nfs/software001/home/software-r8-x86_64/spack-20230328/opt/spack/linux-rocky8-x86_64/gcc-12.2.0/fftw-3.3.10-qiaruimvw6zu2h4f5eolqom7tixem6vk/lib:$LD_LIBRARY_PATH\nexport RELION_HOME=/home/$USER/relion/4.0.1/install\nexport PATH=${RELION_HOME}/bin:$PATH\nexport LD_LIBRARY_PATH=${RELION_HOME}/lib:$LD_LIBRARY_PATH\n</code></pre></p> <p>then open the RELION GUI,  <pre><code>relion &amp;\n</code></pre></p> <p>Users can use GUI to edit files or submit jobs. Refer to details on this page. </p> <p>Alternatively, users can prepare a batch job script to submit jobs. </p> <p>Download RELION benchmarks for testing,  <pre><code>cd ~/relion\nwget ftp://ftp.mrc-lmb.cam.ac.uk/pub/scheres/relion_benchmark.tar.gz\n</code></pre> then all benchmark files will be saved in a directory named <code>relion_benchmark</code>.</p> <p>Here is an exmaple job script, <pre><code>#!/bin/bash\n#SBATCH --partition=sched_mit_mbathe\n#SBATCH --time=12:00:00\n#SBATCH --nodes=1\n#SBATCH -n 20\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=10000\n#SBATCH --gres=gpu:4\n#SBATCH --chdir='.'\n\nmodule use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64\nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64\nexport LD_LIBRARY_PATH=/nfs/software001/home/software-r8-x86_64/spack-20230328/opt/spack/linux-rocky8-x86_64/gcc-12.2.0/fftw-3.3.10-qiaruimvw6zu2h4f5eolqom7tixem6vk/lib:$LD_LIBRARY_PATH\nexport RELION_HOME=\"/home/$USER/relion/4.0.1/install\"\nexport PATH=${RELION_HOME}/bin:$PATH\nexport LD_LIBRARY_PATH=${RELION_HOME}/lib:$LD_LIBRARY_PATH\n\ncd ~/relion/relion_benchmark\nmkdir output\n\nmpirun -np 20 relion_refine_mpi \\\n  --i Particles/shiny_2sets.star \\\n  --o output \\\n  --ref emd_2660.map:mrc \\\n  --ini_high 60 \\\n  --pool 100 \\\n  --pad 2  \\\n  --ctf \\\n  --iter 25 \\\n  --tau2_fudge 4 \\\n  --particle_diameter 360 \\\n  --K 4 \\\n  --flatten_solvent \\\n  --zero_mask \\\n  --oversampling 1 \\\n  --healpix_order 2 \\\n  --offset_range 5 \\\n  --offset_step 2 \\\n  --sym C1 \\\n  --norm \\\n  --scale \\\n  --j 1   \\\n  --gpu \"\" \\\n --dont_combine_weights_via_disc \\\n  --scratch_dir /tmp\n</code></pre></p> <p>Add the above lines in a file named <code>job.sh</code>, then submit the job, <pre><code>sbatch job.sh\n</code></pre></p>","tags":["MPI","RELION","Howto Recipes","Install Recipe"]},{"location":"recipes/torch-gpu-intermediate/","title":"Intermediate Distributed Deep Learning with PyTorch","text":"<p>Deep learning is the foundation of artificial intelligence nowadays. Deep learning programs can be accelerated substantially on GPUs.  </p> <p>There are various parallelisms to enable distributed deep learning on multiple GPUs, including data parallel and model parallel. </p> <p>We have introduced basic recipes of data parallel with PyTorch, which is a popular Python package for working on deep learning projects.</p> <p>In data parallel, the model has to fit into the GPU memory. However, large model sizes are required for large language models (LLMs) based on the transformer architecture. When the model does not fit into the memory of a single GPU, the normal data parallelism does not work. </p> <p>On this page, we will introduce intermediate recipes to train large models on multiple GPUs with PyTorch. </p> <p>First, there is a Fully Sharded Data Parallel (FSDP) approach to split the model into multiple GPUs so that the memory requirement fits. A shard of the model is stored in each GPU, and communication between GPUs happens during the training process. We will introduce FSDP recipes in the first section. </p> <p>However, FSDP does not gain additional speedup beyond the data parallel framework. Better approaches are based on model parallel, which not only splits the model into multiple GPUs but also accelerates the training process with parallel sharded computations. There are various schemes of model parallel, such as pipeline parallel (PP) and tensor parallel (TP). Usually, model parallel is applied on top of data parallel to gain further speedup. In the second section, we will focus on recipes of hybrid Fully Sharded Data Parallel and Tensor Parallel (referred to as FSDP + TP) . </p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu-intermediate/#installing-pytorch","title":"Installing PyTorch","text":"Engaging <p>First, load a Miniforge module to provide a Python platform with PyTorch and CUDA support preinstalled,   <pre><code>module load miniforge/24.3.0-0\n</code></pre></p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu-intermediate/#fully-sharded-data-parallel","title":"Fully Sharded Data Parallel","text":"<p>We use an example code to train a convolutional neural network (CNN) with the MNIST data set.</p> <p>We will first run the example on a single GPU and then extend it to multiple GPUs with FSDP.</p> <p>Download the codes mnist_gpu.py and FSDP_mnist.py for these two cases respectively. </p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu-intermediate/#an-example-with-a-single-gpu","title":"An example with a single GPU","text":"Engaging <p>To run the example on a single GPU, prepare a job script named <code>job.sh</code> like this,  <pre><code>#!/bin/bash\n#SBATCH -p mit_normal_gpu\n#SBATCH --job-name=mnist-gpu\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --mem=20GB\n#SBATCH --gres=gpu:h200:1  \n\nmodule load miniforge/24.3.0-0\n\npython mnist_gpu.py\n</code></pre></p> <p>Here we sepecify the GPU type of H200 with <code>--gres=gpu:h200:1</code>. If one is not particular about the type of GPU, <code>--gres=gpu:1</code> can be used instead. </p> <p>Submit the job script,  <pre><code>sbatch job.sh\n</code></pre></p> <p>While the job is running, you can check if the program runs on a GPU. First, check the hostname that it runs on, <pre><code>squeue --me\n</code></pre> and then log in to the node, <pre><code>ssh &lt;nodeXXX&gt;\n</code></pre> and check the GPU usage with the <code>nvtop</code> command.</p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu-intermediate/#single-node-multi-gpu-fsdp","title":"Single-node multi-GPU FSDP","text":"<p>Now we extend this example to multiple GPUs on a single node with FSDP. </p> Engaging <p>Prepare a job script named <code>job.sh</code> like this,  <pre><code>#!/bin/bash\n#SBATCH -p mit_normal_gpu \n#SBATCH --job-name=fsdp\n#SBATCH -N 1\n#SBATCH -n 2\n#SBATCH --mem=20GB\n#SBATCH --gres=gpu:h200:2\n\nmodule load miniforge/24.3.0-0\n\npython FSDP_mnist.py\n</code></pre>  then submit it,  <pre><code>sbatch job.sh\n</code></pre></p> <p>As set up in the program <code>FSDP_mnist.py</code>, it will run on all GPUs requested in Slurm, that is 2 in this case. That says the model is split into 2 shards, each stored on a GPU, and the training process happens on 2 batches of data simultaneously. Communication between GPUs happens under the hood. </p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu-intermediate/#hybrid-fully-sharded-data-parallel-and-tensor-parallel","title":"Hybrid Fully Sharded Data Parallel and Tensor Parallel","text":"<p>Tensor parallel can be applied on top of data parallel to gain further speedup. In this section, we introduce recipes of hybrid FSDP and TP.</p> <p>We use an example that implements FSDP + TP on LLAMA2 (Large Language Model Meta AI 2). Refer to the description of this example. Download the codes: fsdp_tp_example.py, llama2_model.py, and log_utils.py.</p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu-intermediate/#single-node-multi-gpu-fsdp-tp","title":"Single-node multi-GPU FSDP + TP","text":"<p>First, let's run the example on multiple GPUs within a single node. </p> <p>The code <code>fsdp_tp_example.py</code> is set up for this purpose. The TP size is set to be 2 in the code. The total number of GPUs should be equal to a multiple of the TP size, then the FSDP size is equal to the number of GPUs divided by the TP size.</p> Engaging <p>To run this example on multiple GPUs, prepare a job script like this,  <pre><code>#!/bin/bash                                                                                          \n#SBATCH -p mit_preemptable                                                                           \n#SBATCH -t 60                                                                                        \n#SBATCH -N 1                                                                                         \n#SBATCH -n 4                                                                                         \n#SBATCH --mem=30GB                                                                                   \n#SBATCH --gres=gpu:h200:4                                                                            \n\nmodule load miniforge/24.3.0-0\n\ntorchrun --nnodes=1 --nproc_per_node=4 \\\n     --rdzv_id=$SLURM_JOB_ID \\\n     --rdzv_endpoint=\"localhost:1234\" \\\n     fsdp_tp_example.py\n</code></pre>  then submit it,  <pre><code>sbatch job.sh\n</code></pre></p> <p>With the flags <code>--nnodes=1 --nproc-per-node=4</code>, the <code>torchrun</code> command will run the program on 4 GPUs within one node. The training process happens on 2 batches of data with FSDP, and the model is trained with TP sharded computation on 2 GPUs for each batch of data.</p> <p>The flags with <code>rdzv</code> (meaning the Rendezvous protocol) are required by <code>torchrun</code> to coordinate multiple processes. The flag <code>--rdzv-id=$SLURM_JOB_ID</code> sets to the <code>rdzv</code> ID to be the job ID, but it can be any random number. The flag <code>--rdzv-endpoint=localhost:1234</code> is to set the host and the port. Use <code>localhost</code> when there is only one node. The port can be any 4- or 5-digit number larger than 1024. </p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu-intermediate/#multi-node-multi-gpu-fsdp-tp","title":"Multi-node multi-GPU FSDP + TP","text":"<p>Finally, we run this example on multiple GPUs across multiple nodes. Note that this example requires 8 GPUs which is more than the standard user has access to on Engaging. If you have access to more GPUs, run this example on the corresponding partition.</p> Engaging <p>Prepare a job script like this,  <pre><code>#!/bin/bash\n#SBATCH -p  mit_normal_gpu\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --gpus-per-node=h200:4 \n#SBATCH --mem=30GB\n\nmodule load miniforge/24.3.0-0\n\n# Get IP address of the master node\nnodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )\nnodes_array=($nodes)\nmaster_node=${nodes_array[0]}\nmaster_node_ip=$(srun --nodes=1 --ntasks=1 -w \"$master_node\" hostname --ip-address)\n\nsrun torchrun --nnodes=$SLURM_NNODES \\\n          --nproc-per-node=$SLURM_CPUS_PER_TASK \\\n          --rdzv-id=$SLURM_JOB_ID   \\\n          --rdzv-backend=c10d \\\n          --rdzv-endpoint=$master_node_ip:1234 \\\n          fsdp_tp_example.py\n</code></pre>  then submit it,  <pre><code>sbatch job.sh\n</code></pre></p> <p>The configuration of the <code>#SBATCH</code> and <code>torchrun</code> flags is similar to that in the basic recipe of data parallel. </p> <p>The program runs on 8 GPUs with 4 per node. As is set up in the code <code>fsdp_tp_example.py</code>, the training process happens on 4 batches of data with FSDP,  and the model is trained with TP sharded computation on 2 GPUs for each batch of data.</p> Topology of GPU Communication <p>The NVIDIA Collective Communications Library (NCCL) is set as the backend in all of the PyTorch programs here, so that the communication between GPUs within one node benefits from the high bandwidth of NVLinks, and the communication between GPUs across nodes benefits from the bandwidth of the Infiniband network. </p> <p>The intra-node GPU-GPU communication speed is much faster than the inter-node. The communication overhead of TP is much larger than that of FSDP. The topology of GPU communication is set up (in the code <code>fsdp_tp_example.py</code>) in a way that TP communication is intra-node and FSDP communication is inter-node, so that the usage of network bandwidth is optimized. </p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu/","title":"Deep Learning with PyTorch on GPUs","text":"<p>Deep learning is the foundation of artificial intelligence nowadays. Deep learning programs can be accelerated substantially on GPUs. </p> <p>PyTorch is a popular Python package for working on deep learning projects.</p> <p>This page introduces recipes to run deep-learning programs on GPUs with PyTorch. </p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu/#installing-pytorch","title":"Installing PyTorch","text":"Engaging <p>First, load a Miniforge module to provide a Python platform with PyTorch preinstalled with CUDA support, which enables running on GPUs,  <pre><code>module load miniforge/24.3.0-0\n</code></pre></p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu/#pytorch-on-cpu-and-a-single-gpu","title":"PyTorch on CPU and a single GPU","text":"<p>We start with a recipe to run PyTorch on one CPU and one GPU.</p> <p>We use an example code training a convolutional neural network (CNN) with the CIFAR10 data set. Refer to the description of this example and download the codes for CPU and for GPU. </p> Engaging <p>Prepare a job script named <code>job.sh</code> like this,  <pre><code>#!/bin/bash\n#SBATCH -p mit_normal_gpu   \n#SBATCH --gres=gpu:1 \n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 2\n#SBATCH --mem=10GB\n\nmodule load miniforge/24.3.0-0\n\necho \"~~~~~~~~ Run the program on CPU ~~~~~~~~~\"\ntime python cnn_cifar10_cpu.py\necho \"~~~~~~~~ Run the program on GPU ~~~~~~~~~\"\ntime python cnn_cifar10_gpu.py\n</code></pre>  then submit it,  <pre><code>sbatch job.sh\n</code></pre></p> <p>The <code>mit_normal_gpu</code> partition is for all MIT users. If your lab has a partition with GPUs, you can use it too.  </p> <p>The <code>#SBATCH</code> flags <code>-N 1 -n 2</code> requests two CPU cores on one node, and the <code>--mem=10GB</code> means 10 GB of memory per node (not per core).</p> <p>The programs <code>cnn_cifar10_cpu.py</code> and <code>cnn_cifar10_gpu.py</code> will run on CPUs and a GPU, respectively. When the problem size is large, the program will be accelerated on a GPU. </p> <p>While the job is running, you can check if the program runs on a GPU. First, check the hostname that it runs on, <pre><code>squeue --me\n</code></pre> and then log in the node, <pre><code>ssh &lt;nodeXXX&gt;\n</code></pre> and check the GPU usage with the <code>nvtop</code> command. Documentation for <code>nvtop</code> can be found here.</p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu/#pytorch-on-multiple-gpus","title":"PyTorch on multiple GPUs","text":"<p>Deep learning programs can be further accelerated on multiple GPUs. </p> <p>There are various parallelisms to enable distributed deep learning on multiple GPUs, including data parallel and model parallel. We will focus on data parallel on this page.   </p> <p>Data parallel allows training a model with multiple batches of data simultaneously. The model has to fit into the GPU memory.</p> <p>On a cluster, there are many nodes and multiple GPUs on each node. We will first introduce a recipe to run PyTorch programs with multiple GPUs within one node, and then extend it to multiple nodes. </p> <p>We use an example code that trains a linear network with a random data set, which is implemented with the Distributed Data Parallel package in PyTorch. Refer to the description of this example for multiple GPUs within one node and for multiple GPUs across multiple nodes. </p> <p>Download the codes for this example: datautils.py, multigpu.py, multigpu_torchrun.py, and multinode.py.</p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu/#single-node-multi-gpu-data-parallel","title":"Single-node multi-GPU data parallel","text":"<p>In this section, we introduce a recipe for single-node multi-GPU data parallel. The program <code>multigpu.py</code> is set up for this purpose. </p> Engaging <p>To run the program on 2 GPUs within one node, prepare a job script named <code>job.sh</code> like this,  <pre><code>#!/bin/bash\n#SBATCH -p mit_normal_gpu\n#SBATCH --job-name=ddp\n#SBATCH -N 1\n#SBATCH -n 2\n#SBATCH --mem=20GB\n#SBATCH --gres=gpu:2\n\nmodule load miniforge/24.3.0-0\n\necho \"======== Run on multiple GPUs ========\"\n# run for 100 epochs and save checkpoints every 20 epochs\npython multigpu.py --batch_size=1024 100 20\n</code></pre>  then submit it,  <pre><code>sbatch job.sh\n</code></pre></p> <p>The <code>-N 1 -n 2 --gres=gpu:2</code> flags request 2 CPU cores and 2 GPUs on one node. For most GPU programs, it is recommended to set the number of CPU cores no less than the number of GPUs.</p> <p>As is set up in the code <code>multigpu.py</code>, it will run on all of the GPUs requested in Slurm, which means 2 GPUs within one node in this case. The training process happens on 2 batches of data simultaneously. </p> <p>You can try to check if the program runs on multiple GPUs using the <code>nvtop</code> command as described in the previous section, but the program runs so fast that this might be challenging. You can also check in the Slurm out file that GPUs with different IDs were utilized.</p> <p>There is another way to run a PyTorch prgram with multiple GPUs, that is to use the <code>torchrun</code> command. The program for this purpose is <code>multigpu_torchrun.py</code>. In the above job script, change the last line to this,  <pre><code>torchrun --nnodes=1 --nproc_per_node=2 \\\n         --rdzv_id=$SLURM_JOB_ID \\\n         --rdzv_endpoint=\"localhost:1234\" \\\n         multigpu_torchrun.py --batch_size=1024 100 20\n</code></pre></p> <p>With the flags <code>--nnodes=1 --nproc-per-node=2</code>, the <code>torchrun</code> command will run the program on 2 GPUs within one node. </p> <p>The flags with <code>rdzv</code> (meaning the Rendezvous protocol) are required by <code>torchrun</code> to coordinate multiple processes. The flag <code>--rdzv-id=$SLURM_JOB_ID</code> sets to the <code>rdzv</code> ID be the job ID, but it can be any random number. The flag <code>--rdzv-endpoint=localhost:1234</code> is to set the host and the port. Use <code>localhost</code> when there is only one node. The port can be any 4- or 5-digit number larger than 1024. </p> <p>The <code>torchrun</code> command will be useful for running the program across multiple nodes in the next section. </p> GPU communication within one node <p>The NVIDIA Collective Communications Library (NCCL) is set as the backend in the PyTorch programs <code>multigpu.py</code> and <code>multigpu_torchrun.py</code>, so that the data communication between GPUs within one node benefits from the high bandwidth of NVLinks.  </p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/torch-gpu/#multi-node-multi-gpu-data-parallel","title":"Multi-node multi-GPU data parallel","text":"<p>Now we extend the above example to multi-node multi-GPU data parallel. The program <code>multinode.py</code> is set up for this purpose.</p> <p>There are two key points in this approach.</p> <ol> <li> <p>Use the <code>srun</code> command in Slurm to launch a <code>torchrun</code> command on each node.</p> </li> <li> <p>Set up <code>torchrun</code> to coordinate processes on different nodes.</p> </li> </ol> Engaging <p>To run on multiple GPUs across multiple nodes, prepare a job script like this,  <pre><code>#!/bin/bash\n#SBATCH -p mit_normal_gpu\n#SBATCH --job-name=ddp-2nodes\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=2\n#SBATCH --gpus-per-node=2 \n#SBATCH --mem=20GB\n\nmodule load miniforge/24.3.0-0\n\n# Get IP address of the master node\nnodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )\nnodes_array=($nodes)\nmaster_node=${nodes_array[0]}\nmaster_node_ip=$(srun --nodes=1 --ntasks=1 -w \"$master_node\" hostname --ip-address)\n\necho \"======== Run on multiple GPUs across multiple nodes ======\"     \nsrun torchrun --nnodes=$SLURM_NNODES \\\n     --nproc-per-node=$SLURM_CPUS_PER_TASK \\\n     --rdzv-id=$SLURM_JOB_ID   \\\n     --rdzv-backend=c10d \\\n     --rdzv-endpoint=$master_node_ip:1234 \\\n     multinode.py --batch_size=1024 100 20\n</code></pre>  then submit it,  <pre><code>sbatch job.sh\n</code></pre></p> <p>As the <code>#SBATCH</code> flags <code>-N 2</code> and <code>--ntasks-per-node=1</code> request for two nodes with one task per node, the <code>srun</code> command launches a <code>torchrun</code> command on each of the two nodes.</p> <p>The <code>#SBATCH</code> flags <code>--cpus-per-task=2</code> and <code>--gpus-per-node=2</code> request 2 GPU cores and 2 GPUs on each node. Accordingly, the <code>torchrun</code> flags are set as <code>--nnodes=$SLURM_NNODES --nproc-per-node=$SLURM_CPUS_PER_TASK</code>, so that the <code>torchrun</code> command runs the program on 2 GPUs on each of the two nodes. That says the program runs on 4 GPUs, and thus the training process happens on 4 batches of data simultaneously. </p> <p>The flags with <code>rdzv</code> are required by <code>torchrun</code> to coordinate processes across nodes. The <code>--rdzv-backend=c10d</code> is to use a C10d store (by default TCPStore) as the rendezvous backend, the advantage of which is that it requires no 3rd-party dependency. The <code>--rdzv-endpoint=$master_node_ip:1234</code> is to set up the IP address and the port of the master node. The IP address is obtained in a previous part of the job script.</p> <p>Refer to details of torchrun on this page.</p> GPU communication across nodes <p>The NCCL is set as backend in the PyTorch program <code>multinode.py</code>, so that the data communication between GPUs within one node benefits from the high bandwidth of NVLinks, and the data communication between GPUs across nodes benefits from the bandwidth of the Infiniband network. </p>","tags":["Engaging","PyTorch","GPU","Howto Recipes"]},{"location":"recipes/vscode/","title":"Using VSCode on an ORCD System","text":"<p>VSCode is a convenient IDE for development, and one of its nicest features is its ability to run on a remote system using its RemoteSSH extension. This means you can have the VSCode window on your computer, while the files and anything you run will be on the remote system you are connected to.</p> <p>Once you've installed the RemoteSSH extension this is fairly easy to set up. However, it is also very easy to set up in such a way that it is not only slow for you, but it also puts excess load on the login nodes and in turn slows things down for others on that node. Luckily, with a few extra steps you can run VSCode on a compute node where it can have more resources to run and won't impact others as much.</p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#adjust-the-remotessh-extension-settings","title":"Adjust the RemoteSSH Extension Settings","text":"<p>Before you start, it is helpful to adjust VSCode's RemoteSSH Extension settings. Sometimes VS Code may cause you to be locked out of your Engaging account because it makes repeated Duo authentication attempts. To mitigate this behavior, edit a few of the VS Code settings:</p> <ul> <li>Remote.SSH: Connect Timeout: Set to 60 seconds. Making this longer gives you more time to accept the Duo push before the RemoteSSH extension tries again.</li> <li>Remote.SSH: Max Reconnection Attempts: Set to 0. This prevents RemoteSSH from trying to reconnect automatically over and over, sending you Duo pushes when you aren't expecting them. This is what usually causes the lockout. When you set this to 0 VSCode will ask you before trying to reconnect. You can also safely set this to 1 to allow it to make a single reconnection attempt.</li> </ul>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#setting-up-your-config-file","title":"Setting up your Config File","text":"<p>Click the \"Open a Remote Window\" button in the bottom left corner of your VSCode window (It is a small blue rectangle labeled with <code>&gt;&lt;</code>). In the bar at the top of the page select \"Connect to Host...\", then \"Configure SSH Hosts\", and select first option, which will differ depending on your operating system. This will open your config file in a VSCode tab.</p> <p>To run on a compute node you will need at least 2 entries in this file. The first will be a login node that you'll \"jump\" through and the second will be the compute node that is your final destination.</p> EngagingOpenMind <p>In this config file we are including some settings for Control Channels, for convenience. Control channels minimize the number of 2-Factor prompts that you get when connecting.</p> config<pre><code>Host orcd-login\n  HostName orcd-login.mit.edu\n  ControlMaster auto\n  ControlPath ~/.ssh/%r@%h:%p\n  ControlPersist 300s\n  User USERNAME\n\nHost orcd-compute\n  User USERNAME\n  HostName nodename\n  ProxyJump orcd-login\n</code></pre> <p>When make any initial connections you will be asked for your password in the bar at the top of the window, followed by which 2-Factor method you would prefer. Enter \"1\" for the default method that you've set up. After responding to your 2-factor authentication you should be connected. If you include the Control Channel settings above as long as your initial connection isn't disconnected additional connections won't require you to enter your password or respond to a 2-factor prompt again.</p> config<pre><code>Host om-login\n  HostName openmind7.mit.edu\n  User USERNAME\n\nHost om-compute\n  User USERNAME\n  HostName nodename\n  ProxyJump om-login\n</code></pre> <p>Note</p> <p>To use VSCode on a compute node, an SSH key is required. If you haven't set up SSH keys yet, refer to the SSH Key Setup guide.</p> <p>Replace <code>USERNAME</code> with your username on the system you are connecting to. We will fill in \"nodename\" later.</p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#starting-your-vscode-session-on-a-compute-node","title":"Starting your VSCode Session on a Compute Node","text":"<p>Each time you sit down to do remote work through VSCode you will have three steps:</p> <ol> <li>Start an interactive job on the target system and note the name of the node your job is running on</li> <li>Update your config file with the node name</li> <li>Connect to the compute node using your updated config</li> </ol> <p>We go through these steps in detail below.</p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#start-an-interactive-job","title":"Start an Interactive Job","text":"<p>Open a terminal window and ssh into the login node. If you are not used to doing this you can open a terminal in VSCode and run:</p> EngagingOpen Mind <pre><code>ssh orcd-login\n</code></pre> <pre><code>ssh om-login\n</code></pre> <p>Use the name you have used for the login <code>Host</code> in your config file if different than the one above. The example screenshot below shows logging into one of the Engaging login nodes with ssh in a VSCode terminal window.</p> <p></p> <p>Once you are logged in start an interactive session. If you are planning to only edit files a single core may be sufficient, but if you plan to run code or Jupyter Notebooks you may want to allocate more resources accordingly. Refer to the documentation for your system on how to request an interactive job:</p> EngagingSuperCloudOpenMind <p>Engaging's Documentation for Running Jobs</p> <p>SuperCloud's Documentation for Running Jobs</p> <p>OpenMind's Documentation for Running Jobs</p> <p>Once your job has started you can run the <code>hostname</code> command to get the name of the node your interactive job is running on. You can also run the <code>squeue --me</code> command to list all your running jobs and get the hostname from the last column.</p> <p>The screenshot below shows requesting a single interactive core for 1 hour on Engaging:</p> <p></p> <pre><code>salloc -t 1:00:00 -p mit_normal\n</code></pre> <p>Note that the scheduler will also tell you which node you are allocated in its output. In this screenshot my node name is <code>node2704</code>.</p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#update-your-config-file","title":"Update your Config File","text":"<p>Update the <code>HostName</code> of your compute node entry in your config file. If your config file is not open, follow the instructions above to open it again. Then replace whatever you have for <code>HostName</code> in your config file with the output of the <code>hostname</code> command you ran in your interactive session, or got from <code>squeue --me</code>.</p> <p>If your compute node is <code>node1234</code> then your config file should look something like:</p> EngagingOpen Mind config<pre><code>Host orcd-login\n  HostName orcd-login001.mit.edu\n  User USERNAME\n\nHost orcd-compute\n  User USERNAME\n  HostName node1234\n  ProxyJump orcd-login\n</code></pre> config<pre><code>Host om-login\n  HostName openmind7.mit.edu\n  User USERNAME\n\nHost om-compute\n  User USERNAME\n  HostName node1234\n  ProxyJump om-login\n</code></pre> <p>Where <code>USERNAME</code> is replaced by your username.</p> <p>This screenshot shows updating the config file for an interactive job running on Engaging:</p> <p></p> <p>Since the interactive job in my screenshot is running on <code>node2704</code>, I have updated <code>HostName</code> to <code>node2704</code> for the <code>orcd-compute</code> entry in my config file.</p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#connect-to-the-compute-node","title":"Connect to the Compute Node","text":"<p>You are ready to connect to the compute node you have allocated through your interactive job from VSCode. Select the \"Open a Remote Window\" button in the bottom left corner of your VSCode window. In the bar at the top of the page select \"Connect to Host...\" and select the Host for the compute node that you have created.</p> EngagingOpen Mind <p>In the example config file above this would be <code>orcd-compute</code>.</p> <p>In the example config file above this would be <code>om-compute</code>.</p> <p>Here is what this might look like for Engaging:</p> <p></p>","tags":["Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#other-vscode-best-practices-tips-and-tricks","title":"Other VSCode Best Practices, Tips, and Tricks","text":"<ul> <li>Avoid running VSCode through RemoteSSH on the login nodes. If you are only editing files this might be okay, although it is not encouraged. Beyond editing files please use a compute node for VSCode, as described on this page.</li> <li>Add only the specific directories you need to your workspace. VSCode constantly scans all the files files and runs git commands on any local git repositories in your workspace, and it does this recursively. For this reason adding high-level directories to your workspace can slow things down quite a bit. For example, avoid adding your entire home directory or group storage to your VSCode session workspace.</li> <li>If VSCode is slow to start up on an ORCD System, check to see whether you are activating a conda environment at login. If you are, run the command <code>conda config --set auto_activate_base false</code> to prevent this. You will only have to do this once.</li> <li>Sometimes, VS Code may cause you to be locked out of your Engaging account because it makes repeated Duo authentication attempts. To mitigate this behavior, edit a few of the VS Code settings:<ul> <li>Remote.SSH: Connect Timeout: Set to 60 seconds. Making this longer gives you more time to accept the Duo push before the RemoteSSH extension tries again.</li> <li>Remote.SSH: Max Reconnection Attempts: Set to 0. This prevents RemoteSSH from trying to reconnect automatically over and over, sending you Duo pushes when you aren't expecting them. This is what usually causes the lockout. When you set this to 0 VSCode will ask before trying to reconnect. You can also safely set this to 1 to allow it to make a single reconnection attempt.</li> <li>Remote.SSH: Show Login Terminal: Checking this box would let you see useful debugging information while VS Code is starting up.</li> </ul> </li> </ul>","tags":["Howto Recipes","Best Practices"]},{"location":"running-jobs/application-analysis/","title":"Analyzing Job Resource Usage","text":"<p>Here we provide an overview for how to analyze the resources your jobs use, utilizing the commands <code>htop</code>, <code>nvidia-smi</code>, and <code>nvtop</code>. Using these tools is important for both optimizing your code as well as only requesting the amount of resources your jobs need, leaving more resources open for others to use. A good place to start before using the commands we outline here is with the documentation on Requesting Resources.</p>"},{"location":"running-jobs/application-analysis/#htop","title":"htop","text":"<p>You can get a lot of information about your running jobs through the <code>htop</code> command. It sort of gives you a way to watch the compute node your job is running on work on your job. It will show you your current instantaneous CPU and Memory utilization, how many cores are being used, how many threads are running, a list of your processes running on the node, among other things. If you are familiar with the top Linux command it is similar but shows more information.</p>"},{"location":"running-jobs/application-analysis/#how-to-run-htop","title":"How to Run htop","text":"<p>The key thing to know is you need to run the <code>htop</code> command on the node where your job is running. First, get the name of the node your job is running on. You can do this by running the <code>squeue --me</code> command. The last column has the node name. If you are running in an interactive job the node name will also be at your command prompt.</p> <p>The next step is to ssh to the node where your job is running. Run the command <code>ssh NODENAME</code>, where NODENAME is the name of the node you just found. If you are running an interactive job, open a new terminal tab or window and log into Engaging. Note that you can only ssh to nodes where you have a job running, and you should exit the node once you are done monitoring your job with the exit command.</p> <p></p> <p>In the above image, you can see we have run <code>ssh node1600</code>, and the command prompt now indicates we are on that node. From here we can run:</p> <p><code>htop -u USERNAME</code></p> <p>where USERNAME is your username. Specifying your username filters out processes that don't belong to you and makes it easier to interpret the output.</p>"},{"location":"running-jobs/application-analysis/#using-htop","title":"Using htop","text":"<p>The following is a screenshot of the htop program.</p> <p></p> <p>The first thing you'll notice when you run htop yourself is it is constantly moving to give you instantaneous statistics. There are two main sections in the htop output. The bottom half shows the processes running on the node and the top half shows the activity on the node. Note that by specifying your username you will only see your processes, but the activity in the top half may reflect processes running on the node that aren't yours, so if you want an accurate representation of your job you may want to request a full node.</p> <p>If you are on a node with a lot of cores it may be difficult to see the full output. You can set htop to use a more compact visualization by creating a config file. First create the directory ~/.config/htop if it doesn't already exist. Download this file and put it in the ~/.config/htop directory. The file should be named htoprc.</p> <p></p> <p>In the top half you'll see numbered bars with percentages. These represent the cores on the node. You'll see twice the number of bars as cores. This is due to hyperthreading: each physical core has two \"virtual\" cores. This display gives you a rough idea of how hard the cpus on the node are working. If a CPU is at 100%, it is fully utilized. If there isn't much activity, not much is going on on the node. If you are expecting multiple processes or threads to be working, and you only see one bar fill up, you have an indication that you might need to change a setting. Alternatively, maybe you haven't explicitly asked your program to use multiple cores and you expect it to use only one, but you see multiple CPUs at 100%, even when you are the only person on the node. In that case you may need to tell the package you are using how many threads or cpus to use.</p> <p></p> <p>In the top half you'll also see a bar labeled \"Mem\" under the CPU bars. This is the instantaneous memory usage on the node.  Note that just because you see a certain number here while you are watching doesn't mean this is the maximum memory your job will use. To get that information you'll want to use the sacct command.</p> <p></p> <p>Next to the \"Mem\" bar you'll see some text labeled \"Tasks\". This lists running processes (tasks), threads, and running processes or threads. You generally don't want more running processes or threads than the number of cores on the node. Again, this number may reflect another user's  threads or processes running on the node. In the screenshot above it shows that there are 43 running threads. I requested 16 cores for the job, and with hyperthreading the application created 32 threads. There was another user running on the node who was running their own job that added a few more threads.</p> <p></p> <p>Below the \"Tasks\" is a line that shows \"Load average\". It shows three numbers: the load average for the past 1, 5, and 15 minutes. The load is roughly the number of running processes, the load average over the last X minutes. This number is somewhat related to CPU utilization, as each CPU is realistically capable of running a single process at a time. It therefore gives you roughly a single-number condensed view of the CPU utilization image at the top. If this number is high in relation to the number of cores on the node, then you know those cores are working hard. If for some reason this number is over the number of cores, you are likely overworking the node and should scale back on the number of threads or processes you have running on that node. Engaging is configured to prevent this, but it is a good thing to keep an eye on. You will may find with fewer threads or processes your application will actually run faster in this case, as they are no longer competing for resources.</p> <p></p> <p>Now let's take a quick look at the bottom half. As mentioned above, this shows all of your processes and threads running on the node. You can see the PID (process ID), username (you'll only see your own processes), state, CPU utilization %, memory usage %, and the command that initiated the process, among other things. Clicking on the different headers re-sorts the list by that column, here I have it sorted by CPU%. In this case I started one Python process and that Python application created many threads. The main process happens to be at the top in this case, since I have it sorted by CPU%. When you have a multithreaded application, the CPU usage for the individual threads is included in the number for the main process. Each thread is currently using about 100% of a CPU (100% is full usage of a single CPU), so the total for the entire application is 3092%, meaning my application is using the equivalent of 30-31 full cores across all its threads. So even though I have 43 total running threads and/or processes, they aren't all using the full power of a CPU. All of the numbers on this bottom chart are instantaneous, rather than an average, but you can see the CPU% for the main task matches up somewhat with the load average above, when taking into account another user running on the node.</p>"},{"location":"running-jobs/application-analysis/#nvidia-smi","title":"nvidia-smi","text":"<p>The nvidia-smi command is useful for evaluating how your GPU-accelerated applications are making use of the GPU(s) that they are using. It will give you information about GPU utilization, GPU Memory utilization, and processes using the GPU.</p>"},{"location":"running-jobs/application-analysis/#how-to-run-nvidia-smi","title":"How to Run nvidia-smi","text":"<p>Like htop in the previous unit, you must be on the same node as your job to run nvidia-smi to monitor it. Further, because it is a GPU utility, if you try to run nvidia-smi on a node without a GPU you will get a command not found error. And, similar to htop, you will only see statistics for the GPU(s) you have allocated to your job, so if you are on a node with GPUs but didn't request any, you won't see any listed. For more information about how to request GPUs for your job, see the pages on Requesting Resources and Scheduler Overview.</p> <p>First, get the name of the node your job is running on. You can do this by running the <code>squeue --me</code> command. The last column has the node name. In the image below you can see the node name, node2804, in the last line of the squeue output. The next step is to ssh to the node where your job is running. Run the command <code>ssh NODENAME</code>, where NODENAME is the name of the node you just found. If you are running an interactive job, open a new terminal tab or window and log into Engaging and ssh to the node from there. Note that you can only ssh to nodes where you have a job running, and you should exit the node once you are done monitoring your job with the exit command.</p> <p></p> <p>In the above image, you can see we have run <code>ssh node2804</code>, and the command prompt now indicates we are on that node. From here we can run the nvidia-smi command as shown.</p>"},{"location":"running-jobs/application-analysis/#using-nvidia-smi","title":"Using nvidia-smi","text":"<p>When you run nvidia-smi you will get a single snapshot. If you would like this to continuously refresh, which is a bit more useful in practice, you can use the -l flag, which prints a refresh to the screen at a regular interval. By default this interval is 5 seconds, or you can specify an interval yourself. For example, if you want it to refresh every 10 seconds you would run <code>nvidia-smi -l 10</code>. Pressing Control+C on your keyboard will stop the continuous refresh.</p> <p></p> <p>Similar to htop, the top half of nvidia-smi shows you information about the GPU(s), the bottom half lists the process(es) using them.</p> <p></p> <p>The first box, circled above, tells you about the GPU, its state, and some of its physical properties. You can see the GPU name (L40S), its current temperature (38C), and current power usage and power cap (using 184W, capped at 350W). This is more informational than anything, but knowing how much power your GPU is using might be something you want to think about, especially if you are using them often.</p> <p></p> <p>The most useful part of the second box is the memory usage. It shows both the memory you have used (roughly 8.9GB here) and the total memory on the GPU, 46GB.</p> <p></p> <p>The final box at the top shows the GPU utilization. You can think of this as how much you are making use of the GPU. You want to aim for 100% utilization, that is when you are getting the most out of it. If your utilization is low and if the memory allows, you may want to give the GPU more data to work on. You could also train multiple models on the same GPU. If your GPU utilization is low, it may also be a sign that your application doesn't have enough work for the GPU to do, and you should check the speedup you are getting with the GPU vs CPUs. If it is modest, say 2x or 4x, then it isn't worth using GPUs and you can likely get more performance by scaling out to more CPUs.</p> <p></p> <p>The final table at the bottom shows the process(es) running on the GPU(s) you have allocated. You'll have the GPU number in the first column. If you have asked for one GPU this will always be a \"0\". Type refers to whether the process is a compute (C) or graphics (G) task. You'll have the Process Name, which will be \"python\" if you are running a python script as seen here. The last column shows the GPU Memory Usage.</p>"},{"location":"running-jobs/application-analysis/#nvtop","title":"nvtop","text":"<p><code>nvtop</code> is like <code>htop</code>, except for monitoring GPU usage. Like <code>nvidia-smi</code>, <code>nvtop</code> enables you to see the real-time statistics of your job on the GPU, including memory, compute, and power consumption. It additionally shows a sliding history of GPU compute and memory usage as well as a snapshot of CPU usage. <code>nvtop</code> in general shows more information than <code>nvidia-smi</code> and has a different look. It is up to the user's preference to pick one or use both.</p>"},{"location":"running-jobs/application-analysis/#how-to-run-nvtop","title":"How to Run nvtop","text":"<p>The principles of usage are very similar to that of htop, and we will outline them here. First, ssh into the node on which you are running a job using a GPU (<code>ssh NODENAME</code>) and run <code>nvtop</code>. You can refer to the htop section above for more details on finding the nodename as well as the ssh process. </p>"},{"location":"running-jobs/application-analysis/#using-nvtop","title":"Using nvtop","text":"<p>After running nvtop, you will see your usage updating in real time. Below we show an example of using this to monitor the GPU usage of RAG. If you don't yet have a GPU application and would like to get a sense of how to use nvtop, RAG could be a good example to start with. </p> <p></p> <p>Above, we see a display that comes up after running <code>nvtop</code>. Highlighted in yellow are some static device properties. In the top left, \"Device 0\" is always listed if we are only using one GPU. Next to it is \"L40S\", the model of GPU that we are running on, and to the right of that is the interconnect interface. In the bottom yellow box, the clock speed and memory speed of the L40S are listed.</p> <p></p> <p>In the image above, we have only loaded the model and have not yet asked a prompt which kicks off computation. As we expect, we see that the model takes considerable memory (31GB out of the possible 45GB), but the compute of the GPU is not being used. These can be seen in two places on the screen, highlighted in yellow. Note however that some applications, like JAX, artificially fill the memory of a GPU even if that memory is not needed. This can lead to an inaccurate picture of GPU utilization, and it would be good to check whether your application could be doing this.</p> <p></p> <p>Now, we see the results of nvtop after we have queried the RAG model, and therefore started computation on the GPU. Highlighted is the sliding history window of GPU compute and memory usage, and as expected we see a rise in both values.</p> <p></p> <p>In addition to the GPU's compute running at 99%, the CPU is also running at 99%. nvtop provides a snapshot of both GPU and CPU usage as outlined above.</p> <p></p> <p>On the top right, we see that \"RX\", data reception rate, and \"TX\", data transmission rate, have increased after querying the RAG model. These metrics are for data transfer between the CPU and GPU. We can also notice that the power and temperature of the system have shot up from our first look at the nvtop output. The metrics outlined in this snapshot are ones the user has less direct control over, but they can be useful for getting a better idea of how the GPU is operating.</p> <p>After taking a look at all the metrics above, you could evaluate whether you want to request more or less memory in the future and whether the job can be modified to use the GPU's compute more efficiently.</p>"},{"location":"running-jobs/available-resources/","title":"Engaging Public Partitions","text":""},{"location":"running-jobs/available-resources/#mit_normal","title":"mit_normal","text":"Nodes Cores Memory CPU model Misc. features Node list 6 2x96 1510GB AMD EPYC 9654 96-Core Processor nan node1620-1625 2 2x32 376GB AMD EPYC 9384X 32-Core Processor high_l3 node2704-2705 32 2x48 376GB AMD EPYC 9474F 48-Core Processor nan node1600-1619;node3103-3114 12 2x48 377GB AMD EPYC 9474F 48-Core Processor nan node3303-3314"},{"location":"running-jobs/available-resources/#mit_normal_gpu","title":"mit_normal_gpu","text":"Nodes Cores Memory CPU model GPUs GPU type GPU memory Misc. features Node list 1 2x32 1007GB INTELR XEONR PLATINUM 8562Y+ 3 NVIDIA L40S 44GB nan node3502 62 2x32 1007GB INTELR XEONR PLATINUM 8562Y+ 4 NVIDIA L40S 44GB nan node2804;node3002-3008;node3202-3208;node3302;node3402-3408;node3500-3501;node3503-3512;node4102-4108;node4200-4212;node4302-4305;node4502-4504 1 2x32 1007GB IntelR XeonR Platinum 8462Y+ 4 NVIDIA H100 80GB HBM3 79GB nan node2906 12 2x60 2014GB INTELR XEONR PLATINUM 8580 8 NVIDIA H200 140GB nan node2433-2434;node3000-3001;node3100-3101;node3200-3201;node3300-3301;node3400;node4100"},{"location":"running-jobs/available-resources/#mit_preemptable-cpu","title":"mit_preemptable (CPU)","text":"Nodes Cores Memory CPU model Misc. features Node list 2 2x48 1510GB AMD EPYC 9474F 48-Core Processor nan node3612-3613 6 2x96 1510GB AMD EPYC 9654 96-Core Processor nan node1620-1625 48 2x48 376GB AMD EPYC 9474F 48-Core Processor nan node1600-1619;node1626-1631;node2503-2513;node2523-2525;node3602-3607;node3610-3611 2 2x48 754GB AMD EPYC 9474F 48-Core Processor nan node3608-3609"},{"location":"running-jobs/available-resources/#mit_preemptable-gpu","title":"mit_preemptable (GPU)","text":"Nodes Cores Memory CPU model GPUs GPU type GPU memory Misc. features Node list 2 2x32 1006GB INTELR XEONR PLATINUM 8562Y+ 4 NVIDIA L40S 44GB nan node2643-2644 3 2x32 1006GB IntelR XeonR Platinum 8462Y+ 4 NVIDIA H100 80GB HBM3 79GB nan node2640-2642 1 2x32 1007GB INTELR XEONR PLATINUM 8562Y+ 3 NVIDIA L40S 44GB nan node3502 62 2x32 1007GB INTELR XEONR PLATINUM 8562Y+ 4 NVIDIA L40S 44GB nan node2804;node3002-3008;node3202-3208;node3302;node3402-3408;node3500-3501;node3503-3512;node4102-4108;node4200-4212;node4302-4305;node4502-4504 1 2x32 1007GB IntelR XeonR Platinum 8462Y+ 4 NVIDIA H100 80GB HBM3 79GB nan node2906 8 2x32 2014GB IntelR XeonR Platinum 8462Y+ 4 NVIDIA H100 80GB HBM3 79GB nan node1702-1703;node1802-1803;node2702-2703;node2802-2803 12 2x60 2014GB INTELR XEONR PLATINUM 8580 8 NVIDIA H200 140GB nan node2433-2434;node3000-3001;node3100-3101;node3200-3201;node3300-3301;node3400;node4100 4 2x20 502GB IntelR XeonR Silver 4316 CPU @ 2.30GHz 4 NVIDIA A100 80GB PCIe 80GB nan node2414-2417 13 2x64 502GB AMD EPYC 7763 64-Core Processor 4 NVIDIA A100-SXM4-80GB 80GB nan node1917-1918;node2100-2104;node2300-2304;node2319"},{"location":"running-jobs/available-resources/#mit_quicktest","title":"mit_quicktest","text":"Nodes Cores Memory CPU model Misc. features Node list 6 2x96 1510GB AMD EPYC 9654 96-Core Processor nan node1620-1625 20 2x48 376GB AMD EPYC 9474F 48-Core Processor nan node1600-1619"},{"location":"running-jobs/job-arrays/","title":"Job Arrays","text":"<p>You can make use of job arrays if you are planning to run many jobs with different inputs, or a job that iterates over many inputs and is fully independent. We tend to refer to these types of jobs as throughput jobs.</p> <p>Job Arrays allow you to submit many sub-jobs and parameterize the inputs of these jobs. On this page we will refer to these sub-jobs as job array tasks, or tasks for short.</p> <p>When you run a job array the scheduler will set up two environment variables for each sub-job, or task, in the array:</p> <ul> <li><code>SLURM_ARRAY_TASK_ID</code></li> <li><code>SLURM_ARRAY_TASK_COUNT</code></li> </ul> <p>The first is a unique ID assigned to each task. The second is the total number of tasks. With these two numbers you have the information you need to run your tasks concurrently.</p> <p>The best way to run a Job Array is so that each job array task can be assigned a range of work to do. For example, if have 1000 simulations to run each with their own input file, you want to write your code so that each job array task is assigned multiple input files. If you have 4 tasks each would be assigned 250, if you have 8 tasks they'd each be assigned 125 input files, and so on. Running in this way is more efficient for the scheduler, as it doesn't have to manage as many jobs, and it saves you on startup cost, or the time it takes for the scheduler to find resources and start running for each task.</p> <p>On this page we show the basic framework of how to do this, both in a scripting language like Python and in Bash, as well as a few specific examples.</p>"},{"location":"running-jobs/job-arrays/#python-or-julia","title":"Python or Julia","text":"<p>If your code has a big central for loop that iterates over inputs, here is how to run your code in parallel with a Job array. If your code isn't written this way and you can re-write it so it will run in a loop over your entire set of inputs, we recommend that you do that. Python code that iterates over multiple inputs will save extra startup time overall by importing packages once per set of inputs. This may not always be convenient, and if not you can refer to the bash section below.</p> <p>Examples that demonstrate this way of using a job array are available for both Python and Julia. These are also available on Engaging at the path <code>/orcd/examples/001/teaching-examples</code>.</p> <p>You will need to add the following lines to take in two inputs. Make sure both <code>my_task_id</code> and <code>num_tasks</code> are in scope when you run your for loop.</p> PythonJulia <pre><code># Grab the arguments that are passed in\nmy_task_id = int(sys.argv[1])\nnum_tasks = int(sys.argv[2])\n</code></pre> <pre><code># Grab the arguments that are passed in\ntask_id = parse(Int,ARGS[1])\nnum_tasks = parse(Int,ARGS[2])\n</code></pre> <p>This grabs two arguments that we will pass into the script: a task ID and the number of tasks. Next you will take whatever you are iterating over and filter out the elements assigned to the current task (<code>my_task_id</code>):</p> PythonJulia <p><pre><code># Assign indices to this process/task\nmy_arr = arr[my_task_id:len(arr):num_tasks]\n</code></pre> Here we are taking the array of inputs <code>arr</code>, extracting the elements assigned to <code>my_task_id</code> and putting them in <code>my_arr</code>. This splits up the array <code>arr</code> using a cyclic distribution based on <code>my_task_id</code> and <code>num_tasks</code>. For example, if there are 32 tasks, Task 1 will have <code>my_arr</code> 0, 32, 64, 96, ..., Task 2 will have <code>my_array</code> 1, 33, 65, 97, ..., and Task 32 will have <code>my_array</code> 31, 63, 95, and so on. </p> <pre><code>my_arr = arr[task_id+1:num_tasks:length(fnames)]\n</code></pre> <p>Julia Array Indexing</p> <p>Julia arrays are one-based. If we start our job array indexing at 0 we need to add 1 to <code>task_id</code> as shown above.</p> <p>Here we are taking the array of inputs <code>arr</code>, extracting the elements assigned to <code>my_task_id</code> and putting them in <code>my_arr</code>. This splits up the array <code>arr</code> using a cyclic distribution based on <code>my_task_id</code> and <code>num_tasks</code>. For example, if there are 32 tasks, Task 1 will have <code>my_arr</code> 1, 33, 65, 97, ..., Task 2 will have <code>my_array</code> 2, 24, 66 ..., and Task 32 will have <code>my_array</code> 32, 64, 96, and so on. </p> <p>I'll then iterate over <code>my_arr</code> in the for loop instead of <code>arr</code>:</p> PythonJulia <pre><code>for element in my_arr:\n    # do some work\n</code></pre> <pre><code>for element in my_arr\n    # do some work\nend\n</code></pre> <p>The full script will look something like this:</p> PythonJulia iterate_over_arr.py<pre><code>import os, sys\n\n# Replace with your array of inputs\n# This example uses numbers 0-256\narr = range(256)\n\n# Grab the arguments that are passed in\n# This is the task id and number of tasks that can be used\n# to determine which indices this process/task is assigned\nmy_task_id = int(sys.argv[1])\nnum_tasks = int(sys.argv[2])\n\n# Assign indices to this process/task\nmy_arr = arr[my_task_id:len(arr):num_tasks]\n\nfor num in my_arr:\n    # Do something with num\n    # Your code goes here\n</code></pre> iterate_over_arr.jl<pre><code># Replace with your array of inputs\n# This example uses numbers 1-256\narr = 1:256\n\n# Grab the argument that is passed in\n# This is the index into fnames for this process\ntask_id = parse(Int,ARGS[1])\nnum_tasks = parse(Int,ARGS[2])\n\n# Check to see if the index is valid (so the program exits cleanly if the wrong indices are passed)\nfor i in task_id+1:num_tasks:length(arr)\n\n    num = arr[i]\n\n    # Do something with num\n    # Your code goes here\n\nend\n</code></pre> <p>To run this with a Job Array with 4 tasks I would use the following job script:</p> PythonJulia my_job_array.sh<pre><code>#!/bin/bash\n\n#SBATCH -p mit_normal\n#SBATCH -o myjob.log-%A-%a\n#SBATCH -a 0-3\n\n# Load Anaconda Module\nmodule load miniforge\n\necho \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\necho \"Number of Tasks: \" $SLURM_ARRAY_TASK_COUNT\n\npython iterate_over_arr.py $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT\n</code></pre> my_job_array.sh<pre><code>#!/bin/bash\n\n#SBATCH -p mit_normal\n#SBATCH -o myjob.log-%A-%a\n#SBATCH -a 0-3\n\n# Load Anaconda Module\nmodule load julia\n\necho \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\necho \"Number of Tasks: \" $SLURM_ARRAY_TASK_COUNT\n\njulia iterate_over_arr.jl $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT\n</code></pre> <p>The first job flag (<code>-o myjob.log-%A-%a</code>) specifies the output file name, which will be appended with the Array Job ID (<code>%A</code>) and Task ID (<code>%a</code>). The second flag <code>-a 0-3</code> requests a job array with array task indices 0, 1, 2, 3. Here we specify zero-based indices because Python arrays are zero-based. For a one-based language like Matlab/Octave or Julia, we would use indices <code>1-4</code> instead.</p> <p>As mentioned earlier, <code>$SLURM_ARRAY_TASK_ID</code> is a unique ID assigned to each task and <code>$SLURM_ARRAY_TASK_COUNT</code> is the total number of tasks. In the last line of the script we run the python script <code>iterate_over_arr.py</code> and pass both environment variables into the script.</p> <p>The last step is to run the job with <code>sbatch</code>:</p> <pre><code>sbatch my_array_job.sh\n</code></pre> <p>When you run <code>squeue --me</code> you will see which job array tasks are running and which are still pending. Each running job array task will be on its own line, as shown below. Pending tasks will be listed on a single line together. Note the Job IDs have two numbers. The first number is the Job Array ID, a Job ID given to the entire array, the second is the Task ID.</p> <pre><code>        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n62445052_0 mit_norma my_job_a username  R       0:05      1 node2704\n62445052_1 mit_norma my_job_a username  R       0:05      1 node2704\n62445052_2 mit_norma my_job_a username  R       0:05      1 node2704\n62445052_3 mit_norma my_job_a username  R       0:05      1 node2704\n</code></pre>"},{"location":"running-jobs/job-arrays/#bash","title":"Bash","text":"<p>If you can't re-write your code as described in the Python or Julia example above, you can accomplish the same thing in your job script using <code>bash</code>. I will start with the basic framework and then give some examples of some common variations.</p> <p>For simplicity, let's say we have an application <code>my_cmd</code> that takes a number as an input. To run this on a single number we'd start with a job script that looks like this:</p> run_my_cmd_serial.sh<pre><code>#!/bin/bash\n\n#SBATCH -p mit_normal\n#SBATCH -o my_cmd_serial.log-%j\n\n# Set the number to run my_cmd on\nexport MY_NUM=1\n\nmy_cmd $MY_NUM\n</code></pre>"},{"location":"running-jobs/job-arrays/#few-inputs","title":"Few Inputs","text":"<p>If you plan to run this on relatively few numbers, say less than around 100, and <code>my_cmd</code> runs for longer than a few seconds you can use something like this script below. Let's say we want to run <code>my_cmd</code> on numbers 1-32. We can create a job array with 32 tasks each assigned an index 1-32 by adding the flag <code>-a 1-32</code>. Here is the example script</p> few_inputs.sh<pre><code>#!/bin/bash\n\n#SBATCH -p mit_normal\n#SBATCH -o my_cmd_array32.log-%A-%a\n#SBATCH -a 1-32\n\n# Set the number to run my_cmd on\nexport MY_NUM=$SLURM_ARRAY_TASK_ID\n\nmy_cmd $MY_NUM\n</code></pre> <p>Notice we are setting <code>$MY_NUM</code> to <code>$SLURM_ARRAY_TASK_ID</code> and passing it into <code>my_cmd</code>. I've also changed the output job flag (<code>-o my_cmd_array32.log-%A-%a</code>) so it will be appended with the Array Job ID (<code>%A</code>) and Task ID (<code>%a</code>) to the name of the log file.</p> <p>The last step is to run the job with <code>sbatch</code>:</p> <pre><code>sbatch few_inputs.sh\n</code></pre> <p>When you run <code>squeue --me</code> you will see which job array tasks are running and which are still pending. Each running job array task will be on its own line, as shown below. Pending tasks will be listed on a single line together. Note the Job IDs have two numbers. The first number is the Job Array ID, a Job ID given to the entire array, the second is the Task ID.</p> <pre><code>        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n62445052_0 mit_norma my_job_a username  R       0:05      1 node2704\n62445052_1 mit_norma my_job_a username  R       0:05      1 node2704\n62445052_2 mit_norma my_job_a username  R       0:05      1 node2704\n62445052_3 mit_norma my_job_a username  R       0:05      1 node2704\n...\n62445052_32 mit_norma my_job_a username  R       0:05      1 node2704\n</code></pre>"},{"location":"running-jobs/job-arrays/#many-inputs","title":"Many Inputs","text":"<p>The script above works well for smaller numbers of tasks, but doesn't scale well to larger numbers. Because there are limited resources, you end up spending more time waiting for available resources than you do running your application. The scheduler will also slow down when it has to manage very large numbers of jobs, so we limit the number of jobs each user can run on each partition. Here is an approach that allows you to run many inputs with fewer job array tasks.</p> <p>Let's say we want to run <code>my_cmd</code> on the numbers 1-256. To run this as a job array with 32 tasks, for example, we can use the following job script:</p> many_inputs.sh<pre><code>#!/bin/bash\n\n# Scheduler Options\n#SBATCH -p mit_normal\n#SBATCH -o myout.log-%A-%a\n#SBATCH -a 0-31\n\necho \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\necho \"Number of Tasks: \" $SLURM_ARRAY_TASK_COUNT\n\nexport MAX_NUM=256\n\nmy_array=( $(seq $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT $MAX_NUM) )\n\n# Iterate over my_array\nfor IDX in \"${my_array[@]}\"; do\n    my_cmd $IDX\ndone\n</code></pre> <p>Bash Array Indexing</p> <p>Bash arrays are zero-based. To make indexing easier start your job array indices at 0.</p> <p>As mentioned earlier, <code>$SLURM_ARRAY_TASK_ID</code> is a unique ID assigned to each task and <code>$SLURM_ARRAY_TASK_COUNT</code> is the total number of tasks.</p> <p>In line 13:</p> <pre><code>my_array=( $(seq $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT $MAX_NUM) )\n</code></pre> <p>We are creating an array of the numbers we want the current task to pass into <code>my_cmd</code>. This splits up the numbers 1-256 using a cyclic distribution. Since there are 32 tasks, Task 1 will have <code>my_array</code> 1, 33, 65, 97, ..., Task 2 will have <code>my_array</code> 2, 24, 66 ..., and Task 32 will have <code>my_array</code> 32, 64, 96, and so on. </p> <p>In the final few lines we iterate over <code>my_array</code> with a for loop and run <code>my_cmd</code> on each number in <code>my_array</code> in turn:</p> <pre><code># Iterate over my_array\nfor IDX in \"${my_array[@]}\"; do\n    my_cmd $IDX\ndone\n</code></pre> <p>The last step is to run the job with <code>sbatch</code>:</p> <pre><code>sbatch many_inputs.sh\n</code></pre> <p>When you run <code>squeue --me</code> you will see which job array tasks are running and which are still pending. Each running job array task will be on its own line, as shown below. Pending tasks will be listed on a single line together. Note the Job IDs have two numbers. The first number is the Job Array ID, a Job ID given to the entire array, the second is the Task ID.</p> <pre><code>        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n62445052_0 mit_norma my_job_a username  R       0:05      1 node2704\n62445052_1 mit_norma my_job_a username  R       0:05      1 node2704\n62445052_2 mit_norma my_job_a username  R       0:05      1 node2704\n62445052_3 mit_norma my_job_a username  R       0:05      1 node2704\n...\n62445052_32 mit_norma my_job_a username  R       0:05      1 node2704\n</code></pre> <p>Even though we have 256 numbers we are iterating through, we have 32 job array tasks running. Each job array task will be assigned 8 numbers in <code>my_array</code> (256/32 = 8).</p> <p>This is a very minimal example. In many cases you'll need a bit more than this to run your job array. In Job Array Models we show what adjustments to make for some common situations: using inputs from a file and using files as inputs. We recommend using these example scripts below as models for your own job array jobs. Copy the script and make updates as needed.</p>"},{"location":"running-jobs/job-arrays/#job-array-models","title":"Job Array Models","text":""},{"location":"running-jobs/job-arrays/#inputs-from-a-file","title":"Inputs from a File","text":"<p>In this case you will need a plain text file where each line contains one input. This will also work if your code takes multiple command line arguments, list them separated by a space the same way you would at the command line.</p> <p>Here is the example script: </p> inputs_from_file.sh<pre><code>#!/bin/bash\n\n# Scheduler Options\n#SBATCH -p mit_normal\n#SBATCH -o myout.log-%A-%a\n#SBATCH -a 1-4\n\necho \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\necho \"Number of Tasks: \" $SLURM_ARRAY_TASK_COUNT\n\n# Specify Input File\nINPUT_FILE=file.txt\nNUM_LINES=\"$(wc -l &lt; $INPUT_FILE)\"\n\n# Distribute line numbers\nMY_LINE_NUMS=( $(seq $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT $NUM_LINES) )\n\n# Iterate over $MY_LINE_NUMS\nfor LINE_IDX in \"${MY_LINE_NUMS[@]}\"; do\n\n    # Get the $LINE_IDX-th line from $INPUT_FILE\n    INPUT=\"$(sed \"${LINE_IDX}q;d\" $INPUT_FILE)\"\n\n    # Run my_cmd on $INPUT\n    my_cmd $INPUT\ndone\n</code></pre> <p>sed Command Indexing</p> <p>The <code>sed</code> command, which we use to retrieve lines from the input file, is one based. To make indexing easier start your job array indices at 1.</p> <p>Use this Script</p> <p>To use this script specify the name of your input file in line 12 and adjust line 25 to run your application.</p> <p>This example works very similarly to the one above, with a few additions. Lines 12 specifies the name of the file containing the input strings, and line 13 finds the number of lines in the file using the <code>wc</code> or \"word count\" command.</p> <pre><code># Specify Input File\nINPUT_FILE=file.txt\nNUM_LINES=\"$(wc -l &lt; $INPUT_FILE)\"\n</code></pre> <p>Line 22 within loop above uses the <code>sed</code> command to extract the current iteration's line from the file. The <code>sed</code> (Stream Editor) command can be used for many things, including extracting parts of a file as well as replacing or deleting text from a file. The <code>sed</code> command uses one-based indexing (it starts counting at 1 instead of 0), so it is easiest to start job array indices at 1 (see line 6 above).</p> <pre><code># Get the $LINE_IDX-th line from $INPUT_FILE\nINPUT=\"$(sed \"${LINE_IDX}q;d\" $INPUT_FILE)\"\n</code></pre>"},{"location":"running-jobs/job-arrays/#files-as-inputs","title":"Files as Inputs","text":"<p>This example shows how to pass in a directory of files as inputs.</p> inputs_from_file.sh<pre><code>#!/bin/bash\n\n# Scheduler Options\n#SBATCH -p mit_normal\n#SBATCH -o myout.log-%A-%a\n#SBATCH -a 0-3\n\necho \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\necho \"Number of Tasks: \" $SLURM_ARRAY_TASK_COUNT\n\n# Specify Input Directory\nINPUT_DIR=inputs/*\nFILES=(${INPUT_DIR})\nNUM_FILES=${#FILES[@]}\n\n# Distribute files\nMY_FILE_NUMS=( $(seq $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT \"$(($NUM_FILES-1))\") )\n\n# Iterate over $MY_FILE_NUMS\nfor IDX in \"${MY_FILE_NUMS[@]}\"; do\n\n    # Get the $IDX-th file from $FILES\n    INPUT_FILE=${FILES[$IDX]}\n\n    # Run my_cmd on $INPUT_FILE\n    my_cmd $INPUT_FILE\ndone\n</code></pre> <p>Use this Script</p> <p>To use this script specify specify the files that you are using for inputs (be sure to include a wildcard *) in line 12 and adjust line 25 to run your application.</p> <p>In this example a list of files are passed into the <code>my_cmd</code> application. Line 12 specifies which files are the input files. The expansion that happens in line 13 to get the names of the files will only work if you include a wildcard in line 12. To test if you have it right run <code>ls $INPUT_DIR</code>. You should see all the files you expect to pass into your script.</p> <pre><code># Specify Input Directory\nINPUT_DIR=inputs/*\nFILES=(${INPUT_DIR})\n</code></pre> <p>Line 22 gets the file assigned to the current iteration, and line 25 passes that into <code>my_cmd</code></p> <pre><code># Get the $IDX-th line from $INPUT_FILE\nINPUT_FILE=${FILES[$IDX]}\n\n# Run my_cmd on $INPUT_FILE\nmy_cmd $INPUT_FILE\n</code></pre>"},{"location":"running-jobs/overview/","title":"Job Scheduler Overview","text":"<p>To run something on an HPC cluster, like Engaging, you will request resources for your application using a piece of software called the scheduler. The scheduler that Engaging uses is Slurm (you'll see and hear \"scheduler\" and \"Slurm\" used interchangeably). It is the scheduler's responsibility to allocate the resources that satisfy your request and those of everyone else using the system. This temporary allocation of resources is called a job. As with all software, the scheduler uses a specific syntax for requesting resources. This section describes how to work with the scheduler and how to run jobs efficiently.</p>"},{"location":"running-jobs/overview/#partitions","title":"Partitions","text":"<p>Engaging is a large heterogenous cluster, meaning there are many different types of nodes with different configurations. Some nodes are freely available for anyone at MIT to use, and some have been purchased by labs or departments for priority use by their group. Some nodes are meant for specific types of workloads. Nodes are grouped together in partitions, which designate who can access them and what they should be used for. Different partitions may have different sets of rules about how many resources you can use and how long your jobs can run on them.</p> <p>To see which partitions you have access to, run the <code>sinfo</code> command: </p> <pre><code>sinfo\nPARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST\nmit_normal      up   12:00:00      2   resv node[2704-2705]\nmit_normal      up   12:00:00     30   idle node[1600-1625,1706-1707,1806-1807]\nmit_normal_gpu  up   12:00:00      1    mix node2906\nmit_normal_gpu  up   12:00:00      5   idle node[1706-1707,1806-1807,2804]\nmit_quicktest   up      15:00     26   idle node[1600-1625]\nmit_preemptable up 2-00:00:00      1    mix node2906\nmit_preemptable up 2-00:00:00     27   idle node[1600-1625,2804]\n</code></pre> <p>The <code>sinfo</code> command will tell you the names of the partitions you have access, what their time limits are, how many nodes are in each state (see Checking Available Resources below), and the names of the nodes in the partitions.</p> <p>The standard partitions that the full MIT community has access to are:</p> Partition Name Purpose Hardware Type(s) Max Time Limit Resource Limit <code>mit_normal</code> Longer running batch and interactive jobs that do not need a GPU CPU only 12 hours 96 cores <code>mit_normal_gpu</code> Batch and interactive jobs that need a GPU GPUs (L40S, H100, H200) 6 hours 2 GPUs, 32 cores <code>mit_quicktest</code> Short batch and interactive jobs, meant for testing CPU only 15 minutes 48 cores <code>mit_preemptable</code> Low-priority preemtable jobs- jobs that may be stopped by another job with higher priority CPU-only, GPUs (A100, L40S, H100, H200) 48 hours 1024 cores, 4 GPUs <p>To see a summary of the nodes in each of these partitions, take a look at our Available Resources page.</p> <p>Older Partitions</p> <p>There are a few additional partitions that contain older nodes. These nodes run on a different operating system (Centos 7) than the ones above and therefore have a different software stack. Software built or installed on Rocky 8 or newer nodes will most likely not work on these older nodes. These partitions include <code>sched_mit_hill</code>, <code>newnodes</code>, <code>sched_any</code>, <code>sched_engaging_default</code>, and <code>sched_quicktest</code>.</p> <p>If you are part of a group that has purchased nodes you may see additional partitions. They will be named based on your PI's Kerberos or your group's name, depending on who purchased the nodes.</p>"},{"location":"running-jobs/overview/#preemptable-jobs","title":"Preemptable Jobs","text":"<p>We provide the <code>mit_preemptable</code> partition so that nodes owned by a group or PI can be used by researchers outside that group when those nodes are idle. When someone from the group that owns the node runs a job on their partition, the scheduler will stop, or preempt, any job that is running on the lower-priority <code>mit_preemptable</code> partition. Jobs running on <code>mit_preemptable</code> should be checkpointed so that they don't lose their progress when the job is stopped.</p> <p>To make your batch jobs requeue if they are stopped on <code>mit_preemptable</code>, add the <code>--requeue</code> flag to your job submission. If your job has a recent checkpoint it should pick up where it left off once additional resources are available. You can read more about job flags below.</p>"},{"location":"running-jobs/overview/#checking-available-resources","title":"Checking Available Resources","text":"<p>To see what resources are available run <code>sinfo</code>. The <code>sinfo</code> command will show how many nodes are in each state. Nodes in \"idle\" state have all cores available, nodes in \"mix\" state have some cores available, and nodes in \"alloc\" state have no cores or other resources available.</p> <pre><code>sinfo -p mit_normal\nPARTITION  AVAIL  TIMELIMIT  NODES  STATE  NODELIST\nmit_normal    up   12:00:00      2   resv  node[2704-2705]\nmit_normal    up   12:00:00      1   mix   node1707\nmit_normal    up   12:00:00      1   alloc node1708\nmit_normal    up   12:00:00     29   idle  node[1600-1625,1706,1806-1807]\n</code></pre> <p>Common node states are:</p> <ul> <li><code>idle</code>: nodes that are fully available</li> <li><code>mix</code>: nodes that have some, but not all, resources allocated</li> <li><code>alloc</code>: nodes that are fully allocated</li> <li><code>resv</code>: nodes that are reserved and only available to people in their reservation</li> <li><code>drained</code>: nodes that are unavailable for use per system administrator request, usually for maintenance purposes. It is shown as <code>drain*</code> due to the default 5-column limit.</li> <li><code>drng</code>: nodes that are currently allocated a job, but will not be allocated additional jobs. The node state will be changed to state drained when the last job on it completes. </li> <li><code>down</code>: nodes that are unavailable for use. </li> <li><code>plnd</code>: nodes that are planned by the backfill scheduler for a higher priority job</li> </ul> <p>You can also use <code>sinfo</code> to see what resources each node has. The output can be quite long so we recommend limiting to a specific partition(s). For example, to see what node types are in the <code>mit_normal</code> and <code>mit_normal_gpu</code> partitions, run the command:</p> <pre><code>sinfo -p mit_normal,mit_normal_gpu -O Partition,Nodes,CPUs,Memory,Gres -e\n</code></pre> <p>In the output you'll see a summary of how many nodes of each configuration is in the partition. You can include multiple partitions by providing a comma separated list to the <code>-p</code> flag. The output shows the partition, number of nodes, number of CPUs, amount of Memory (in MB), and any GPUs available on the node:</p> <pre><code>PARTITION           NODES               CPUS                MEMORY              GRES                \nmit_normal          32                  96                  386000              (null)              \nmit_normal          2                   64                  386000              (null)              \nmit_normal          6                   192                 1547000             (null)              \nmit_normal          12                  96                  386223              (null)              \nmit_normal_gpu      1                   64                  1031314             gpu:h100:4(S:0-1)   \nmit_normal_gpu      8                   120                 2063213             gpu:h200:8(S:0-1)   \nmit_normal_gpu      49                  64                  1031314             gpu:l40s:4(S:0-1)\n</code></pre>"},{"location":"running-jobs/overview/#running-jobs","title":"Running Jobs","text":"<p>How you run your job depends on the type of job you would like to run. There are two \"modes\" for running jobs: interactive and batch jobs. Interactive jobs allow you to run interactively on a compute node in a shell. Batch jobs, on the other hand, are for running a pre-written script or executable. Interactive jobs are mainly used for testing, debugging, and interactive data analysis. Batch jobs are the traditional jobs you see on an HPC system and should be used when you want to run a script that doesn't require that you interact with it.</p>"},{"location":"running-jobs/overview/#job-flags","title":"Job Flags","text":"<p>When you start any type of job you specify what resources you need for your job, including cores, memory, GPUs, and other features. You also specify which partition you would like your job to run on. If you don't specify any of these you will get the default resources: 1 core, a small amount of memory, no GPUs, and it will run on the current default partition. See the page on Requesting Resources for the flags to use to request different types of resources.</p>"},{"location":"running-jobs/overview/#interactive-jobs","title":"Interactive Jobs","text":"<p>The basic command for requesting an interactive job on the <code>mit_normal</code> partition is:</p> <pre><code>salloc -p mit_normal\n</code></pre> <p>The <code>-p mit_normal</code> is a flag that is passed to the scheduler, <code>-p</code> specifies the partition. This command will allocate 1 core on a node in the <code>mit_normal</code> partition. For example:</p> <pre><code>[user01@orcd-login001 ~]$ salloc -p mit_normal\nsalloc: Pending job allocation 60159437\nsalloc: job 60159437 queued and waiting for resources\nsalloc: job 60159437 has been allocated resources\nsalloc: Granted job allocation 60159437\nsalloc: Waiting for resource configuration\nsalloc: Nodes node1806 are ready for job\n[user01@node1806 ~]$ \n</code></pre> <p>Notice how the command prompt changes from <code>[user01@orcd-login001 ~]$</code> to <code>[user01@node1806 ~]$</code>. This indicates that <code>user01</code> has started an interactive job on <code>node1806</code> and any commands issued will run on this node.</p>"},{"location":"running-jobs/overview/#batch-jobs","title":"Batch Jobs","text":"<p>Batch jobs are used to run pre-written scripts or run commands that do not need input from you throughout the run. The first step to running a batch job is to write a job script. Job scripts can be \"launched\" with the <code>sbatch</code> command:</p> <pre><code>sbatch myscript.sh\n</code></pre> <p>When you run this command the scheduler will look for the resources requested in the script, allocate those resources to your job, run your script on those resources, and then release those resources once your script completes or the time limit is reached.</p> <p>Here is an example job script:</p> <pre><code>#!/bin/bash\n\n# Job Flags\n#SBATCH -p mit_normal\n\n# Set up environment\nmodule load miniforge\n\n# Run your application\npython myscript.py\n</code></pre> <p>This script requests the same resources as the interactive job above: 1 cpu core on the <code>mit_normal</code> partition. The <code>#SBATCH -p mit_normal</code> may look like a comment but it is not, it is a directive to the scheduler to run with the specified flags. Note that this is the same flag used in the interactive job example above. It then sets up the job environment to use python with the <code>miniforge</code> module, and then runs a python script. In general, the same steps and commands you would use to run your job in an interactive job you can put in your job script.</p> <p>You can think of job scripts as having three sections:</p> <ol> <li>Scheduler/Job flags: This is where you request your resources using Slurm flags.</li> <li>Set up your environment: Load any modules you need, set environment variables, etc. It is better to set this in your job scripts to ensure consistent environments across jobs. We don't recommend putting these commands in your <code>.bashrc</code> or running them at the command line before you launch your job.</li> <li>Run your code or application as you would from the command line.</li> </ol>"},{"location":"running-jobs/overview/#checking-job-status","title":"Checking Job Status","text":"<p>To see all your currently running and pending jobs run the <code>squeue --me</code> command: </p> <pre><code>squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          60735877 mit_norma interact username  R      29:32      1 node1707\n</code></pre> <p>each line in the output is a different job. The default fields are:</p> <ul> <li><code>JOBID</code>: The Job ID, each job has a unique ID that can be used to identify it.</li> <li><code>PARTITION</code>: The partition the job is running in</li> <li><code>NAME</code>: The name of the job. By default this is the name of your job script, or \"interactive\" for interactive jobs.</li> <li><code>USER</code>: Your username</li> <li><code>ST</code>: The job status. Common statuses are <code>R</code> for \"Running\", <code>PD</code> for \"Pending\", and <code>CG</code> for completing. Jobs in pending state have the reason the job is pending listed in the final <code>NODELIST(REASON)</code> column.</li> <li><code>TIME</code>: The amount of time the job has been running for.</li> <li><code>NODES</code>: The number of nodes your job is using.</li> <li><code>NODELIST(REASON)</code>: If your job is running this is the name of the node or nodes that your job is running on. If your job is pending this lists the reason the job is pending. Common pending reasons are \"Resources\", meaning the resources you've requested aren't yet available, \"Priority\", meaning there is someone else ahead of you in line to use the resources you've requested. Other reasons are listed in the Slurm documentation.</li> </ul>"},{"location":"running-jobs/overview/#stopping-jobs","title":"Stopping Jobs","text":"<p>You can stop running or pending jobs with the <code>scancel</code> command. You can stop an individual job by providing a job id:</p> <pre><code>scancel 123456\n</code></pre> <p>or a list of jobs by separating job IDs with a comma:</p> <pre><code>scancel 123456,123457\n</code></pre> <p>You can also stop all of your jobs by providing your username:</p> <pre><code>scancel -u USERNAME\n</code></pre> <p>where <code>USERNAME</code> is your username.</p>"},{"location":"running-jobs/overview/#retrieving-job-stats","title":"Retrieving Job Stats","text":"<p>You can get information above currently and previously run jobs with the <code>sacct</code> command. This command can be very helpful for troubleshooting job issues, and is particularly helpful to check that you are getting the resources you expect allocated to your job.</p> <p>Running <code>sacct</code> with no flags will show some basic information about the jobs that you've run in the past day:</p> <pre><code>[USERNAME@orcd-login001 ~]$ sacct\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n60764362     interacti+ sched_any+ mit_gener+          1 CANCELLED+      0:0 \n60764363     interacti+ mit_normal mit_gener+          1  COMPLETED      0:0 \n60764363.in+ interacti+            mit_gener+          1  COMPLETED      0:0 \n60764363.ex+     extern            mit_gener+          1  COMPLETED      0:0 \n60764366     interacti+ mit_normal mit_gener+          1    RUNNING      0:0 \n60764366.in+ interacti+            mit_gener+          1    RUNNING      0:0 \n60764366.ex+     extern            mit_gener+          1    RUNNING      0:0 \n</code></pre> <p>You can select which fields are shown with the <code>-o</code> (output) flag, for example:</p> <pre><code>[USERNAME@orcd-login001 ~]$ sacct -o JobID,JobName,AllocCPUs,NodeList,Elapsed,State\nJobID           JobName  AllocCPUS        NodeList    Elapsed      State \n------------ ---------- ---------- --------------- ---------- ---------- \n60764362     interacti+          1   None assigned   00:00:00 CANCELLED+ \n60764363     interacti+          1        node1806   00:00:19  COMPLETED \n60764363.in+ interacti+          1        node1806   00:00:19  COMPLETED \n60764363.ex+     extern          1        node1806   00:00:19  COMPLETED \n60764366     interacti+          1        node1806   00:01:40  COMPLETED \n60764366.in+ interacti+          1        node1806   00:01:39  COMPLETED \n60764366.ex+     extern          1        node1806   00:01:40  COMPLETED \n</code></pre> <p>There are many fields that give a lot of information about your jobs. Running the <code>sacct -e</code> command will show all of them. A few that can be helpful are:</p> <ul> <li><code>AllocCPUs</code>, <code>AllocNodes</code>- Number of CPUs, number of nodes, or GPUs allocated to your job.</li> <li><code>ReqTRES</code>- Requested \"trackable resources\" (TRES). This is a fairly long string that will include any GPUs requested for the job. If the string is too long to display add <code>%60</code> to show 60 characters (<code>ReqTRES%60</code>), you can adjust the number of characters as needed.</li> <li><code>NodeList</code>- The list of nodes that your job ran on. This is helpful to see whether your jobs are consistently failing on the same node. If so, reach out to orcd-help-engaging@mit.edu and let us know which node seems to be having an issue.</li> <li><code>Start</code>, <code>End</code>, <code>Elapsed</code>- The start time, end time, and total amount of time your job ran for.</li> <li><code>State</code>, <code>ExitCode</code>- The Job State and Exit Code. If the job failed there may be an exit code that can help determine why the job failed.</li> <li><code>MaxRSS</code>- The peak memory utilization of your job. This number can be used to fine-tune how much memory to request for your job. See the section on requesting memory for more information.</li> </ul> <p>You can specify a particular job with the <code>-j</code> flag (<code>sacct -j 60764366</code> for example). You can also specify a start and end time if you are interested in seeing jobs that ran more than a day ago. Use the <code>--starttime</code> and <code>--endtime</code> flags to specify a date range, using the format <code>MMDDYY</code>, for example <code>sacct --starttime=010125 --endtime=010225</code>retrieves all your jobs that ran on the first two days of 2025.</p> <p>See the Slurm sacct documentation for more information on flags and fields.</p>"},{"location":"running-jobs/requesting-resources/","title":"Running Jobs","text":"<p>On the Job Scheduler Overview page we described how to run both batch and interactive jobs. This page describes how to request resources for your job.</p>"},{"location":"running-jobs/requesting-resources/#cores-for-multicore-and-shared-memory-jobs","title":"Cores for Multicore and Shared Memory Jobs","text":"<p>For any kind of multithreading, multiprocessing, shared memory, OpenMP, or similar jobs you can use the <code>-c</code> or <code>--cpus-per-task</code> flag to request additional cores.</p> <p>An example batch script that requests 4 cores on the mit_normal partition would be:</p> <pre><code>#!/bin/bash\n\n# Job Flags\n#SBATCH -p mit_normal\n#SBATCH -c 4\n\n# Set up environment\nmodule load miniforge\n\n# Run your application\npython myscript.py\n</code></pre>"},{"location":"running-jobs/requesting-resources/#mpi-and-distributed-jobs","title":"MPI and Distributed Jobs","text":"<p>There are a few flags to be aware of for running distributed and MPI jobs. These types of jobs can run across multiple nodes, so there are a few flags that can control how these tasks are spread out.</p> <ul> <li><code>-n</code> or <code>--ntasks</code>: This flag is used to specify the total number of distributed or MPI processes or ranks that your application uses. \"Task\" is roughly the term that Slurm uses for a process. By default you get <code>ntasks</code> number of cores for your job, one per task.</li> <li><code>-N</code> or <code>--nodes</code>: This is the number of nodes your tasks are spread across. If you don't specify <code>ntasks</code> or <code>ntasks-per-node</code> you will get one per node. If you specify <code>ntasks</code> they may be split roughly evenly across <code>nodes</code>, but not necessarily. It is generally more efficient for processes on the same node to communicate, so in practice you often want to avoid spreading your tasks across more nodes than necessary. </li> <li><code>--ntasks-per-node</code>: Use with <code>--nodes</code>, this specifies how many tasks should be assigned to each node.</li> </ul> <p>At minimum you'll need to use the <code>-n</code> or <code>--ntasks</code> flag. This job here runs an MPI job with 4 processes:</p> <pre><code>#!/bin/bash\n\n# Job Flags\n#SBATCH -p mit_normal\n#SBATCH -n 4\n\n# Set up environment\nmodule load openmpi\n\n# Run your application\nmpirun my_program\n</code></pre> <p>Note that <code>mpirun</code> does not necessarily need to be told how many how many processes to start, it can get this information from the scheduler.</p> <p>When your MPI job gets too big to fit on a single node, or you want to have more control over how your processes are spread out, use the <code>--nodes</code> and <code>--ntasks-per-node</code>. This script runs an MPI job that requests 2 nodes and 64 tasks per node, for a total of 128 MPI processes.</p> <pre><code>#!/bin/bash\n\n# Job Flags\n#SBATCH -p mit_normal\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=64\n\n# Set up environment\nmodule load openmpi\n\n# Run your application\nmpirun my_program\n</code></pre> <p>The following will still allocate 128 MPI processes across 2 nodes, but they may not necessarily be evenly split between the two nodes.</p> <pre><code>#!/bin/bash\n\n# Job Flags\n#SBATCH -p mit_normal\n#SBATCH --nodes=2\n#SBATCH --ntasks=128\n\n# Set up environment\nmodule load openmpi\n\n# Run your application\nmpirun my_program\n</code></pre>"},{"location":"running-jobs/requesting-resources/#memory","title":"Memory","text":"<p>Each job is allocated some default amount of memory, the amount depending on the node. If you find that you are running out of memory you can request more with the <code>--mem</code> or <code>--mem-per-core</code> flags.</p> <p>Here is an example of a job multicore job that requests 4 cores and 16GB of RAM:</p> <pre><code>#!/bin/bash\n\n# Job Flags\n#SBATCH -p mit_normal\n#SBATCH -c 4\n#SBATCH --mem=16G\n\n# Set up environment\nmodule load miniforge\n\n# Run your application\npython myscript.py\n</code></pre> <p>How do you know how much memory your job needs? You can find out how much memory a job used after the job is completed. First, run your job with your best estimate for how much memory you need (you can overestimate), and run the job long enough to get an idea of the memory requirement. If the job fails run the job again requesting more memory. Then you can use the <code>sacct</code> slurm command to get the memory used:</p> <pre><code>sacct -j JOBID -o JobID,JobName,State,ReqMem,MaxRSS --units=G\n</code></pre> <p>where <code>JOBID</code> is your job ID. <code>State</code> shows the job status, keep in mind that the amount of memory reported by <code>sacct</code> is only accurate for jobs that are no longer running, and <code>ReqMem</code> is the number of memory that was allocated to the job. <code>MaxRSS</code> is the maximum resident memory (maximum memory footprint) used by each job.</p> <p>If the <code>MaxRSS</code> value is larger than what you have requested, or you run out of memory, you will have to request additional memory for your job.</p>"},{"location":"running-jobs/requesting-resources/#gpus","title":"GPUs","text":"<p>GPUs are available through the <code>mit_normal_gpu</code> partition. You can also run preemptable GPU jobs on the <code>mit_preemptable</code> partition.</p> <p>To request a GPU use the <code>--gres</code> flag in the following way:</p> <pre><code>--gres=gpu:[GPU_TYPE]:[#GPUS]\n</code></pre> <p>For example, to request 1 L40S GPU, use the flag <code>--gres=gpu:l40s:1</code>, or to request 2 H100 GPUs use the flag <code>--gres=gpu:h200:2</code>. If you don't request a GPU type you will be allocated an L40S GPU.</p> <p>The <code>--gres</code> flag is applied per node, so for a multi-node (distributed) GPU job use the number of GPUs you need per node rather than the total number of GPUs.</p> <p>You can see how many GPUs of which type are on each node in a partition using the <code>sinfo</code> command. For example, to check <code>mit_normal_gpu</code> run the command:</p> <pre><code>sinfo -p mit_normal_gpu -O Partition,Nodes,CPUs,Memory,Gres\n</code></pre>"},{"location":"software/R/","title":"R","text":"<p>R is a powerful programming language widely used for statistical computing and graphics. It provides a variety of statistical techniques and graphical tools, making it a useful tool for data analysis and visualization.</p>","tags":["Software","R"]},{"location":"software/R/#r-with-conda","title":"R with Conda","text":"<p>Conda is a package manager commonly used for Python, but is compatible with R and can be very useful for installing packages. This can be helpful when the packages you need have specific dependency requirements. Because of these benefits, Conda is our recommended process for using R on the cluster. You can find more information on Conda in the Python software section.</p> <p>When you create a Conda environment, you can specify exactly the packages you need. First, you'll need to load a pre-installed Conda module. There are multiple available, but we recommend Miniforge:</p> EngagingSuperCloud <pre><code>module load miniforge/24.3.0-0\n</code></pre> <pre><code>module load anaconda/2023b\n</code></pre> <p>More information on running R on SuperCloud can be found here.</p> <p>Now, you should be able to run <code>conda</code> commands, such as creating and activating an environment:</p> <pre><code>conda create -n my_R_environment\nconda activate my_R_environment\n</code></pre> <p>To search for specific R packages (beginning with \"r-\"), you can use <code>conda search</code>. For example, the following looks for all versions of Tidyverse available through Conda:</p> <pre><code>conda search r-tidyverse\n</code></pre> <p>The base R installation through Conda is called <code>r-base</code>. This will automatically be downloaded and installed when you install any R package to your environment. To install packages, use <code>conda install</code>:</p> <pre><code>conda install r-tidyverse\n</code></pre> <p>By default, the latest compatible versions of <code>r-base</code> and other R packages are automatically installed. If you prefer different versions of R (<code>r-base</code>) or you need a specific version of a package, you can specify in your <code>install</code> command:</p> <pre><code>conda install r-base=4.1.2\nconda install r-tidyverse=2.0.0\n</code></pre> <p>Note</p> <p>It's much more efficient to specify all the packages you need when you first create your environment rather than installing them one by one. This way, the environment only needs to be solved once, and Conda ensures that packages are compatible with each other. You can do this by naming the packages in the <code>create</code> command: <code>conda create -n my_R_env r-tidyverse r-pillar</code></p> <p>Once your environment is created and activated, entering <code>which R</code> should direct you to the version of R within your Conda environment.</p>","tags":["Software","R"]},{"location":"software/R/#pre-installed-r-modules","title":"Pre-Installed R Modules","text":"<p>There are currently a few different versions of R installed on our systems. You can find these versions by running <code>module avail</code>. To use an R interactive environment, first load an R module, then enter <code>R</code>.</p> Engaging <p>On Rocky8 nodes (orcd-login001, orcd-login002, orcd-login003, orcd-login004):</p> <pre><code>module load r/4.2.2-x86_64\nR\n</code></pre> <p>On Centos7 nodes (orcd-vlogin001, orcd-vlogin002, orcd-vlogin003, orcd-vlogin004):</p> <pre><code>module load R/4.2.2\nR\n</code></pre>","tags":["Software","R"]},{"location":"software/R/#installing-packages","title":"Installing Packages","text":"<p>The pre-installed R modules come with a limited number of packages, but it is possible to install more. This can be acheived using the standard R command <code>install.packages(\"packageName\")</code>.</p> <p>R will first try to install this package system-wide but will be blocked to avoid editing the module for the entire cluster. You will be asked if you want to use a personal library instead. Type \"yes\" and press enter. This creates a folder in your home directory that contains the packages for each version of R you use. You can check this directory by running <code>.libPaths()</code> from the R CLI.</p>","tags":["Software","R"]},{"location":"software/R/#rstudio","title":"RStudio","text":"<p>You can use RStudio on the Engaging cluster via Engaging OnDemand &gt; Interactive Apps &gt; RStudio Server. From there, select the specifications you need, including runtime, memory, and R version.</p>","tags":["Software","R"]},{"location":"software/R/#using-the-rstudio-module-on-engaging","title":"Using the RStudio Module on Engaging","text":"<p>Currently, OnDemand does not support local installations of R or versions of R installed through Conda. This can make it difficult to install certain packages. While we plan on updating this soon, we have a current workaround that takes a bit of setup but makes packages much easier to install.</p> <p>First, you will need to start a job on Engaging. This can be an interactive job or a batch job, but we will use an interactive job in the current example:</p> <pre><code>salloc -N 1 -n 4 --mem-per-cpu=4G -p mit_normal\n</code></pre> <p>Be sure to edit the flags as necessary to request the resources you need, such as CPU cores, memory, or GPUs. See Requesting Resources for more information.</p> <p>Next, load the <code>rstudio/4.4.2</code> module and run the <code>rstudio</code> command:</p> <pre><code>module load rstudio/4.4.2\nrstudio\n</code></pre> <p>You should now see printed instructions for setting up port forwarding to interact with RStudio using your web browser. If using a batch session, these instructions will appear in your job output file.</p>","tags":["Software","R"]},{"location":"software/R/#jupyter","title":"Jupyter","text":"<p>Similar to RStudio, Jupyter notebooks offer a handy cell-based interface to run R code. You can run R on Jupyter notebooks through the Engaging web portal.</p> Engaging <p>Jupyter notebooks are available through Engaging OnDemand &gt; Interactive Apps &gt; Jupyter Notebook. To run R, you must create a Conda environment with both <code>r-irkernel</code> and <code>jupyterlab</code> installed (see R with Conda and Jupyter). When starting up the notebook, enter the name of your custom Conda environment. Once you launch the session and open your notebook, you may need to change your kernel to R. Your current kernel is shown in the top right, and likely defaults to \"Python 3 (ipykernel)\". Click this to change it to R.</p>","tags":["Software","R"]},{"location":"software/R/#faqs","title":"FAQs","text":"<p>I am trying to use a specific R installation, but it is not being recognized. What should I do?</p> <p>Sometimes, the way your environment is set up may cause the system to default to certain R installations that you don't want. The culprit can often be found in your <code>.bashrc</code> and/or <code>.bash_profile</code> file. Usually, running <code>module purge</code> from the command line before loading the version of R you want solves the problem.</p> <p>How do I change the path where my libraries are installed?</p> <p>Before starting R, you can set the <code>R_LIBS_USER</code> environment variable from your Bash terminal:</p> <pre><code>export R_LIBS_USER=/path/to/R/library/directory\n</code></pre> <p>You can also set the path from within R:</p> <pre><code>.libPaths(\"/path/to/R/library/directory\")\n</code></pre> <p>Both of these commands essentially prepend your custom path to the library path that already existed.</p>","tags":["Software","R"]},{"location":"software/R/#further-resources","title":"Further Resources","text":"<p>SuperCloud: Software and Package Management - R Libraries</p>","tags":["Software","R"]},{"location":"software/apptainer/","title":"Singularity and Apptainer","text":"<p>Containers provide an isolated environment that supports user applications. In many cases, it is helpful to use a container to obtain the right environment for your applications on HPC clusters, so as to avoid installing too many dependencies.</p> <p>Containers have great portability and mobility, that says, it is convenient to migrate your applications bewtween different platforms, such as laptops/desktops, cloud platforms and HPC clusters. </p> <p>The most well known container is Docker, which is designed for laptops/desktops and cloud platforms. On ORCD clusters, we use Apptainer and Singularity instead, which are particularly designed for high-perfromance computing. Apptainer is extended from Singularity. Both are compatible with Docker. </p> <p>Note</p> <p>In the following, the terminology Singularity will be used in most cases. The statements hold if the terminologies Singularity and Apptainer are switched. </p> <p>Users can use Singularity to support many applications, such as Python, R, C/Fortran packages, and many GUI software. In particular, containers are popular in supporting Python pakcages for the artificial intelligence (AI) and data science communities, such as Pytorch, Tensorflow, and many others. The Ubuntu operating system (OS) is widely used in the AI community and it is convinient to install many AI appications in Ubuntu environment. Users can use Singularity to obtain Ubuntu OS other than Rocky 8 OS on the host cluster.</p> <p>In this document, we will focus on how to use Singularity on ORCD clusters. First, many applications are well-supported in existing Docker images. Search for an image on the internet, in which your target applicaiton has already been installed by some developers, then download the image and use it directly. If there is no suitable image for your target application, you can build an image to support it.</p> <p>Note</p> <p>An image is a file to support container. Users can launch a containter based on an image.</p>","tags":["Engaging","OpenMind","Apptainer","Singularity","Software"]},{"location":"software/apptainer/#run-applications-with-singularity","title":"Run applications with Singularity","text":"<p>Let us start with running an application with Singularity on the cluster first. </p>","tags":["Engaging","OpenMind","Apptainer","Singularity","Software"]},{"location":"software/apptainer/#preparations","title":"Preparations","text":"EngagingOpenMind <p>Log in to a head node,  <pre><code>ssh &lt;user&gt;@orcd-login002.mit.edu\n</code></pre>  Check available Apptainer versions in modules,  <pre><code>module av apptainer\n</code></pre>  Load an Apptainer module and its dependency, for example,   <pre><code>module load apptainer/1.1.9\n</code></pre></p> <p>Log in to the head node,  <pre><code>ssh &lt;user&gt;@openmind7.mit.edu\n</code></pre>  As a certain amount of computing resources are required to run Singularity, always start with getting an  interactive session on a compute node,  <pre><code>srun -t 60 --constraint=rocky8 -c 4 --mem=10G --pty bash\n</code></pre>  The <code>constraint=rocky8</code> is to request a node with the Rocky 8 OS. </p> <p>Check available Apptainer versions in modules,  <pre><code>module av openmind8/apptainer\n</code></pre>  Load an Apptainer module, for example,   <pre><code>module load openmind8/apptainer/1.1.7\n</code></pre></p> <p>Note</p> <p>Apptainer modules support both apptainer and singularity commands.</p>","tags":["Engaging","OpenMind","Apptainer","Singularity","Software"]},{"location":"software/apptainer/#download-an-image","title":"Download an image","text":"<p>Search for an image that provides your target application, for exmaple on Docker Hub. Here is an example for downloading a Docker image to support Pytorch, <pre><code>singularity pull my-image.sif docker://bitnami/pytorch:latest\n</code></pre> The <code>my-image.sif</code> is the name of the image. You can name it as you like. </p> <p>Note</p> <p>In Apptainer, the command <code>singularity</code> is a soft link to an executable named <code>apptainer</code>, so all <code>singularity</code> commands on this page can be replaced by the <code>apptainer</code> command. They work the same. </p>","tags":["Engaging","OpenMind","Apptainer","Singularity","Software"]},{"location":"software/apptainer/#run-a-program-interactively","title":"Run a program interactively","text":"<p>When the image is ready, launch a container based on the image and then run your application in the container. If you want to work interactively to test and debug codes, it is convineient to log in the containe shell, for exmaple,  <pre><code>$ singularity shell my-image.sif \nApptainer&gt; python\nPython 3.11.9 (main, May 13 2024, 16:49:42) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; # Run your programs here.\n</code></pre></p> <p>Alternatively, execute a command in the container to run your programs,  <pre><code>singularity exec my-image.sif python my-code.py\n</code></pre></p> <p>Use the full path to the image file if it is not in the current directory. </p> <p>The <code>python</code> here is installed in the container and has nothing to do with the <code>python</code> or <code>anaconda</code> modules that have been installed on the host. As the python environment in the container provides all pacakges you need, you don't need to install any python packages and their dependecies. Now you can see the advantage of using a conatainer. </p>","tags":["Engaging","OpenMind","Apptainer","Singularity","Software"]},{"location":"software/apptainer/#submit-a-batch-job","title":"Submit a batch job","text":"<p>When the tests are completed, you can submit a batch job to run your program in the background. Here is a typical batch job script (e.g. named <code>job.sh</code>).</p> EngagingOpenMind <pre><code>#!/bin/bash                      \n#SBATCH -t 01:30:00                  # walltime = 1 hours and 30 minutes\n#SBATCH -N 1                         # one node\n#SBATCH -n 2                         # two CPU cores\n#SBATCH -p mit_normal     # a partition with Rocky 8 nodes\n\nmodule load apptainer/1.1.9   # load modules\nsingularity exec my-image.sif python my-code.py   # Run the program \n</code></pre> <pre><code>#!/bin/bash                      \n#SBATCH -t 01:30:00                  # walltime = 1 hours and 30 minutes\n#SBATCH -N 1                         # one node\n#SBATCH -n 2                         # two CPU cores\n#SBATCH --constraint=rocky8          # nodes with Rocky 8 OS\n\nmodule load openmind8/apptainer/1.1.7      # load an apptainer module\nsingularity exec my-image.sif python my-code.py   # Run the program \n</code></pre> <p>The last line is a command to run a Python program uisng Singularity.  </p> <p>Submit the job script using <code>sbatch</code>, <pre><code>sbatch job.sh\n</code></pre></p>","tags":["Engaging","OpenMind","Apptainer","Singularity","Software"]},{"location":"software/apptainer/#more-on-using-singularity","title":"More on using Singularity","text":"<p>In many cases, GPUs are needed to accelerate programs. As the GPU driver is installed on the host, use the flag <code>--nv</code> to pass necessary GPU driver libraries into the container, so that the program can \"see\" the GPUs in the container. </p> <p>Check if GPUs are available in a container, <pre><code>singularity exec --nv my-image.sif nvidia-smi\n</code></pre></p> <p>Here is an exmaple to run Python programs on GPUs. <pre><code>singularity exec --nv my-image.sif python my-code.py  \n</code></pre></p> <p>By default, the home directory and the <code>/tmp</code> directory are bound to the container. If your programs read/write data files in other directories (e.g. <code>/path/to/data</code>), bind them to the container using the flag <code>-B</code>, <pre><code>singularity exec -B /path/to/data my-image.sif python my-code.py  \n</code></pre></p> <p>In summary, a commonly used syntax to run a program with Singularity is the following, <pre><code>singularity exec [--nv] [-B &lt;path-to-data&gt;] &lt;image-name&gt; &lt;executable-name&gt; [source-code-name]\n</code></pre> The terms in <code>&lt;&gt;</code> are must-needed, while the terms in <code>[]</code> are optional, depending on use cases. </p> EngagingOpenMind <p>Here is an example job script to run a python program with a GPU and data files saved in <code>/nobakcup1</code> or <code>/pool001</code> directories,  <pre><code>#!/bin/bash                      \n#SBATCH -t 01:30:00         # walltime = 1 hours and 30 minutes\n#SBATCH -N 1                # one node\n#SBATCH -n 2                # two CPU cores\n#SBATCH --gres=gpu:1        # one GPU\n#SBATCH -p sched_mit_psfc_gpu_r8     # a partition with Rocky 8 nodes\n\nmodule load apptainer/1.1.9   # load modules\nsingularity exec --nv -B /nobakcup1,/pool001 my-image.sif python my-code.py   # Run the program\n</code></pre></p> <p>Here is an example job script to run a Python program with a GPU and data files saved in <code>/om</code> or <code>/om2</code> directories,  <pre><code>#!/bin/bash                      \n#SBATCH -t 01:30:00             # walltime = 1 hours and 30 minutes\n#SBATCH -N 1                    # one node\n#SBATCH -n 2                    # two CPU cores\n#SBATCH --gres=gpu:1            # one GPU\n#SBATCH --constraint=rocky8     # nodes with Rocky 8 OS\n\nmodule load openmind8/apptainer/1.1.7        # load an apptainer module\nsingularity exec --nv -B /om,om2 my-image.sif python my-code.py  # Run the program\n</code></pre></p>","tags":["Engaging","OpenMind","Apptainer","Singularity","Software"]},{"location":"software/apptainer/#build-singularity-images","title":"Build Singularity images","text":"<p>In the previous section, it is assumed that all needed packages have been installed in the image. Users need to build a new image if some needed packages do not exist in the image. </p> <p>To save work for the building process, search for an image providing the right OS and necessary dependencies to support your target application, then use it as a base image and build your target application on top of it. </p> <p>The following is an example of building Python packages such as Pytorch and Pandas in a container image. </p> <p>First, download a Docker image that provides the Ubuntu OS and has Python and PyTorch installed already, <pre><code>singularity build --sandbox my-image  docker://bitnami/pytorch:latest\n</code></pre></p> <p>The command <code>build</code> here does not build anything yet, but just downloads the image and converts it to a new format. The flag <code>--sandbox</code> tells <code>build</code> to convert the image to the Sandbox format, which is convenient for installing packages interactively. </p> <p>Now you can install additional packages in the base image. In many installation processes, it requires virtual root privilege, which is enabled by the <code>fakeroot</code> command here. </p> EngagingOpenMind <p>Log in a head node that provides the <code>fakeroot</code> command,  <pre><code>ssh &lt;user&gt;@orcd-login004.mit.edu\n</code></pre></p> <p>Get an interative session on node115 that provides the <code>fakeroot</code> command,  <pre><code>srun -t 120 -w node115 --pty bash\n</code></pre></p> <p>Start a container shell with the flags <code>--writable --fakeroot</code>, which enables the write permission and virtual root privileges in the container, and then you can install your packages. Here is an exmaple,  <pre><code>$ singularity shell --writable --fakeroot my-image\nApptainer&gt; apt-get update\nApptainer&gt; pip install pandas\nApptainer&gt; python \nPython 3.8.17 (default, Jun 16 2023, 21:48:21) \n[GCC 10.2.1 20210110] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import pandas as pd\n</code></pre></p> <p>Here the <code>apt-get</code> is to install system software on an Ubuntu machine, which is supported by the <code>fakeroot</code> flag. The <code>pip install</code> is to build Python packages. Finally, the package Pandas is built in the base image with PyTorch. </p> <p>Alternatively, you can build a Docker image on a laptop/desktop such as MAC or PC, transfer it to the cluster, and run it with Singularity. Many packages have been developed and built in the Docker environment, so this approach is more convenient in many cases. </p>","tags":["Engaging","OpenMind","Apptainer","Singularity","Software"]},{"location":"software/building-software/","title":"Building Software","text":"<p>If you need software that is not a Python, Julia, or R package and available as a module on the cluster (current modules can be seen with the command module avail) you can request this software be compiled as a module by reaching out to orcd-help-engaging@mit.edu. If you expect there won't be many others who will be using this software, or if you need a special version of software for your job, you can compile the software yourself for personal use.</p> <p>For some common software we have written recipes for how to build them on Engaging. Check the \"ORCD Recipes\" section on the sidebar, or check out the Howto Recipes on the Index page.</p> <p>You can usually install whatever software you need in any directory you have write access to: your home, pool, or shared storage. This means the installation will be in this directory, rather than in a system-wide directory available for everyone. Most software can be installed this way, but it is not always well documented. If the binaries, or the executable files, for the software are available, you can put those in your home directory and add that path to your <code>$PATH</code> environment variable (see the Environment Variables unit in the section on the Linux Command Line for more information). Otherwise, you may have to build the software from source.</p> <p>The <code>sudo</code> Command</p> <p>You will not be able to run the <code>sudo</code> command on any ORCD system. The sudo command is used to run commands that could create system-wide changes that affect all users on the system. If instructions tell you to run a <code>sudo</code> command, see if you can run the command without <code>sudo</code>, or search for instructions on how to install without sudo, such as by building from source. If you are still having trouble reach out to orcd-help@mit.edu for help.</p> <p>Software compilation workflows will vary based on the software and its dependencies, but this is a general workflow for compiling software. Check if the documentation has instructions for building from source and refer to those in addition to these steps.</p> <p>Suggested Directory Structure</p> <p>There can be a lot of moving parts when building software, and it's easy to forget where or what you installed, so it is helpful to stay organized. We recommend a directory structure that looks like this:</p> <ul> <li>$HOME/software - A directory called \"software\" to keep all your builds<ul> <li>[software_name] - The name of the software<ul> <li>[source_code] - The directory of source code you downloaded</li> <li>install - For the installation files</li> <li>deps - For any dependencies needed to build your software<ul> <li>[dep1_src] - Downloaded source for dependency 1</li> <li>[dep2_src] - Downloaded source for dependency 2</li> <li>install - Directory for all dependency installs so they are in one place</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"software/building-software/#step-0-check-for-pre-built-binaries","title":"Step 0: Check for Pre-Built Binaries","text":"<p>In some cases your software will distribute pre-built binaries, or executable files, that you can run without building the software yourself. You will want to look for files that are labeled for the x86 architecture and linux operating system.</p> <p>You can follow the instructions in Step 1 for downloading the binaries onto Engaging and unpacking them. Once you have done that, skip to Step X to set your <code>$PATH</code> to use the software.</p> <p>Please note that this process assumes you have downloaded the source code for the software you wish to compile, and that source code is located at /home/$USER on the engaging cluster:</p>"},{"location":"software/building-software/#step-1-download-and-unpack-the-source-code","title":"Step 1: Download and Unpack the Source Code","text":"<p>Go to your software's download or releases page and look for the source code distributions. If there are different distributions for Linux and Windows, be sure to select the one for Linux.</p> <p>You can usually left-click and select \"Copy Link Address\" to get the URL to the download, then in a terminal on Engaging use the <code>wget</code> command to download the source. If the code is in GitHub you can check if they have tagged releases in the right column under \"Releases\", or clone the repository.</p> <p>Often the source is distributed in <code>tar.gz</code> files, so you will need to use the <code>tar</code> command to unpack:</p> <pre><code>tar -xf my-software.tar.gz\n</code></pre>"},{"location":"software/building-software/#step-2-start-an-interactive-job-for-compilation","title":"Step 2: Start an Interactive Job for Compilation","text":"<p>Builds are not too computationally heavy, but tend to run more slowly on the login nodes. To start an interactive job for compilation, use the command:</p> <p><pre><code>salloc -p mit_normal -c 4\n</code></pre> This will allocate 1 node from the <code>mit_normal</code> partition with 4 CPUs for compiling your software. You will receive output that your request for allocation has been submitted, and when a node has been allocated for you to use, it will say \u201c[Node_name] are ready for job\u201d.</p> <p>Note</p> <p>Be sure to request the partition you intend to run your jobs on. The node you build on will need to match the operating system of the nodes you plan to run your jobs on.</p>"},{"location":"software/building-software/#step-3-load-any-dependency-modules","title":"Step 3: Load Any Dependency Modules","text":"<p>Make sure you have a <code>gcc</code> module loaded with the <code>module list</code> command. If you don't see one, pick one of the more recent versions and load it with the <code>module load</code> command.</p> <p>If your software uses <code>cmake</code> to build, you will also need to load a cmake module:</p> <pre><code>module load cmake\n</code></pre> <p>If your application uses MPI load an MPI module as well. Again, if you aren't sure which version to choose we recommend using a newer version.</p> <p>Check your documentation for any other dependencies and load any available modules for those.</p>"},{"location":"software/building-software/#step-4-configure-software","title":"Step 4: Configure Software","text":"<p>The next step is often to run some kind of configure step. This is the step where you specify what build options you want or where any dependencies are installed. During this step there are usually checks for required dependencies and a working compiler. It is also at this step that you usually specify where you want your software installed.</p> <p>The two most common technologies for this step are a <code>configure</code> script and <code>cmake</code>. For these your software will come with either a <code>configure</code> script or a <code>CMakeLists.txt</code> (for <code>cmake</code>). If it comes with both you can check your software's documentation to see if they recommend one over the other. You will also want to check for any additional build flags you would like enabled, or additional instructions for this step.</p>"},{"location":"software/building-software/#running-the-configure-script","title":"Running the <code>configure</code> script","text":"<p>First go to the directory that contains the <code>configure</code> script. When you run <code>configure</code> you will need to specify the install location, usually somewhere in your home directory. You can do this with the <code>--prefix</code> flag:</p> <pre><code>./configure --prefix=$HOME/[install_directory]\n</code></pre> <p>where \u2018install_directory\u2019 is the directory where you would like the software to be installed to. If you plan to install multiple pieces of personal software, we recommend making a folder entitled <code>`software</code> in your home directory and installing software there. We like to use the path <code>/home/$USER/software/[software_name]/install</code> where <code>[software_name]</code> is the name of your software. The <code>install</code> directory is there to differentiate from any source code that may be stored in the same location. If you keep the source code elsewhere you can leave off <code>install</code> from the path.</p>"},{"location":"software/building-software/#running-cmake","title":"Running <code>cmake</code>","text":"<p>If your software has a <code>CMakeLists.txt</code> file, it uses cmake to build and you can use the flag <code>-DCMAKE_INSTALL_PREFIX</code> to specify an install location in your home directory. If you've installed any additional dependencies you can specify their location with the <code>-DCMAKE_PREFIX_PATH</code> flag. Consult the install documentation for any additional flags for other options that you might want.</p> <p>First go to the top level source code directory that should have the <code>CMakeLists.txt</code> file. Create a build directory and enter it:</p> <pre><code>mkdir build\ncd build\n</code></pre> <p>This <code>build</code> directory is where you will run cmake.</p> <p>Then use the cmake command with the option <code>-DCMAKE_INSTALL_PREFIX</code> pointing to the install location you want and <code>-DCMAKE_PREFIX_PATH</code> pointing to the location of any dependencies (if you have them). The \u201c..\u201d at the end tells cmake to look in the directory above for the <code>CMakeLists.txt</code> file:</p> <pre><code>cmake -DCMAKE_INSTALL_PREFIX=$HOME/[install_directory] -DCMAKE_PREFIX_PATH=$HOME/path/to/deps ..\n</code></pre>"},{"location":"software/building-software/#step-5-build-the-software-with-make","title":"Step 5: Build the Software with <code>make</code>","text":"<p>Next you will run the \u2018make\u2019 command to build the software. If you have started an interactive job, you will want to specify that you want to use the CPUs you had allocated for compilation (4 in this example). You can do this by using the command:</p> <pre><code>make -j 4\n</code></pre> <p>This step can take a long time depending on the size of the software you are building.</p> <p>If you ran the configure step with <code>cmake</code>, sometimes <code>cmake</code> will run this step for you. You will get a message that there is nothing to do.</p> <p>Tip</p> <p>One nice thing about <code>make</code> is if it gets interrupted it will pick up where it left off. If you do happen to get logged out while running <code>make</code>, make sure you have set up your environment in the same way you did the first time. This includes loading modules or setting environment variables.</p>"},{"location":"software/building-software/#step-6-install-software","title":"Step 6: Install Software","text":"<p>Once your make command finishes successfully, it is time to install the software. This is done simply with the command:</p> <pre><code>make install\n</code></pre> <p>This command copies all the installation files, the files needed to run the software, to its final install location that you set in Step 4. This command will often fail if you don't set an install location, as the most common default is to install in a system-wide directory.</p>"},{"location":"software/building-software/#step-7-confirm-software-installation-was-successful","title":"Step 7: Confirm Software Installation Was Successful","text":"<p>At this point, you should be able to run the software you have compiled if compiled successfully. Binaries, or the executable files, are usually placed in the <code>bin</code> directory of your install directory. You run a binary to test that it works by typing out the path to that binary. For example:</p> <pre><code>$HOME/software/[software_name]/install/bin/my_cmd\n</code></pre> <p>Note that this path will change depending on where you installed your software.</p> <p>You may need to load some or all of the modules that you had loaded when you built the software, and you may need to set additional environment variables. Your software's documentation may help.</p>"},{"location":"software/building-software/#step-8-add-the-software-to-your-path","title":"Step 8: Add the Software to Your PATH","text":"<p>The <code>$PATH</code> environment variable lists the directories that Linux searches for command executable files. While you can type out the full path to your newly installed binary as in Step 7, it can be more convenient to type only the command itself. To do that you can add the install location to your <code>$PATH</code>. It is important to set it properly, if you remove existing entries from your <code>$PATH</code> it can affect your ability to run basic commands like <code>ls</code>.</p> <pre><code>export PATH=$HOME/software/[software_name]/install/bin:$PATH\n</code></pre> <p>Note that this path will change depending on where you installed your software.</p>"},{"location":"software/building-software/#examples","title":"Examples","text":"<p>The Recipes section in the documentation contains several examples of builds that have been done on Engaging. Some examples include:</p> <ul> <li>Vasp</li> <li>Gromacs</li> <li>Orca (Installing pre-built binaries)</li> </ul>"},{"location":"software/compile/","title":"Compiling Source Code and GNU Make","text":"<p>This page covers the basics of building programs from C source code, and automating this process using GNU Make. It is intended for scientists venturing into scientific programming, to help ease the frustrations that typically come up when starting to work in compiled programming languages.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#preparation","title":"Preparation","text":"Engaging <p>A GCC compiler is needed to compile codes. Load a GCC module first,   <pre><code>module load gcc/12.2.0\n</code></pre></p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#building-a-single-file-program","title":"Building a single-file program","text":"<p>Let's start with a simple example: building a \"hello world\" C program with the GCC compiler. The program (hello.c) looks like this: <pre><code>#include &lt;stdio.h&gt;\nint main()\n{\n    printf(\"Hello World\\n\");\n    return (0);\n}\n</code></pre></p> <p>To build a working executable from this file, run: <pre><code>gcc hello.c -o hello\n</code></pre></p> <p>This command creates an executable with a name of hello. Running this command prints the familiar message: <pre><code>$ ./hello\nHello World\n</code></pre></p> <p>More happened here behind the scene. In fact, this command wraps up 4 steps of the build process: Preprocess, Compile, Assemble, and Link.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#step-1-preprocess","title":"Step 1: Preprocess","text":"<p>In this step, <code>gcc</code> calls preprocessing program <code>cpp</code> to interpret preprocessor directives and modify the source code accordingly.</p> <p>Some common directives are:</p> <ul> <li> <p><code>#include</code>: includes contents of the named file, typically a header file, e.g. <code>#include &lt;stdio.h&gt;</code>.</p> </li> <li> <p><code>#define</code>: macro substitution, e.g. <code>#define PI 3.14159</code>.</p> </li> <li> <p><code>#ifdef ... #end</code>: conditional compilation, the code block is included only if a certain macro is defined, e.g: <pre><code>#ifdef TEST_CASE\n  a=1; b=0; c=0;\n#endif\n</code></pre></p> </li> </ul> <p>We could perform just this step of the build process like so: <pre><code>cpp hello.c hello.i\n</code></pre></p> <p>Examining the output file (<code>vim hello.i</code>) shows that the long and messy stdio.h header has been appended to our simple code. </p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#step-2-compile","title":"Step 2: Compile","text":"<p>In this step, the (modified) source code is translated from the C programming language into assembly code.</p> <p>Assembly code is a low-level programming language with commands that correspond to machine instructions for a particular type of hardware. It is still just plain text, that says you can read assembly and write it too if you so desire.</p> <p>To perform just the compilation step of the build process, we would run: <pre><code>gcc -S -c hello.i -o hello.s\n</code></pre></p> <p>Examining the output file (<code>vim hello.s</code>) shows processor-specific instructions needed to run our program on this specific system. Interestingly, for such a simple program as ours, the assembly code is actually shorter than the preprocesses source code (though not the original source code).</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#step-3-assemble","title":"Step 3: Assemble","text":"<p>Assembly code is then translated into object code. This is a binary representation of the actions your computer needs to take to run your program. It is no longer human-readable, but it can be understood by computers.</p> <p>To perform just this step of the build process, we would run: <pre><code>gcc -c hello.s -o hello.o\n</code></pre></p> <p>You can try to view this object file like we did the other intermediate steps (<code>vim hello.o</code>), but the result will not be useful . Your text editor is trying to interpret binary machine language commands as ASCII characters, and (mostly) failing. Perhaps the most interesting result of doing so is that there are intelligable bits --- these are the few variables, etc, that actually are ASCII characters.</p> <p>Also note that object files are not executables, you can't run them until after the next step.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#step-4-link","title":"Step 4: Link","text":"<p>In the final step, <code>gcc</code> calls the linker program <code>ld</code> to combine the object file with any external functions it needs (e.g. library functions or functions from other source files). In our case, this would include <code>printf</code> from the C standard library.</p> <p>To perform just this step of the build process, we would run: <pre><code>gcc hello.o -o hello\n</code></pre> This produces the executable <code>hello</code> finally. </p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#building-a-multi-file-program","title":"Building a multi-file program","text":"<p>For most projects in the real world, it is convenient to break up the source code into multiple files. Typically, these include a main function in one file, and one or more other files containing functions / subroutines called by <code>main()</code>. In addition, a header file is usually used to share custom data types, function prototypes, preprocessor macros, etc.</p> <p>As an example, we create several source code files in a directory named multi_string, which consists of:</p> <ul> <li>main.c: the main driver function, which calls a subroutine and exits</li> <li>WriteMyString.c: a module containing the subroutine called by main</li> <li>header.h: one function prototype and one macro definition</li> </ul> Side note: source codes for the multi_string program <p>main.c:  <pre><code>#include \"header.h\"\n#include &lt;stdio.h&gt;\nchar    *AnotherString = \"Hello Everyone\";\nmain()\n{\n  printf(\"Running...\\n\");\n  WriteMyString(MY_STRING);\n  printf(\"Finished.\\n\");\n}\n</code></pre></p> <p>WriteMyString.c: <pre><code>#include &lt;stdio.h&gt;\nextern char *AnotherString;\nvoid WriteMyString(char *ThisString)\n{\n  printf(\"%s\\n\", ThisString);\n  printf(\"Global Variable = %s\\n\", AnotherString);\n}\n</code></pre></p> <p>header.h:  <pre><code>#define MY_STRING \"Hello World\"\nvoid WriteMyString();\n</code></pre></p> <p>The easiest way to compile such a program is to include all the required source files at the <code>gcc</code> command line: <pre><code>gcc main.c WriteMyString.c -o my_string\n./my_string\n</code></pre></p> <p>It is also quite common to separate out the process into two steps:</p> <ol> <li> <p>source code -&gt; object code <pre><code>gcc -c WriteMyString.c\ngcc -c main.c\n</code></pre></p> </li> <li> <p>object code -&gt; executable (or library) <pre><code>gcc WriteMyString.o main.o -o my_string\n</code></pre></p> </li> </ol> <p>The reason is that this allows you to reduce compiling time by only recompiling objects that need to be updated. This seems silly for a program with only a few source files, but becomes important when many source files are involved. We will use this approach later when we discuss automating the build process.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#including-header-files","title":"Including header files","text":"<p>In the above process, it is not necessary to include the header file explicitly on the <code>gcc</code> command line. This makes sense since we know that the (bundeled) preprocessing step will append any required headers to the source code before it is compiled.</p> <p>There is one caveat: the preprocessor must be able to find the header files in order to include them. Our example works because the header.h file is in the current directory when we run <code>gcc</code>. We can break it by moving the header to a new subdirectory, like so: <pre><code>mkdir include\nmv header.h include\ngcc main.c WriteMyString.c -o my_string\n</code></pre></p> <p>The above commands give the output error: <pre><code>main.c:4:10: fatal error: header.h: No such file or directory\n    4 | #include \"header.h\"\n      |          ^~~~~~~~~~\ncompilation terminated.\n</code></pre></p> <p>We can fix this by specifically telling gcc where it can find the requisite headers, using the <code>-I</code> flag: <pre><code>gcc -I ./include main.c WriteMyString.c -o my_string\n</code></pre></p> <p>This is most often needed in the case where you wish to use external libraries installed in non-standard locations. We will explore this case in the next section. </p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#linking-external-libraries","title":"Linking external libraries","text":"<p>A library is a collection of pre-compiled object files that can be linked into your programs via the linker. In simpler terms, they are machine code files that contain functions / subroutines that you can use in your programs.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#shared-libraries-vs-static-libraries","title":"Shared libraries vs static libraries","text":"<p>A static library has file extension of .a (meaning archive file). When your program links a static library, the machine code of external functions used in your program is copied into the executable. At runtime, everything your program needs is wrapped up inside the executable.</p> <p>A shared library has file extension of .so (meaning shared objects). When your program is linked against a shared library, only a small table is created in the executable. At runtime, the exectutable must be able to locate the functions listed in this table. This is done by the operating system - a process known as dynamic linking.</p> <p>Static libraries certainly seem simpler, but most programs use shared libraries and dynamic linking. There are several reasons why the added complexity is thought to be worth it:</p> <ul> <li>Makes executable files smaller and saves disk space, because one copy of a library can be shared between multiple programs.</li> <li>Most operating systems allow one copy of a shared library in memory to be used by all running programs, saving memory.</li> <li>If your libraries are updated, programs using shared libraries automatically take advantage of these updates, programs using static libraries would need to be recompiled.</li> </ul> <p>Because of the advantage of dynamic linking, GCC will prefer a shared library to a static library if both are available (by default). We will only use shared libraries in the following. </p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#building-with-libraries-in-default-locations","title":"Building with libraries in default locations","text":"<p>Many useful fuctions are provided by libraries in the operating system. These are two widely-used examples:</p> <ul> <li><code>printf()</code> from the libc.so shared library</li> <li><code>sqrt()</code> from the libm.so shared library</li> </ul> <p>In this section, we will introduce how to build a program with shared libraries in the system default locations. Let's start with an example (roots.c) that uses the <code>sqrt()</code> function from the math library: <pre><code>#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\nvoid main()\n{ \n    int i;\n\n    printf(\"\\t Number \\t\\t Square Root of Number\\n\\n\");\n\n    for (i=0; i&lt;=360; ++i)\n        printf(\"\\t %d \\t\\t\\t %d \\n\", i, sqrt((double) i));\n\n}\n</code></pre></p> <p>Notice the function <code>sqrt</code>, which we use, but do not define. The (machine) code for this function is stored in libm.so, and the function definition is stored in the header file math.h.</p> <p>To build successfully, we must:</p> <ol> <li>Include the header file for the external library;</li> <li>Instruct the linker to link to the external library.</li> </ol> <p>We build the program using the two-step scheme: <pre><code>gcc -c roots.c\ngcc roots.o -lm -o roots\n</code></pre></p> <p>The first command preprocesses roots.c, appending the header files, and then translates it to object code. This step does need to find the header file, but it does not yet require the library.</p> <p>The second command links all of the object code into the executable. It does not need to find the header file, which has already been compiled into roots.o, but it does need to find the library file.</p> <p>Library files are linked using the <code>-l</code> flag. Their names are given excluding the lib prefix and exluding the .so suffix, which translates libm.so into <code>m</code> in this case. So we use <code>-lm</code> in the command. </p> <p>Just as we did above, we can combine the two steps into a single command: <pre><code>gcc roots.c -lm -o roots\n</code></pre></p> <p>Finally, we can run the programm: <pre><code>./roots\n</code></pre></p> <p>Note</p> <p>Because we are using shared libraries, the linker must be able to find the linked libraries at runtime, otherwise the program will fail. You can check the libraries required by a program, and whether they are being found correctly or not using the <code>ldd</code> command. For out roots program, we get the following <pre><code>$ ldd roots\nlinux-vdso.so.1 (0x00007ffd2c962000)\nlibm.so.6 =&gt; /lib64/libm.so.6 (0x00007fceadbef000)\nlibc.so.6 =&gt; /lib64/libc.so.6 (0x00007fcead82a000)\n/lib64/ld-linux-x86-64.so.2 (0x00007fceadf71000)\n</code></pre></p> <p>This shows that our executable requires a few basic system libraries such as libc.so as well as the math library <code>libm.so</code> we explicitly included, and that all of these dependencies are found by the linker.</p> Side note: where does the preprocessor look to find header files? <p>The preprocessor will search some default paths for included header files. Before we go down the rabbit hole, it is important to note that you do not have to do this for a typical build, but the commands may prove useful when you are trying to work out why something fails to build.</p> <p>To look for the header, we can run the following command to show the preprocessor search path: <pre><code>cpp -Wp,-v\n</code></pre> The output show the paths where GCC will search for header files by default. </p> Side note: where does the linker look to find libraries? <p>The linker will search some default paths for library files. Again, it is important to note that you do not have to do this for a typical build, but the commands may prove useful when you are trying to work out why something fails to build.</p> <p>To look for the library, we can run the following command to get a list of all library files the linker is aware of,  <pre><code>ldconfig -p \n</code></pre> or search that list for the math library we need: <pre><code>ldconfig -p | grep libm.so\n</code></pre> The latter command gives the output: <pre><code>libm.so.6 (libc6,x86-64, OS ABI: Linux 3.2.0) =&gt; /lib64/libm.so.6\n</code></pre> which shows that the math library is available. </p> <p>We might also want to peek inside a library file (or any object code for that matter) to see what functions and variables are defined within. We can list all the names, then search for the one we care about, like so: <pre><code>nm /lib64/libm.so.6 | grep \" sqrt\"\n</code></pre> The output of this command contains the following line, which shows that it does indeed include something called <code>sqrt</code>. <pre><code>000000000000f7d0 W sqrt\n</code></pre></p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#building-with-libraries-in-non-default-locations","title":"Building with libraries in non-default locations","text":"<p>In many cases, you may need to use external libraries that are not included in the operating system. These libraries can be built by you or other develepers and they are saved in non-default locations. In this section, we will introduce how to build a program with libraries in non-default locations. </p> <p>Let's switch to a new example code. We create a source code named use_ctest.c that reads the following: <pre><code>#include &lt;stdio.h&gt;\n#include \"ctest.h\"\n\nint main(){\n    int x;\n    int y;\n    int z;\n    ctest1(&amp;x);\n    ctest2(&amp;y);\n    z = (x / y);\n    printf(\"%d / %d = %d\\n\", x, y, z);\n    return 0;\n}\n</code></pre> This code calls two functoins <code>ctest1</code> and <code>ctest2</code>, which are included in a custom library named ctest.</p> Side note: building a library <p>In the same level of the main code use_ctest.c, we create a directory named ctest_dir to save all files related to the library ctest.  <pre><code>mkdir ctest_dir\n</code></pre></p> <p>First, create a subdirectory named <code>src</code>,  <pre><code>cd ctest_dir\nmkdir src\ncd src\n</code></pre> and save the following two source code files in there. </p> <p>ctest1.c: <pre><code>void ctest1(int *i){\n  *i=100;\n}\n</code></pre></p> <p>ctest2.c: <pre><code>$ cat ctest2.c \nvoid ctest2(int *i){\n  *i=5;\n}\n</code></pre></p> <p>Each code does nothing but defines an interger.</p> <p>Second, create another subdirectory named <code>include</code>, <pre><code>mkdir include\n</code></pre> and save the following header file in there, ctest.h: <pre><code>#ifndef CTEST_H\n#define CTEST_H\nvoid ctest1(int *);\nvoid ctest2(int *);\n#endif\n</code></pre> This is for the declaration of the funtcions.</p> <p>Third, use the following commands to build the shared library named <code>libctest.so</code>: <pre><code>gcc -Wall -fPIC -c ctest1.c ctest2.c\ngcc -shared -Wl,-soname,libctest.so -o libctest.so ctest1.o ctest2.o\n</code></pre></p> <p>Finally, we move the library to a directory named lib,  <pre><code>cd ..\nmkidr lib\nmv src/libctest.so lib\n</code></pre></p> <p>Assuming that the library ctest has been built (as instructed in the above side note), we will build the program use_ctest and fix possbile errors in the process.</p> <p>First, we start with the simplest command: <pre><code>gcc -c use_ctest.c\n</code></pre> It fails with an error: <pre><code>use_ctest.c:2:10: fatal error: ctest.h: No such file or directory\n</code></pre></p> <p>As the error message indicates, the problem here is that an included header file is not found by the preprocessor. We know that we can use the <code>-I</code> flag to fix this problem: <pre><code>gcc -I ctest_dir/include -c use_ctest.c\n</code></pre> then it creates an object file use_ctest.o.</p> <p>The next step is to use the linker to create an executable. As we have known, we need to explicitly add the library with the <code>-l</code> flag, <pre><code>gcc use_ctest.o -lctest -use_ctest\n</code></pre> but in this case we still get an error: <pre><code>/usr/bin/ld: cannot find -lctest\ncollect2: error: ld returned 1 exit status\n</code></pre> Just like for the header, we need to explicitly specify the path to the library file using <code>-L</code>: <pre><code>gcc -L ctest_dir/lib  use_ctest.o -lctest -o use_ctest\n</code></pre> An executable use_ctest is created successfully! </p> <p>Howerver, what happens when we try to run our shiny new executable? <pre><code>$ ./use_ctest \n./use_ctest: error while loading shared libraries: libctest.so: cannot open shared object file: No such file or directory\n</code></pre></p> <p>Frustrating? No worry. This is a commonly seen error. We can diagnose this problem by checking to see if the dynamic linker is able to gather up all the dependencies at runtime: <pre><code>$ ldd use_ctest\n    linux-vdso.so.1 (0x00007fff56d9d000)\n    libctest.so =&gt; not found\n    libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f7f46df6000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007f7f471bb000)\n</code></pre></p> <p>The output clearly shows that it does not. The problem here is that the dynamic linker will only search the system default paths. There are a few solutions. </p> <ul> <li> <p>Permanently add our custom library to one of the system default paths. This option needs root permissoins, which is not available for HPC users and thus is not recommended here. </p> </li> <li> <p>Specify the location of libraries using the <code>LD_LIBRARY_PATH</code> environment variable. <code>LD_LIBRARY_PATH</code> contains a colon (:) separated list of directories where the dynamic linker should look for shared libraries. The linker will search these directories before the default system paths. You can define the value of <code>LD_LIBRARY_PATH</code> like so: <pre><code>export LD_LIBRARY_PATH=./ctest_dir/lib:$LD_LIBRARY_PATH\n</code></pre> and then run the program: <pre><code>$ ./use_ctest \n100 / 5 = 20\n</code></pre> It works! </p> </li> <li> <p>Hard-code the location of libraries into the executable. Setting (and forgeting to set) <code>LD_LIBRARY_PATH</code> all the time can be tiresome. An alternative approach is to burn the location of the shared libraries into the executable as an <code>RPATH</code> or <code>RUNPATH</code>. This is done by adding some additional flags for the linker, like so: <pre><code>gcc -L ctest_dir/lib use_ctest.o -lctest -Wl,--rpath=ctest_dir/lib -o use_ctest_1\n</code></pre> We can confirm that this works by running the program: <pre><code>$ ./use_ctest_1\n100 / 5 = 20\n</code></pre> or examining the executable to show that it contains the needed library: <pre><code>$ readelf -d use_ctest_1 |grep ctest\n 0x0000000000000001 (NEEDED)             Shared library: [libctest.so]\n 0x000000000000000f (RPATH)              Library rpath: [ctest_dir/lib]\n</code></pre></p> </li> </ul>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#automating-the-build-process-with-gnu-make","title":"Automating the build process with GNU Make","text":"<p>The manual build process we used above can become quite tedious in real world. There are many ways that we might automate this process. The simplest would be to write a shell script that runs the build commands each time we invoke it. Let's take the simple hello.c program as a test case. Here is a bash shell script (named build.sh) to automate the building process, <pre><code>#!/bin/bash\ngcc -c hello.c\ngcc hello.o -o hello\n</code></pre> Run it like so: <pre><code>chmod +X build.sh\n./build.sh\n</code></pre></p> <p>This works fine for small projects, but for large multi-file projects, we would have to compile all the source codes every time we change anything in the codes.</p> <p>The GNU Make utility provides a useful way around this problem. The solution is that we (the programmers) write a special script that defines all the dependencies between source files, edit one or more files in our project, then invoke Make to re-compile only those files that have been changed.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#how-gnu-make-works","title":"How GNU Make works","text":"<p>GNU Make is a mini-programming language unto itself. To start, we need to create a file named Makefile or makefile to define a set of tasks to be executed. For the simple hello program, a Makefile is like this: <pre><code>hello: hello.o\n        gcc hello.o -o hello\n\nhello.o: hello.c\n        gcc -c hello.c\n\nclean:\n        rm hello hello.o\n</code></pre></p> <p>We can see that each section starts with a line specifyting dependency like so: <code>target: prerequisites</code>. The command block that follows will be executed to generate the target if any of the prerequisites have been modified. </p> <p>Note</p> <p>Every line in the command block should be started a <code>tab</code> character instead of multiple <code>space</code> characters.</p> <p>Once a Makefile is ready, the program can be built by executing one single command, <pre><code>make\n</code></pre> It looks for the Makefile in the same directory and build the targets. The first (top) target will be built by default, or you can specify a specific target to be built,  <pre><code>make hello\n</code></pre></p> <p>When we run <code>make</code>, the computer will take the following actions:</p> <ol> <li> <p>Find the default target, which is our executable hello.</p> </li> <li> <p>Check if the target file hello is up-to-date. A target is considered out-of-date if it does not exist or is older than any of the prerequisites. As hello does not exist, so it will be built.</p> </li> <li> <p>The prerequisite of hello is hello.o, which is also a target, so check if it is up-to-date. As hello.o does not exist, so it will be built.</p> </li> <li> <p>The prerequisite of hello.o is hello.c, which is not a target, so there is nothing left to check. The command <code>gcc -c hello.c</code> will be run to create hello.o.</p> </li> <li> <p>Now hello.o is up-to-date, so the next target hello will be built by running the command <code>gcc hello.o -o hello</code>.</p> </li> </ol> <p>Note that the command under the clean target is not executed by <code>make</code>, because it is neither the first target nor an prerequisite of any other target. To bring it up, we need to specify the target name: <pre><code>make clean\n</code></pre> The command under this target will remove the executable and all of the <code>.o</code> files. Note that if all targets are up-to-date, make does not do anything. </p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#makefile-for-a-multi-file-program","title":"Makefile for a multi-file program","text":"<p>Now let's switch to the multi-file example program used in previous sections. A <code>Makefile</code> is like this:  <pre><code>write: main.o WriteMyString.o\n        gcc main.o WriteMyString.o -o write\n\nmain.o: main.c header.h\n        gcc -c main.c\n\nWriteMyString.o: WriteMyString.c\n        gcc -c WriteMyString.c\n\nclean: \n        rm write *.o\n</code></pre></p> <p>For the first build, <code>make</code> builds the targets in the following order: main.o, WriteMyString.o, and write. This compiles all source codes and then links all object files to create the executable. </p> <p>When the program is rebuilt, <code>make</code> will only build the targets whose prerequisites have been modified since the last build. This feature makes the building process efficient for a program with many source files. For example, if WriteMyString.c is modified, it is recompiled, while main.c is not. If main.c or header.h is modified, only main.c is recompiled, while WriteMyString.c is not. In either case, the write target will be rebuilt, since either main.o or WriteMyString.o is updated.</p> <p>We need to run <code>make clean</code> and then run <code>make</code>, if we want to completely rebuild evetyhing.</p> Note: silent mode <p>By default, <code>make</code> prints on the screen all the commands that it executes. To suppress the print, add an <code>@</code> before the commands, or turn on the silent mode with the option <code>-s</code>:  <pre><code>make -s\n</code></pre></p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#writing-a-good-makefile","title":"Writing a good Makefile","text":"<p>A Makefile could be very compilcated in a practical program with many source files. It is important that the text in a Makefile should be as simple and clear as possbile. In this section, we will learn more useful features of GNU Make and write a real-world Makrefile at the end.</p> <p>In our previous examples, you may have noticed that there are many duplications of the same file name or command name. It is convinient to use varialbes in this case. Again, take our multi-file program for example. The Makefile can be rewitten as follows, <pre><code>CC=gcc\nOBJ=main.o WriteMyString.o\nEXE=write\n\n$(EXE): $(OBJ)\n        $(CC) $(OBJ) -o $(EXE)\n\nmain.o: main.c header.h\n        $(CC) -c main.c\n\nWriteMyString.o: WriteMyString.c\n        $(CC) -c WriteMyString.c\n\nclean:\n        rm $(EXE) *.o\n</code></pre></p> <p>Here we have defined the varialbes <code>CC</code> for the compiler, <code>OBJ</code> for object files, and <code>EXE</code> for the executable file. If we want to change the compiler or the file names, we only modify the corresponding variable at one place. </p> <p>If there are many varialbes to be defined, it is better to write the definition of all variables in another file, and then include the file in Makefile: <pre><code>include ./variables \n</code></pre> The file variables reads the following: <pre><code>CC=gcc\nOBJ=main.o WriteMyString.o\nEXE=write\n</code></pre></p> <p>Furthermore, we can upgrade the Makefile to a higher automatic level using the so-called automatic variables: <pre><code>$(EXE): $(OBJ)\n        $(CC) $^ -o $@\n\nmain.o: main.c header.h\n        $(CC) -c $&lt;\n\nWriteMyString.o: WriteMyString.c\n        $(CC) -c $&lt; \n</code></pre></p> <p>Here we have used these automatic variables:</p> <ul> <li><code>$@</code>  -- the name of the current target</li> <li><code>$^</code>  -- the names of all the prerequisites</li> <li><code>$&lt;</code>  -- the name of the first prerequisite</li> </ul> <p>These variables automatically take the names of current target or prerequisites, no matter what values are assigned to them.</p> <p>We then notice that the main.o and WriteMyString.o targets are built by the same command. Is there a way to combine these two duplicated commands into one so as to compile all source code files by only one command line? The answer is yes. It can be done with an implicit pattern rule: <pre><code>%.o: %.c\n        $(CC) -c $&lt;\n\nmain.o: header.h \n</code></pre></p> <p>Here the <code>%</code> stands for the same thing in the prerequisites as it does in the target. Usually, any object file with a subfix <code>.o</code> has a corresponding source file with a subfix <code>.c</code> as an implied prerequisite, so we can use the <code>%</code> to represent the name for both files. If a target (e.g. main.o) needs additional prerequisites (e.g. header.h), write an actionless rule with those prerequisites. We can imagine that applying this impilict rule should significantly simpify a Makefile when there are a large number of source files.</p> <p>Mostly the target name is a file name. But there are exceptions, such as the clean target in this example. The <code>rm</code> command will not create any file named <code>clean</code>. What if there exists a file named clean in this directory? Let's do an experiment. <pre><code>$ touch clean\n$ make clean\nmake: `clean' is up to date.\n</code></pre></p> <p>We can see that the clean target does not work properly. Since it has no prerequisite, the target clean will be considered up-to-date, and thus nothing will be done. To solve this issue, we can declare the target to be phony by making it a prerequisite of the special target <code>.PHONY</code> as follows: <pre><code>.PHONY: clean\n</code></pre></p> <p>A phony target is one that is not really the name of a file; rather it is just a name for a recipe to be executed.</p> <p>In summary, we write a real-world Makefile like so: <pre><code>include ./variables\n.PHONY: clean\n\n$(EXE): $(OBJ)\n        $(CC) $^ -o $@\n\n%.o: %.c\n        $(CC) -c $&lt;\n\nmain.o: header.h\n\nclean:\n        rm $(EXE) *.o\n</code></pre></p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/julia/","title":"Julia","text":"<p>Julia is a high-level, high-performance programming language designed for technical and numerical computing, known for its speed and ease of use. Because of its popularity, we have pre-installed versions of Julia on each of our clusters. You can begin using Julia right away by running <code>module load julia</code> or by specifying the module specifically:</p> EngagingSuperCloud <p>On Rocky8 nodes:</p> <pre><code>module load julia/1.10.1\n</code></pre> <p>On Centos7 nodes:</p> <pre><code>module load julia/1.8.5\n</code></pre> <pre><code>module load julia/1.10.1\n</code></pre>","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/julia/#installing-packages","title":"Installing Packages","text":"<p>Julia organizes packages by the version of Julia you're running. When you install packages, a <code>.julia</code> folder automatically gets created in your home directory that holds all package installations. Julia will automatically create an environment for that version, which will be saved in <code>~/.julia/environments</code>.</p> <p>You can change the default package install location by setting the <code>$JULIA_DEPOT_PATH</code> environment variable from the command line before you start Julia. For example:</p> <pre><code>export JULIA_DEPOT_PATH=/home/$USER/orcd/r8/pool\n</code></pre>","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/julia/#juliaup","title":"Juliaup","text":"<p>Juliaup provides a convenient way to manage different versions of Julia and different package installations. It can be installed by running the following command:</p> <pre><code>curl -fsSL https://install.julialang.org | sh\n</code></pre> <p>You will be asked if you want to proceed with default settings or to customize your installation. We recommend customizing your installation. The default settings are as follows:</p> <ol> <li> <p>Save the <code>.juliaup</code> folder to your home directory. This folder contains all installations of Julia and their associated packages that are managed by Juliaup.</p> </li> <li> <p>Edit your <code>.bashrc</code> and <code>.bash_profile</code> files to automatically add the Juliaup-managed version of Julia to your <code>$PATH</code> environment variable.</p> <ul> <li> <p>We generally discourage editing your <code>.bashrc</code> file because it can cause issues when trying to use other software. For example, if you want to use a pre-installed Julia module, you would have to manually remove Juliaup from your <code>$PATH</code> any time you connect to the cluster.</p> </li> <li> <p>To add Juliaup to your <code>$PATH</code> manually, run: <pre><code>export PATH=/path/to/.juliaup/bin${PATH:+:${PATH}}\n</code></pre></p> </li> </ul> </li> </ol> <p>Click here for more information on installing and using Juliaup.</p>","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/julia/#using-different-julia-versions","title":"Using Different Julia Versions","text":"","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/julia/#juliaup-versions","title":"Juliaup Versions","text":"<p>If you're using Juliaup, installing different versions of Julia is straightforward:</p> <pre><code># Install Julia 1.9.0:\njuliaup add 1.9.0\n# Use Julia 1.9.0:\njulia +1.9.0\n</code></pre> <p>Note that Juliaup installs versions and packages to your <code>.julia</code> folder.</p>","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/julia/#manual-installation","title":"Manual Installation","text":"<p>If you are unable to use Juliaup and you need a version of Julia that is not pre-installed on the cluster, you can manually download it.</p> <p>A complete list of previous Julia versions can be found here. From this site, copy the link to the <code>.tar.gz</code> file that corresponds to the version you need. Be sure to select the version for a Linux operating system and x86_64 architecture.</p> <p>Download the <code>.tar.gz</code> file: <pre><code>wget [link to file]\n</code></pre></p> <p>Extract the <code>.tar.gz</code> file: <pre><code>tar -xvzf [file name]\n</code></pre></p> <p>Add the downloaded version to your path: <pre><code>export PATH=\"~/path/to/julia/bin:$PATH\"\n</code></pre></p> <p>The following example is for Julia 1.9.0: <pre><code>wget https://julialang-s3.julialang.org/bin/linux/x64/1.9/julia-1.9.0-linux-x86_64.tar.gz\ntar -xvzf julia-1.9.0-linux-x86_64.tar.gz\nexport PATH=\"~/julia-1.9.0/bin:$PATH\"\n</code></pre></p>","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/julia/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>While Jupyter is heavily integrated with Python, it supports compatibility with Julia. You can run Jupyter notebooks on the web portals of SuperCloud and Engaging.</p> EngagingSuperCloud <p>On the Engaging OnDemand web portal, you can specify one of the pre-installed Julia modules under the \"Additional Modules\" section. You can see which Julia modules are available by running <code>module avail julia</code> from the command line.</p> <p>For Jupyter to recognize Julia, you need to have the <code>IJulia</code> package installed and built in your Julia environment:</p> <pre><code>using Pkg\nPkg.add(\"IJulia\")\nPkg.build(\"IJulia\")\n</code></pre> <p>Note that the Julia environment you are using needs to match the version of Julia that you are loading as a module.</p> <p>To use a Jupyter notebook on the SuperCloud web portal, navigate to <code>/jupyter/</code> and launch a notebook. When the session is running, open the notebook and select a Julia kernel.</p> <p>For more information on running Jupyter notebooks on SuperCloud, check the SuperCloud documentation.</p>","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/julia/#port-forwarding","title":"Port Forwarding","text":"<p>If you would like to use a different version of Julia that is not offered as a module, we suggest running a Jupyter notebook manually via port forwarding. This involves running the notebook on a compute node, and then accessing the notebook on your local machine by SSH tunnelling through a login node.</p> <p>To do this, first request a compute node with your desired resources:</p> <pre><code>salloc -N 1 -n 4 -p mit_normal\n</code></pre> <p>Make a note of the node that your job is running on. In this example, we're running on <code>node1600</code>.</p> <p>Then, start your desired version of Julia add the <code>IJulia</code> package to your environment:</p> <pre><code>using Pkg\nPkg.add(\"IJulia\")\nPkg.build(\"IJulia\")\nquit()\n</code></pre> <p>Because of Jupyter's interaction with Python, you need to create and activate a Conda environment with <code>jupyter</code> installed:</p> <pre><code>module load miniforge\nconda create -n jupyter_env jupyter\nconda activate jupyter_env\n</code></pre> <p>Note</p> <p>For more information on Conda, see our Python documentation.</p> <p>Now, we can run the notebook. To be able to access it on our local machine, we need to add a few arguments:</p> <pre><code>jupyter-lab --ip=0.0.0.0 --port=8888\n</code></pre> <p>The port can be any number between 1024 and 9999. When you run the notebook, the output will contain a link with a token that allows you to access the notebook:</p> <pre><code>http://127.0.0.1:&lt;remote port&gt;/lab?token=&lt;token&gt;\n</code></pre> <p>For example:</p> <pre><code>http://127.0.0.1:8888/lab?token=7e97d59f9a17c91c11289bc5bec35ad3921725c6db55fe33\n</code></pre> <p>In a second terminal window, set up an SSH tunnel to your Jupyter notebook that's running on the compute node:</p> <pre><code>ssh -L &lt;local port&gt;:&lt;node&gt;:&lt;remote port&gt; &lt;USER&gt;@orcd-login001.mit.edu\n</code></pre> <p>In general, it's easier if you keep the local port and the remote port as the same number:</p> <pre><code>ssh -L 8888:node1600:8888 &lt;USER&gt;@orcd-login001.mit.edu\n</code></pre> <p>Now you can access Jupyter in an internet browser:</p> <pre><code>http://127.0.0.1:&lt;local port&gt;/lab?token=&lt;token&gt;\n</code></pre> <p>If you kept the local and remote ports as the same number, then you can directly copy the link that was given to you earlier:</p> <pre><code>http://127.0.0.1:8888/lab?token=7e97d59f9a17c91c11289bc5bec35ad3921725c6db55fe33\n</code></pre>","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/julia/#vs-code","title":"VS Code","text":"<p>Please refer to the VS Code page for using VS Code on the cluster.</p> <p>VS Code supports compatibility with Jupyter notebooks. If you have installed and built <code>IJulia</code> in your Julia environment, then you should be able to find the correct Julia kernel by navigating to <code>Select Kernel</code> &gt; <code>Select Another Kernel</code> &gt; <code>Jupyter Kernel</code>.</p>","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/julia/#faqs","title":"FAQs","text":"<p>I have loaded/installed a specific version of Julia, but it is not being recognized. What do I do?</p> <p>There may be another/no version of Julia in your <code>PATH</code> environment variable. You can check this by running <code>echo $PATH</code>.</p> <p>If you have loaded a Julia module but do not want to use it, you can run <code>module purge</code>. </p>","tags":["Software","Julia","vscode","Engaging","SuperCloud"]},{"location":"software/licensed/","title":"Licensed Software","text":"<p>It is possible to use licensed software on Engaging. The overall steps are:</p> <ol> <li>Obtain a license</li> <li>Host or install the license</li> <li>Install the software</li> </ol>"},{"location":"software/licensed/#step-1-obtain-a-license","title":"Step 1: Obtain a License","text":"<p>You may first want to check whether MIT, your department, or your group already has a license. MIT IS&amp;T maintains site licenses for some commonly used software, which are listed on their software grid page. Some departments have similar services.</p> <p>If you, your group, department, or MIT don't already have a license you will need to reach out to the company that owns the software to purchase one. ORCD does not purchase licenses for research software.</p> <p>Once you get a license make sure you are aware of any requirements or restrictions for the license. Some have stricter requirements than others.</p>"},{"location":"software/licensed/#step-2-host-or-install-the-license","title":"Step 2: Host or Install the License","text":"<p>Some licenses consist of a single file, those are the simplest to install. You will need to put the license file on the cluster. Once you install your software there is usually a way to let the software know where your license file is, often through an environment variable.</p> <p>For a network license you will need to have that license hosted on a server through a license manager. We recommend reaching out to IS&amp;T and asking for either a Managed Server or DLCI Server. They can work with you to set up the license manager and install the license. ORCD does not set up and maintain license servers for research software.</p>"},{"location":"software/licensed/#step-3-install-your-software","title":"Step 3: Install your Software","text":"<p>Install your licensed software following the install instructions. If you are the only one using the software you can install it in your home directory, or install it in a directory shared by your group. Do not place licensed software or license files in world-readable directories.</p> <p>Licensed software should not be world readable</p> <p>Do not place licensed software or license files in world-readable directories. Place the software in a directory only accessible by individuals covered by the license. If you need help creating a group space for sharing software reach out to orcd-help@mit.edu and we can help.</p> <p>Licensed software can sometimes be tricky to install. We have recipes for some software available on these documentation pages, the quickest way to find them is to search for the software name in the search box in the top right corder, or check the <code>#Install Recipes</code> tag on the Index page. If you need any help reach out to orcd-help@mit.edu.</p>"},{"location":"software/modules/","title":"Modules","text":"<p>Modules are a handy way to set up your environment for particular work, especially in a shared environment. They provide an easy way to load a particular version of a software, language, or compiler.</p> <p>To see what modules are available, type the command:</p> <pre><code>module avail\n</code></pre> <p>Note</p> <p>(For Engaging only) Engaging's newest hardware is running with the Rocky 8 operating system, whereas older hardware is using CentOS 7. You will see different module lists depending on whether you are on Rocky 8 or CentOS 7 node.</p> <p>By default you will only see the modules for core and community software. How to show deprecated modules will differ depending on the operating system for the node you are using.</p> Rocky 8Centos 7 <p>To show deprecated modules load the <code>deprecated-modules</code> module: <pre><code>module load deprecated-modules\n</code></pre></p> <p>Historically community modules were not separated in the Centos 7 software stack, however some newer community modules are separate. These can be seen by running the following <code>module use</code> command: <pre><code>module use /orcd/software/community/001/centos7/modulefiles\n</code></pre></p> <p>To load a module, use the command:</p> <pre><code>module load moduleName\n</code></pre> <p>Where <code>moduleName</code> can be any of the modules listed by the <code>module avail</code> command.</p> <p>Note</p> <p>We do not recommend including <code>module load</code> commands in your <code>.bashrc</code>, <code>.bash_profile</code>, or any other startup scripts and instead include them in your job scripts. This provides a more predictable and consistent environment for your jobs. It is also very easy to forget that you have modules loaded in your <code>.bashrc</code>, and these can have impact on future workloads.</p> <p>If you want to list the modules you currently have loaded, you can use the <code>module list</code> command:</p> <pre><code>module list\n</code></pre> <p>If you want to change to a different version of the module you have loaded, you can switch the module you have loaded. This is important to do when loading a different version of a module you already have loaded, the module command will not allow you to load two different versions of the same software. To switch modules run:</p> <pre><code>module switch oldModuleName newModuleName\n</code></pre> <p>where <code>oldModuleName</code> is the name of the module you currently have loaded, and <code>newModuleName</code> is the new module that you would like to load.</p> <p>If you would like to unload the module, or remove the changes the module has made to your environment, use the following command:</p> <pre><code>module unload moduleName\n</code></pre> <p>If you would like to unload all modules in your environment, you can use the command:</p> <pre><code>module purge\n</code></pre> <p>This command is helpful when you want to ensure a clean environment. You can include it at the start of your job scripts to make sure your jobs all have a consistent environment. Loaded modules may carry over into your jobs, and sometimes can interfere with the work you are doing in unexpected ways.</p>","tags":["Software","Modules"]},{"location":"software/overview/","title":"Software Overview","text":"<p>Each ORCD system has its own software stack. Many basic and commonly used software and libraries are already installed, so it is good to check before spending the time to install it yourself. This page discusses the general overview of what kinds of software are supported and points you to how to use them and what to do if what you need isn't there.</p>","tags":["Software"]},{"location":"software/overview/#software-landscape","title":"Software Landscape","text":"<p>While the software stack will be different on each system, there are three general classes of software:</p> Category Description Core Commonly used or fundamental software and libraries that are fully supported. Core software is expected to work until it is officially deprecated, and often newer versions are provided to replace them. Community Software that has been built and installed by request, but is not commonly used. Support is on a best-effort basis. These should work when built but are not guaranteed to work indefinitely or when replaced with newer versions when deprecated, except by request. Deprecated Software that is no longer supported or expected to work. May be kept for legacy reasons, or will soon be removed. If software you are using is listed as deprecated or soon to be deprecated, migrate to the newer version (if available) or request a newer version (if not available). If migrating to a newer version is not an option you may be able to run your application with Apptainer. <p>Each individual ORCD system may not label or organize their software in this way. However, this is the support model that will be used going forward. Engaging, in particular, organizes its software in this way.</p> <p>Here are some general notes by system on this for Engaging and SuperCloud. Click on the tab for the system you are interested in:</p> EngagingSuperCloud <p>Engaging nodes are one of two operating systems: Centos 7 (<code>sched_mit_hill</code> partition) and Rocky 8 (<code>mit_normal</code>, <code>mit_normal_gpu</code>, <code>mit_preemptable</code>, and <code>mit_quicktest</code> partitions). Each operating system has its own software stack.</p> <p>Centos 7 has been around for longer, so it has more software installed. These nodes have a very large list of modules, older ones that no longer work have not necessarily been removed but can be considered deprecated. Centos 7 nodes will either be retired or migrated to Rocky 8 in the near future, so when given the choice use Rocky 8 nodes.</p> <p>Rocky 8 nodes have a significantly shorter list of modules and are organized into core, community, and deprecated. Core software will be displayed by default, community and deprecated software will require a \"module use\" command to display. See the page on modules for more information.</p> <p>The SuperCloud software stack is managed by the Lincoln Laboratory Supercomputing Center. The modules listed are considered \"core\" software. Deprecated software that was part of the core stack is removed from the system. \"Community\" software is provided in the llgrid_beta directory in the groups location. Anyone can use them but they are not officially supported.</p>","tags":["Software"]},{"location":"software/overview/#steps-for-getting-software","title":"Steps for Getting Software","text":"<p>One of the first steps for getting a workflow running on a new system is to set up any software or packages needed to run it. Here are a few steps to do that on an ORCD system.</p>","tags":["Software"]},{"location":"software/overview/#check-if-the-software-or-package-is-already-installed","title":"Check if the Software or Package is Already Installed","text":"<p>As mentioned above, there is a lot of software already installed. Using the software we've installed saves you time. This software may also perform better or be better configured to use on the system. For example, it may be installed in a faster part of the filesystem or configured to use special hardware available on the cluster.</p> <p>For software check the <code>module avail</code> command (see the page on modules for more information). Some software is available without a module, you can check if a particular command is available using the <code>which</code> command at the command line. For example, run <code>which git</code> to see if the <code>git</code> command is available. If it is, the path to the <code>git</code> command will print to the screen.</p> <p>Common languages like Python, Julia, and R are provided through modules as well. Packages for these are sometimes provided along with the installation. A quick way to check if a package is available is to try to import it.</p>","tags":["Software"]},{"location":"software/overview/#install-the-software-or-package","title":"Install the Software or Package","text":"<p>If we don't have the software you need, you can often install it yourself. You will need to install them in your home directory or another directory you have access to. You will not be able to install software in any of the system-wide directories, as changes to these affect everyone using the system (for example you will not be able to install in any location that requires <code>sudo</code>).</p> Why can't I use sudo? <p>The <code>sudo</code> command is used to make system-wide administrator-level changes. On a system where you are the only user this is usually fine, the only person you can affect is yourself. On large shared systems with many users any command that uses <code>sudo</code> has the potential to affect the workflow of other researchers and potentially cause harm, even when it is not intentional. For this reason only trained system administrators have the ability to use <code>sudo</code>.</p> <p>You should not need <code>sudo</code> to install packages in your own space. For software installs, the <code>sudo</code> is only used to put the installation files in the system-wide directory, so it is not needed to install in your own directories. The Installing Software page covers how to specify installation directories for some of the more common build systems. </p> <p>Sometimes you can find pre-built binaries for the software you want. These are the easiest to install. Often you will need to build the software you need. See the page on Installing Software for more information. You may also check the Recipes section of these pages to see if there is an existing recipe for installing the software you are interested in.</p> <p>For Python, Julia, and R packages, each of these have their own package managers for installing packages. See the respective documentation pages linked above for each of these.</p>","tags":["Software"]},{"location":"software/overview/#ask-for-help","title":"Ask for Help","text":"<p>If you are having trouble installing software you can reach out to orcd-help@mit.edu or one of the other lists on Getting Help for help. You can also stop by office hours if you prefer. Depending on the software and the system you are using, we may help walk you through installing it for yourself or install it in a community location.</p>","tags":["Software"]},{"location":"software/python/","title":"Installing Python Packages","text":"<p>There are a few different ways to install Python packages. Each ORCD system has its own set of Python modules and naming conventions for those modules, along with a set of recommendations for installing Python packages. This page is meant to give a general overview and link to those pages.</p> SuperCloudOpenMind <p>SuperCloud Installing Python Packages Documentation</p> <p>OpenMind Installing Python Packages Documentation</p> <p>Python packages will need to be installed in your home directory or other directory you have write access to. There are a few different ways to do this, each with its own pros and cons. At a high level, you can:</p> <ul> <li>Install packages in your own Python virtual environment (venv)</li> <li>Install packages in your own conda/mamba environment</li> <li>Install packages to you home directory space using the <code>pip install --user</code> command</li> </ul> <p>Which should you use? That can depend on a lot of things. Our recommendation will usually depend on the system, what you are doing, and which packages your are installing. Python virtual environments tend to be a good all-around option as a starting point. With them you can stay organized with environments, but they don't tend to take up as much space or create as many files as conda or mamba environments. Read through the pros and cons for each, they are meant to help you see when one might be better than another.</p>","tags":["Software","python"]},{"location":"software/python/#modules-for-python","title":"Modules for Python","text":"<p>Python is provided on all ORCD systems through either Python, Anaconda or similar modules. See the documentation for the system your are using below for a description of the python modules available. Some systems have some commonly used Python packages installed with their modules, so it is worth checking to see if these packages will satisfy your use case before installing your own.</p> <p>Refer to the tab below to find out more about the Python modules available on the system you are using.</p> EngagingSuperCloudOpenMind <p>Some nodes on Engaging have different operating systems (OS). The newest nodes on Engaging are Rocky 8 and older nodes are Centos 7. Each OS has a different software stack, and so has different sets of Python and Anaconda modules. Both will have both Python and Anaconda modules, but will may have different names and versions. Check <code>module avail</code> for this information. Be sure the OS of the login node you are using to launch jobs matches the OS of the compute nodes you are requesting.</p> <p>We recommend using the newest miniforge modules for both. For Rocky 8 run:</p> <pre><code>module load miniforge/24.3.0-0\n</code></pre> <p>For Centos 7:</p> <pre><code>module load miniforge/24.3.0-0\n</code></pre> <p>SuperCloud releases two anaconda modules per year, named for the year and the release (anaconda/2024a and anaconda/2024b, for example). The most recent modules may continue to be updated until the next \"release\". There are separate modules for machine learning frameworks (ex: anaconda/Python-ML-2023b). SuperCloud anaconda modules contain a lot of the most commonly used packages and are installed on the local disk of each node, so it is best if you can use packages installed in the modules as much as possible. To enable that, SuperCloud recommends installing packages to you home directory space using the <code>pip install --user</code> command, or creating python virtual environments using the <code>--system-site-packages</code> flag. SuperCloud has a section on their Best Practices page about installing Python packages.</p> <p>OpenMind has both Anaconda and miniconda modules available. They have some of the most commonly used packages already installed.</p>","tags":["Software","python"]},{"location":"software/python/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>Environments allow you to make self-contained \u201cbundles\u201d of packages that can be loaded and unloaded. This helps keep a consistent set of packages and versions for a given project, rather than putting all packages you've ever installed together like they would be when you install with <code>pip install --user</code>. Python virtual environments can be placed anywhere you have write access to and have all their packages in that environment\u2019s directory structure.</p>","tags":["Software","python"]},{"location":"software/python/#creating-python-virtual-environments","title":"Creating Python Virtual Environments","text":"<p>To create a new environment, first load a Python or Anaconda module using the <code>module load</code> command. See the page on Modules for more information on how to load modules. See Modules for Python above for information about specific modules for the system you are using.</p> <p>To create a the environment use the <code>python -m venv</code> command: </p> <pre><code>python -m venv /path/to/virtual/environment\n</code></pre> <p>Note that you specify a path, and not only a name for the environment. This means you can place your environment wherever you'd like. Usually it is placed somewhere in the project directory it is used for. For example, if I want to create an environment for a project called <code>my_project</code> that lives in my home directory, I would run something like:</p> <pre><code>cd ~/my_project\npython3 -m venv my_project_env\n</code></pre> <p>This will create an environment at the path <code>~/my_project/my_project_env</code>.</p> <p>Note</p> <p>Virtual Environment documentation and tutorials will often tell you to run the command <code>python3 -m venv .venv</code> to create an environment. The <code>.</code> at the beginning of <code>.venv</code> creates a hidden directory in the current working directory. You won't see this directory if you run the <code>ls</code> command by itself, run <code>ls -a</code> to see hidden files and directories. If you are new to virtual environments and Linux we recommend using a descriptive name for your environment that is not hidden (doesn't start with <code>.</code>).</p> <p>By default environments will be fully isolated and Python will only see the packages you've installed in the environment. You can signal your virtual environment to \"see\" system installed packages by using the flag <code>--system-site-packages</code> when creating the environment. This is useful when the module you are using has packages already installed. For example:</p> <pre><code>python3 -m venv --system-site-packages my_project_env\n</code></pre> <p>Before installing packages or using an environment we need to activate the environment:</p> <pre><code>source /path/to/virtual/environment/bin/activate\n</code></pre> <p>Using our example above, if we are in the <code>my_project</code> directory already, we can run:</p> <pre><code>source my_project_env/bin/activate\n</code></pre> <p>Finally we can now install packages into the environment. Once activating the environment new packages can be installed with the <code>pip</code> command:</p> <pre><code>pip install pkgname\n</code></pre> <p>Note</p> <p>Do not use <code>--user</code> flag to install, this will install into <code>$HOME/.local</code> instead of the environment.</p> <p>When you are done using or installing packages into an environment you can deactivate it with the command:</p> <pre><code>deactivate\n</code></pre>","tags":["Software","python"]},{"location":"software/python/#using-python-virtual-environments","title":"Using Python Virtual Environments","text":"<p>In order to use an environment you will need to activate it as described above:</p> <pre><code>source /path/to/virtual/environment/bin/activate\n</code></pre> <p>Once it is activated any <code>python</code> commands will run in that environment and have access to the packages in the environment. To use an environment in a job activate the environment in your job script. An environment activated on the login node will not necessarily carry over to your job. Using the same example above, let's look at the script <code>myjob.sh</code> in the <code>my_project</code> directory:</p> myjob.sh<pre><code>#!/bin/bash\n\nsource my_project_env/bin/activate\n\npython myscript.py\n</code></pre> <p>When this job runs it will activate the environment in <code>my_project_env</code> and then run the python script <code>myscript.py</code> using the packages in that environment.</p>","tags":["Software","python"]},{"location":"software/python/#requirementstxt-and-virtual-environments","title":"Requirements.txt and Virtual Environments","text":"<p>Environments can be described by a <code>requirements.txt</code> file, which lists packages and optionally their versions. This file can be created from any existing virtual environment and used to re-create that environment. Version numbers are required to recreate the environment exactly.</p> <p>You can either create a <code>requirements.txt</code> file by hand by creating a file with one package name on each line, or you can create one from a currently active environment with the command:</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>Given a <code>requirements.txt</code> file you can use <code>pip</code> to install the packages in that file into your environment. First activate the environment, then install the packages with:</p> <pre><code>source /path/to/virtual/environment/bin/activate\npip install -r requirements.txt\n</code></pre>","tags":["Software","python"]},{"location":"software/python/#pros-and-cons-for-virtual-environments","title":"Pros and Cons for Virtual Environments","text":"<p>Pros</p> <ul> <li>Can be set to build \u201con top of\u201d the central installation packages (not default behavior, use the <code>--system-site-packages</code> flag when creating)</li> <li>Self-contained environments for each project help stay organized and avoid package dependency conflicts. For software development it allows you to keep better track of your package\u2019s dependencies so others know what they need to install.</li> <li>Virtual environments are self-contained, fairly lightweight and can be put anywhere easily without additional configuration files</li> </ul> <p>Cons</p> <ul> <li>Environments will be the same version of the python that you used to make them</li> <li>Only installs packages available through PyPI, cannot install anaconda/conda-forge distributed packages or libraries</li> </ul>","tags":["Software","python"]},{"location":"software/python/#conda-environments","title":"Conda Environments","text":"<p>Environments allow you to make self-contained \u201cbundles\u201d of packages that can be loaded and unloaded. This helps keep a consistent set of packages and versions for a given project, rather than putting them all together like they would be when you install with <code>pip install --user</code>. Conda environments are a bit different from Python virtual environments, by default they are stored in the <code>.conda</code> directory in your home directory. They also tend to contain a lot more files than Virtual Environments. You can also install some system libraries into conda environments, which can make installing packages with system library dependencies easier.</p> <p>You'll also see mention of mamba environments. Mamba and conda are nearly the same, however mamba has a different dependency solver. Mamba is often better and faster than conda at solving dependencies and picking packages, so we recommend using mamba whenever possible.</p>","tags":["Software","python"]},{"location":"software/python/#creating-conda-environments","title":"Creating Conda Environments","text":"<p>First, load a conda or Anaconda module using the <code>module load</code> command. See the page on Modules for more information on how to load modules. See Modules for Python above for information about specific modules for the system you are using.</p> <p>If there is no anaconda module on the system you are using, or the modules available aren't sufficient for your work, we recommend installing miniforge or miniconda in your home directory. Wwe have had the most  success with miniforge, which is distributed by conda-forge and is packaged with mamba. It is best to avoid installing the full Anaconda as it is very big and can fill up your home directory. One of the most common reasons for slow logins, job startups, and package imports are from a full anaconda installation in the home directory.</p> <p>Note</p> <p>If you are using SuperCloud do not install miniconda or miniforge in your home directory. SuperCloud keeps up to date anaconda modules so installing your own is not necessary and will slow down your applications. See this Best Practices page on SuperCloud-Docs for more information.</p> <p>To create an environment you can use the <code>mamba create</code> or <code>conda create</code> command after loading your conda module:</p> <pre><code>module load conda_module\nmamba create -n my_env python=3.10 pkg1 pkg2 pkg3\n</code></pre> <p>or, using <code>conda</code>:</p> <pre><code>module load conda_module\nconda create -n my_env python=3.10 pkg1 pkg2 pkg3\n</code></pre> <p>where <code>conda_module</code> is the name of the conda or mamba module for the system you are using. In this example I am creating a conda environment named <code>my_env</code> with Python 3.10 and installing packages pkg1, pkg2, pkg3. We have found that conda/mamba creates more robust environments when you include all the packages you need when you create the environment.</p> <p>You can install additional packages by activating the environment and using the <code>mamba install</code> or <code>conda install</code> command. Again you would first load the appropriate module if it isn't already loaded:</p> <pre><code>module load conda_module\nsource activate my_env\nmamba install pkg4\n</code></pre> <p>where <code>conda_module</code> is the name of the conda or mamba module for the system you are using. Here I am showing with <code>mamba</code>, replace with <code>conda</code> if you have a conda environment. Packages that aren't available through conda channels can be installed with the <code>pip</code> command when the environment is activated:</p> <pre><code>module load conda_module\nsource activate my_env\npip install pkg5\n</code></pre> <p>Note</p> <p>To install packages with <code>pip</code> to a conda or mamba environment you should not include the <code>--user</code> flag. Further, if you are using a conda environment and want Python to only use packages in your environment, you can run the following two command:</p> <pre><code>export PYTHONNOUSERSITE=True\n</code></pre> <p>This will make sure your conda environment packages will be chosen before those that may be installed in your home directory.</p> <p>If you would like to use your conda environment in Jupyter, install the \"jupyter\" package into your environment. Once you have done that, you should see your conda environment listed in the available kernels.</p>","tags":["Software","python"]},{"location":"software/python/#using-conda-environments","title":"Using Conda Environments","text":"<p>Then, whenever you want to activate the environment, first load the anaconda module, then activate with <code>source activate my_env</code>. Using <code>source activate</code>\u00a0instead of <code>conda activate</code> allows you to use your conda environment at the command line and in submission scripts without additional steps. See Conda/Mamba Init in the Troubleshooting section below for more information.</p> <pre><code>module load conda_module\nsource activate my_env\n</code></pre> <p>To use a conda environment in a job you can usually add these lines to you job script:</p> myjob.sh<pre><code>#!/bin/bash\n\nmodule load conda_module\n\nsource activate my_env\n\npython myscript.py\n</code></pre> <p>If this isn't working, see the Troubleshooting section below on activating an environment in a job script.</p>","tags":["Software","python"]},{"location":"software/python/#pros-and-cons-for-conda-environments","title":"Pros and Cons for Conda Environments","text":"<p>Pros</p> <ul> <li>Environments can be created with any supported version of Python.</li> <li>Can install packages available through various conda channels as well as PyPI (packages installed with <code>pip</code>).<ul> <li>Conda channels include many system libraries, making packages with complicated dependencies easier to install.</li> </ul> </li> <li>Self-contained environments for each project help stay organized and avoid package dependency conflicts. For software development it allows you to keep better track of your package\u2019s dependencies so others know what they need to install.</li> </ul> <p>Cons</p> <ul> <li>Can not be set to build \u201con top of\u201d the central installation packages, which means basic package will be re-installed in your home directory.</li> <li>Can get very large and take up a lot of space in your home directory.</li> <li>It can sometimes be slower than other options.</li> </ul>","tags":["Software","python"]},{"location":"software/python/#home-directory-install","title":"Home Directory Install","text":"<p>First, load a Python or Anaconda module using the <code>module load</code> command. See the page on Modules for more information on how to load modules.</p> <p>Then, install the package with pip using the <code>--user</code> flag:</p> <pre><code>pip install --user packageName\n</code></pre> <p>Where <code>packageName</code> is the name of the package that you are installing.</p> <p>With <code>pip install --user</code> you are installing a single package and any missing dependencies. Pip will see any packages already installed in the central Python installation and won\u2019t reinstall those as log as they satisfy the dependency requirements. These get installed to:</p> <pre><code>$HOME/.local/lib/pythonV.V/site-packages\n</code></pre> <p>This location is usually first in Python\u2019s package search path, so Python will pick up any libraries installed here before centrally installed ones. The exceptions are:</p> <ul> <li>If you have the <code>PYTHONPATH</code> environment variable set, that location will be searched first</li> <li>You have the <code>PYTHONNOUSERSITE</code> environment variable set to True, this tells Python to remove it from the path</li> </ul>","tags":["Software","python"]},{"location":"software/python/#pros-and-cons-for-local-install","title":"Pros and Cons for .local Install","text":"<p>Pros</p> <ul> <li>The installs are usually pretty easy</li> <li>Only installs what is absolutely needed, allowing Python to use centrally installed packages</li> </ul> <p>Cons</p> <ul> <li>Keeping and tracking a consistent environment is harder, not great for package development or working on different projects with conflicting requirements</li> <li>Everything you\u2019ve installed is always in your environment, which can cause two issues:<ul> <li>The space can eventually get \u201cdirty\u201d or \u201ccorrupted\u201d, the easiest fix is to delete or rename <code>$HOME/.local</code> and start again</li> <li>You can run into package dependency conflict issues, which could be fixed by uninstalling packages no longer needed or by deleting or renaming <code>$HOME/.local</code> and starting again</li> </ul> </li> </ul>","tags":["Software","python"]},{"location":"software/python/#troubleshooting-python-package-issues","title":"Troubleshooting Python Package Issues","text":"","tags":["Software","python"]},{"location":"software/python/#check-your-python-executable","title":"Check your Python Executable","text":"<p>When you use the <code>python</code> command Linux will pick the first <code>python</code> executable it finds in your <code>$PATH</code>. If Python is not finding your installed packages it is possible that the <code>python</code> running is not what you expect. Run the command:</p> <pre><code>which python\n</code></pre> <p>This will print out the path to the <code>python</code> executable that is running. If it doesn't print the right one, check that you've activated your environment or loaded the module you were intending.</p>","tags":["Software","python"]},{"location":"software/python/#check-pythons-path","title":"Check Python's Path","text":"<p>Python has its own path that gets set when it looks for packages. This path depends on a few things, such as whether you have an environment loaded and how that environment is configured. When in doubt you can view this path in Python with the following commands:</p> <pre><code>import sys\nsys.path\n</code></pre> <p>Similar to the <code>PATH</code> environment variable, Python checks the locations on the path in the order they are listed and imports the first of the specified package it finds. If Python is loading the wrong version of a package, checking the path will tell you where to check for the wrong-version package. If Python can't find the package, the path will give you more information about where it is looking.</p>","tags":["Software","python"]},{"location":"software/python/#condamamba-init","title":"Conda/Mamba Init","text":"<p>If you use conda or mamba you may at some point be asked to run <code>conda init</code> or <code>mamba init</code>. These commands will edit your <code>.bashrc</code> file which gets run every time you log in. These additional lines will make permanent changes to your environment that could have unintended effects on your use of the system. This can cause issues, including slowing down your logins and affecting any software builds you try to do.</p> <p>The best thing to do is to never run these commands, or if you have, to remove the lines that they  have added to your <code>.bashrc</code> file. Instead of running <code>conda activate</code> or <code>mamba activate</code>, you can use the <code>source activate</code> command.</p> <p>If this doesn't work for you, as an alternative, you can run the following line:</p> <pre><code>source /path/to/conda/install/etc/profile.d/conda.sh\n</code></pre> <p>whenever you want to use the <code>conda activate</code> command. Be sure to replace <code>/path/to/conda/install</code> with the path to the conda installation, for example if you installed miniforge in your home directory the full path might be <code>$HOME/miniforge3/etc/profile.d/conda.sh</code>.</p> <p>If you would like to use <code>mamba activate</code>, run the following line as well:</p> <pre><code>source /path/to/conda/install/etc/profile.d/mamba.sh\n</code></pre> <p>Both of these lines can be run at the command line, or put in a job script or setup script. Note that environments activated at the command line before launching a job may not carry over to the job itself, so it is always best load these environments in your job script.</p> mycondajob.sh<pre><code>#!/bin/bash\n\nsource /path/to/conda/install/etc/profile.d/conda.sh\nsource /path/to/conda/install/etc/profile.d/mamba.sh\n\nmamba activate myenv # or `conda activate myenv` for conda environments\n\npython myscript.sh\n</code></pre> <p>If you feel strongly that you want to keep the setup that <code>conda init</code> gives you in your <code>.bashrc</code>, first be aware that it could cause issues and it might be something to look into removing when troubleshooting. Second, one of the things these lines do is activate the base environment associated with the conda installation, which is usually the source of most of the issues you can run into. You can set conda to not activate the base environment at login by running the following line:</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> <p>You only need to run this line once. It will edit a configuration file for conda (.condarc). You will still be able to run <code>mamba activate</code> or <code>conda activate</code>.</p>","tags":["Software","python"]},{"location":"software/python/#environment-is-not-activating-in-a-job-script","title":"Environment is not Activating in a Job Script","text":"<p>Getting your python virtual environment or conda/mamba environment to activate in a batch job can sometimes be finicky.</p> <p>Check your log file to be sure you aren't getting any errors when you try to activate the environment. They may give further instruction on what to do.</p> <p>Usually the cause is something in your environment that is interfering, these could be:</p> <ul> <li>The <code>PYTHONPATH</code> environment variable is set- this changes where Python looks for packages first. Best practice is to not use <code>PYTHONPATH</code> when possible.</li> <li>You have another environment activated (see Conda/Mamba Init).</li> <li>Your python script starts with a line like <code>#!/bin/python</code> or similar. This tells the system to run the script with a python executable that isn't part of your environment. Removing the line will fix the issue.</li> </ul> <p>Beyond this, there are a few things you can try. These won't necessarily fix your environment issues, but might work around them. First let's talk about virtual environments, then conda environments.</p>","tags":["Software","python"]},{"location":"software/python/#activating-a-virtual-environment-in-a-script","title":"Activating a Virtual Environment in a Script","text":"<p>Usually activating the environment in your job script before running your Python script is sufficient. </p> <p>If your environment isn't activating, specifying the full path to the python executable in your environment sometimes fixes things. Here is a sample job script:</p> myjob.sh<pre><code>#!/bin/bash\n\nsource /path/to/virtual/environment/bin/activate\n\n/path/to/virtual/environment/bin/python myscript.py\n</code></pre> <p>Be sure to replace <code>/path/to/virtual/environment</code> with the actual path to your virtual environment.</p>","tags":["Software","python"]},{"location":"software/python/#activating-a-condamamba-environment-in-a-script","title":"Activating a Conda/Mamba Environment in a Script","text":"<p>Usually loading an anaconda module and then running <code>source activate myenv</code> will work to activate a conda or mamba environment in a job script (as shown above).</p> <p>One thing to try is suggested above in Conda/Mamba Init:</p> mycondajob.sh<pre><code>#!/bin/bash\n\nsource /path/to/conda/install/etc/profile.d/conda.sh\nsource /path/to/conda/install/etc/profile.d/mamba.sh #optional\n\nconda activate myenv # or `mamba activate myenv` for mamba environments\n\npython myscript.sh\n</code></pre> <p>If this doesn't work sometimes adding <code>eval \"$(conda shell.bash hook)\"</code> in the line before activating the environment will make it work:</p> mycondajob.sh<pre><code>#!/bin/bash\n\nsource /path/to/conda/install/etc/profile.d/conda.sh\nsource /path/to/conda/install/etc/profile.d/mamba.sh #optional\neval \"$(conda shell.bash hook)\"\n\nconda activate myenv # or `mamba activate myenv` for mamba environments\n\npython myscript.sh\n</code></pre>","tags":["Software","python"]}]}