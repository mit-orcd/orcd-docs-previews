{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MIT Office of Research Computing and Data Hands On Help Pages","text":"<p>The MIT Office or Research Computing and Data (ORCD) provides access and support for the compute and data needs of a wide range of research activities. These pages provide  help material for hands-on working with ORCD supported services. </p> <p>Help working with ORCD services is also available through email to orcd-help@mit.edu, please  feel free to contact us with questions and suggestions. For more information on existing documentation,  office hours, and other ways to get help, please see our Getting Help page.</p>"},{"location":"acknowledgements/","title":"Acknowledging Us","text":"<p>If have used our systems or consultation services and you would like to acknowledge the MIT Office of Research Computing an Data in your paper, we recommend adding the following to your Acknowledgments section (be sure to select the applicable resource(s) from among those in the brackets below):</p> <p>Acknowledgement Statement</p> <p>The authors acknowledge the MIT Office of Research Computing and Data for providing [high performance computing, consultation, data] resources that have contributed to the research results reported within this paper.</p> <p>Thank you for acknowledging us \u2013 we appreciate it.</p>"},{"location":"code-of-conduct/","title":"Acceptable Use and Code of Conduct","text":"<p>The ORCD systems are operated by the MIT Office of Research Computing and Data and certain appropriate common sense rules apply to working on it.</p>"},{"location":"code-of-conduct/#acceptable-use-guidelines","title":"Acceptable Use Guidelines","text":"<p>ORCD systems are intended for research associated with MIT projects or collaborations around MIT research projects. That can cover a lot of things, but all account holders are expected to use judgement and apply common sense to their use of the system. The system is not to be used to support commercial activities or for non-MIT related activities. It is not to be used for anything that might be construed as illegal or criminal. Datasets on the system must have been obtained legitimately and the system is not to be used for working with unanonymized data or data subject to ITAR or other national security  restrictions. See the Data Security and Privacy page for more information about data. If you are unsure about a planned use, please feel free to contact orcd-help@mit.edu. </p> <p>All systems are covered by MIT Institute wide policies for acceptable use of information technology, including the MITnet Rules of Use. For data practices refer to our page on Data Security and Privacy and the links on that page, particularly MIT's page on Information Protection.</p> <p>Account holders should not share accounts and should take reasonable precautions to ensure  that credentials for accessing the system (passwords, ssh keys, etc.) are kept secure.  All account holders agree to respect requests from support staff around how they use  the system. The support staff may, as needed, impose whatever policies are required to ensure the system runs well for all projects on the system. </p>"},{"location":"code-of-conduct/#code-of-conduct","title":"Code of Conduct","text":"<p>ORCD systems are shared resource used by a wide community. All people involved in its use and operations should try their utmost to be courteous and kind at all times. Members of the  ORCD community should be respectful toward one another and endeavor to ensure a  welcoming and collegial environment for all. Account holders are also expected to respect privacy of others activities on the system, and not to try to gain access to parts of the system they are not explicitly authorized to access.</p>"},{"location":"data-security/","title":"Data Security and Privacy","text":""},{"location":"data-security/#what-data-can-be-stored","title":"What data can be stored?","text":"<p>All ORCD current systems are only suitable for storing data with low-level security requirements. This means that they are not to be used to store sensitive data, such as personal information, financial information, or intellectual property. Additionally, they are not to be used to store data that is subject to use agreements that require security controls or audit tracking.</p> <p>The following data types are suitable for ORCD systems:</p> <ul> <li>Anything in the low-risk category described at the MIT IS&amp;T data risk classification pages</li> <li>Drafts of unpublished research papers and results that are based on low-risk data</li> </ul> <p>Anything else in the medium-risk or higher risk levels of the MIT IS&amp;T data risk classification pages should not be stored or analyzed on current ORCD systems.</p> <p>If you have any questions about whether your data is appropriate for storing on ORCD systems please feel free to reach out to us at orcd-help@mit.edu. </p>"},{"location":"data-security/#where-can-more-sensitive-data-be-processed-and-stored","title":"Where can more sensitive data be processed and stored?","text":"<p>The ORCD team is currently developing a system for more sensitive data. If you have sensitive data and would like to learn about our plans please feel free to get in touch at orcd-help@mit.edu.</p>"},{"location":"data-security/#support-team-access-to-orcd-system-accounts-and-resources","title":"Support team access to ORCD system accounts and resources","text":"<p>Support staff may occasionally access accounts of other users to help debug and troubleshoot problems. All access will be limited to the minimum reasonably needed to address a problem. Staff will not share any data or contents beyond the needs for providing adequate systems support and ensuring stability and security of the systems. </p>"},{"location":"getting-help/","title":"Getting Help","text":"","tags":["Getting Help"]},{"location":"getting-help/#additional-documentation","title":"Additional Documentation","text":"<p>If you haven't found your answer elsewhere in these pages, your answer may be in the documentation for the system you are using:</p> <ul> <li>Engaging Documentation</li> <li>Satori Documentation</li> <li>SuperCloud Documentation</li> <li>OpenMind Documentation</li> </ul>","tags":["Getting Help"]},{"location":"getting-help/#email","title":"Email","text":"<p>If you can't find your answer in the documentation, please use one of the email lists below to contact us. With the exception of SuperCloud, these lists will create a ticket that our team can assign and track. In all cases these mailing lists includes the entire team, so the best available person to answer your question will respond. Sending email to the entire team will also likely get you the fastest response. Please do not send email directly to individual team members.</p> <ul> <li>General ORCD Questions: orcd-help@mit.edu</li> <li>Engaging: orcd-help-engaging@mit.edu</li> <li>Satori: orcd-help-satori@mit.edu</li> <li>OpenMind: orcd-help-openmind@mit.edu</li> <li>SuperCloud: supercloud@mit.edu</li> </ul> <p>In this email, please provide, where applicable:</p> <ul> <li>Description of your issue or request</li> <li>The command that you used to launch your job</li> <li>Job ID(s)</li> <li>What you tried</li> <li>The full error message you are receiving</li> <li>Any supporting files (code, submission scripts, screenshots, etc)</li> </ul>","tags":["Getting Help"]},{"location":"getting-help/#office-hours","title":"Office Hours","text":"<p>We host weekly office hours. Office Hours are a time when you can drop in and ask us questions. It is a great time to discuss or troubleshoot something that is difficult over email. See the table below for the available Office Hours sessions.</p> Session Time Location Tuesday In Person Office Hours Tuesdays 10-11 am 46-4199 Thursday In Person Office Hours Thursdays 2-3 pm GIS and Data Lab in the Rotch Library (7-238) Friday Virtual Office Hours First, Third, and Fifth Fridays 2-3pm Zoom, email orcd-help@mit.edu for the link","tags":["Getting Help"]},{"location":"getting-started/","title":"Getting Started Tutorial","text":"<p>This page contains the most common steps for setting up and getting started with your ORCD system account. We provide this page as a convenient reference to get started. Each system has its own in-depth documentation which can be found on the ORCD Systems page.</p> <p>Sections that are system-specific will be shown under a list of tabs. Click on the tab for the system you are using and the rest of the page will show the information for that system.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#getting-an-account","title":"Getting an Account","text":"<p>If you don't already have an account, click on the tab for the system you are interested in and follow the instructions.</p> EngagingSatoriSuperCloudOpenMind <p>Login into the respective OnDemand Portal https://engaging-ood.mit.edu using your MIT kerberos credentials. The system will then be prompted to create your account automatically. Wait couple minutes for the system to create all the pieces for your account before submitting your first job.</p> <p>Login into the respective OnDemand Portal https://satori-portal.mit.edu/ using your MIT kerberos credentials. The system will then be prompted to create your account automatically. Wait couple minutes for the system to create all the pieces for your account before submitting your first job.</p> <p>Follow the instructions on the Account Request Page.</p> <p>OpenMind will be available to the general MIT Community starting 2024. Those currently eligible can follow the instructions on the Getting an Account page.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#logging-in","title":"Logging In","text":"<p>The first thing you should do when you get a new account is verify that you can log in. The different ORCD systems provide multiple ways to log in, including both ssh and web portals. Links to instructions for the different systems are below.</p> EngagingSatoriSuperCloudOpenMind <p>See the Logging into Engaging page for full documentation.</p> <p>See the Logging into Satori page for full documentation.</p> <p>See the Logging into SuperCloud page for full documentation.</p> <p>See the Logging into OpenMind page for full documentation.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#terminal-with-ssh","title":"Terminal with SSH","text":"EngagingSatoriSuperCloudOpenMind <p>Engaging has four login nodes: <code>eofe7</code>, <code>eofe8</code>, <code>eofe9</code>, and <code>eofe10</code>. Replace <code>USERNAME</code> below with your Kerberos username and use the login node you would like to log in with, the example below is using <code>eofe10</code>.</p> <pre><code>ssh USERNAME@eofe10.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password.  You can add an ssh key if you do not want to enter your Kerberos password at login. <code>eofe9</code> and <code>eofe10</code> also require Two-Factor Authentication.</p> <p>Satori has two login nodes: <code>satori-login-001</code> and <code>satori-login-002</code>. Replace <code>USERNAME</code> below with your Kerberos username and use the login node you would like to log in with, the example below is using <code>satori-login-001</code>.</p> <pre><code>ssh USERNAME@satori-login-001.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password. You can add an ssh key if you do not want to enter your Kerberos password at login.</p> <p>In order to log into SuperCloud with ssh you will need to add ssh keys to your account on the Web Portal. Follow the instructions on the SuperCloud Getting Started page to add your keys.</p> <p>Then you can log in with ssh using the following command, where <code>USERNAME</code> is your username on the MIT SuperCloud system:</p> <pre><code>ssh USERNAME@txe1-login.mit.edu\n</code></pre> <p>Log into OpenMind with the following command in a terminal window. Replace <code>USERNAME</code> below with your Kerberos username.</p> <pre><code>ssh USERNAME@openmind.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password. You can add an ssh key if you do not want to enter your Kerberos password at login.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#web-portal","title":"Web Portal","text":"EngagingSatoriSuperCloudOpenMind <p>You can log into OnDemand Web Portal with the link: https://engaging-ood.mit.edu. For full detailed instructions please see the Engaging Documentation.</p> <p>You can log into the OnDemand Web Portal with the link: https://satori-portal.mit.edu. For full detailed instructions please see the Satori Documentation.</p> <p>You can log into the SuperCloud Web Portal with the link: https://txe1-portal.mit.edu. For full detailed instructions please see the SuperCloud Documentation.</p> <p>OpenMind does not currently have a web portal, but there are plans to add one in the future. Check back, and in the meantime check out OpenMind's documentation on the FastX Remote Desktop. You may find it provides what you are looking for.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#shared-hpc-clusters","title":"Shared HPC Clusters","text":"<p>Each ORCD system is a shared HPC cluster. You are sharing this resources with a number of other researchers, staff, and students so it is important that you read this page and use the system as intended.</p> <p>Being a cluster, there are several machines connected together with a network. We refer to these as nodes. Most nodes in the cluster are referred to as compute nodes, this is where the computation is done on the system (where you will run your code). When you ssh into the system you are on a special purpose node called the login node. The login node, as its name suggests, is where you log in and is for editing code and files, installing packages and software, downloading data, and starting jobs to run your code on one of the compute nodes.</p> <p>Each job is started using a piece of software called the scheduler, which you can think of as a resource manager. You let it know what resources you need and what you want to run, and the scheduler will find those resources and start your job on them. When your job completes those resources are relinquished. The scheduler is what ensures that no two jobs are using the same resources, so it is very important not to run anything unless it is submitted properly through the scheduler.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#software-and-packages","title":"Software and Packages","text":"<p>The first thing you may want to do is make sure the system has the software and packages you need. We have installed a lot of software and packages on the system already, even though it may not be immediately obvious that it is there. Review the page for the system you are using paying particular attention to the section on modules and installing packages for the language that you use:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging Software Documentation Page</p> <p>Satori Software Documentation Page</p> <p>SuperCloud Software Documentation Page</p> <p>OpenMind Software Documentation Page</p> <p>If you are ever unsure if we have a particular software, and you cannot find it, please send us an email and ask before you spend a lot of time trying to install it. If we have it, we can point you to it, provide advice on how to use it, and if we don't have it we can often give pointers on how to install it. Further, if a lot of people request the same software, we may consider adding it to the system image.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#linux-command-line","title":"Linux Command Line","text":"<p>Every ORCD system runs Linux, so much of what you do on the cluster involves the Linux command line. That doesn't mean you have to be a Linux expert to use the system! However the more you can get comfortable with the Linux command line and a handful of basic commands, the easier using the system will be. If you are already familiar with Linux, feel free to skip this section, or skim as a refresher.</p> <p>Most Linux commands deal with directories and files. A directory, synonymous to a folder, contains files and other directories. The list of directories that lead to a particular directory or file is called its path. In Linux, directories on a path are separated by forward slashes <code>/</code>. It is also important to note that everything in Linux is case sensitive, so a file <code>myScript.sh</code> is not the same as the file <code>myscript.sh</code>. When you first log in you are in you home directory. Your home directory is where you can put all the code and data you need to run your job. Your home directory is not accessible to other users, if you need a space to share files with other users, let us know and we can make a shared group directory for you.</p> EngagingSatoriSuperCloudOpenMind <p>The path to your home directory on Satori is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on Satori is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on SuperCloud is <code>/home/gridsan/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on OpenMind is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>Anytime after you start typing a Linux command you can press the \"Tab\" button your your keyboard. This called tab-complete, and will try to autocomplete what you are typing. This is particularly helpful when typing out long directory paths and file names. Pressing \"Tab\" once will complete if there is a single completion, pressing it twice will list all potential completions. It is a bit difficult to explain in text, but you can try it out yourself and watch the short demonstration here.</p> <p>Finally, click on the box below for a list of Linux Commands. If you are new to Linux try them out for yourself at the command line.</p> Common Linux Commands <ul> <li>Creating, navigating and viewing directories:<ul> <li><code>pwd</code>: tells you the full path of the directory you are     currently in</li> <li><code>mkdir dirname</code>: creates a directory with the name \"dirname\"</li> <li><code>cd dirname</code>: change directory to directory \"dirname\"<ul> <li><code>cd ../</code>: takes you up one level</li> </ul> </li> <li><code>ls</code>: lists the files in the directory<ul> <li><code>ls -a</code>: lists all files including hidden files</li> <li><code>ls -l</code>: lists files in \"long format\" including ownership     and date of last update</li> <li><code>ls -t</code>: lists files by date stamp, most recently updated     file first</li> <li><code>ls -tr</code>: lists files by dates stamp in reverse order, most     recently updated file is listed last (this is useful if you     have a lot of files, you want to know which file you changed     last and the list of files results in a scrolling window)</li> <li><code>ls dirname</code>: lists the files in the directory \"dirname\"</li> </ul> </li> </ul> </li> <li>Viewing files<ul> <li><code>more filename</code>: shows the first part of a file, hitting the     space bar allows you to scroll through the rest of the file, q     will cause you to exit out of the file.</li> <li><code>less filename</code>: allows you to scroll through the file, forward     and backward, using the arrow keys.</li> <li><code>tail filename</code>: shows the last 10 lines of a file (useful when     you are monitoring a log file or output file to see that the     values are correct)<ul> <li>t<code>ail &lt;number&gt; filename</code>: show you the last &lt;number&gt;     lines of a file.</li> <li><code>tail -f filename</code>: shows you new lines as they are written     to the end of the file. Press CMD+C or Control+C to exit.     This is helpful to monitor the log file of a batch job.</li> </ul> </li> </ul> </li> <li>Copying, moving, renaming, and deleting files<ul> <li><code>mv filename dirname</code>: moves filename to directory dirname.<ul> <li><code>mv filename1 filename2</code>: moves filename1 to filename2, in     essence renames the file. The date and time are not changed     by the mv command.</li> </ul> </li> <li><code>cp filename dirname</code>: copies to directory dirname.<ul> <li><code>cp filename1 filename2</code>: copies filename1 to filename2. The     date stamp on filename2 will be the date/time that the file     was moved</li> <li><code>cp -r dirname1 dirname2</code>: copies directory dirname1 and its     contents to dirname2.</li> </ul> </li> <li><code>rm filename</code>: removes (deletes) the file</li> </ul> </li> </ul>","tags":["Getting Started","Linux"]},{"location":"getting-started/#transferring-files","title":"Transferring Files","text":"<p>One of the first tasks is to get your code, data, and any other files you need into your home directory on the system. If your code is in github you can use git commands on the system to clone your repository to your home directory. You can also transfer your files to your home directory from your computer by using the commands <code>scp</code> or <code>rsync</code>. Read the page on Transferring Files for the system you are using to learn how to use these commands and transfer what you need to your home directory.</p> <p>You can use <code>scp</code> or <code>rsync</code> from the command line on your local computer for any ORCD system. Both commands work similarly to the <code>cp</code> command, following the pattern <code>&lt;command&gt; &lt;source&gt; &lt;destination&gt;</code>, the only difference being that you will need to include the hostname of the system you are transferring to or from. For this reason you must run this command from the terminal on your computer before you've logged in.</p> <p>To transfer a file from your computer to the ORCD system:</p> EngagingSatoriSuperCloudOpenMind <pre><code>scp &lt;file-name&gt; USERNAME@eofe8.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp &lt;local-file-name&gt; USERNAME@satori-login-001:&lt;path-to-satori-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp &lt;local-file-name&gt; USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; &lt;user&gt;@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <p>To transfer a file from an ORCD system to your computer:</p> EngagingSatoriSuperCloudOpenMind <pre><code>scp USERNAME@eofe8.mit.edu:&lt;path-to-engaging-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp USERNAME@satori-login-001:&lt;path-to-satori-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <pre><code>scp &lt;user&gt;@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>Similar to <code>cp</code>, use the <code>-r</code> flag to copy over an entire directory and its contents. </p> EngagingSatoriSuperCloudOpenMind <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@eofe8.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@satori-login-001:&lt;path-to-satori-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-dir-name&gt; &lt;user&gt;@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <p>The <code>rsync</code> command can be used similarly and has some additional flags you can use. It also can be used to transfer only new or modified files to the destination, which makes it easy to keep a directory in \"sync\".</p> <p>For more information on transferring files and additional methods please see the documentation page for your system:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging Transferring Files Documentation Page Coming Soon</p> <p>Satori Transferring Files Documentation Page</p> <p>SuperCloud Transferring Files Documentation Page</p> <p>OpenMind Transferring Files Documentation Page</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#running-your-first-job","title":"Running your First Job","text":"<p>At this point you may want to do a test-run of your code. You always want to start small in your test runs, so you should choose a small example that tests the functionality of what you would ultimately like to run on the system. If your test code is serial and runs okay on a moderate personal laptop or desktop you can request an interactive session to run your code in by executing the command:</p> EngagingSatoriSuperCloudOpenMind <pre><code># Requesting a single core for an interactive job for 1 hour\nsrun -n 1  -t 01:00:00 --pty /bin/bash\n</code></pre> <pre><code>#Requesting a single core for an interactive job for 1 hour\nsrun -n 1  -t 01:00:00 --pty /bin/bash\n</code></pre> <pre><code># Requesting a single core for an interactive job\nLLsub -i\n</code></pre> <pre><code># Requesting a single core for an interactive job for 1 hour\nsrun -n 1 -t 01:00:00  --pty bash  \n</code></pre> <p>After you run this command you will be on a compute node and you can do a test-run of your code. This command will allocate one core to your job. If your test code is multithreaded or parallel, uses a lot of memory, or requires a GPU you should request additional resources as needed. Not requesting the resources you will be using can negatively impact others on the system.</p> <p>Please see your system's documentation pages for more information on requesting more resources for running interactive jobs, and how to run batch jobs.</p> EngagingSatoriSuperCloudOpenMind <p>Engaging's Documentation for Running Jobs</p> <p>Satori's Documentation for Running Jobs</p> <p>SuperCloud's Documentation for Running Jobs</p> <p>OpenMind's Documentation for Running Jobs</p>","tags":["Getting Started","Linux"]},{"location":"orcd-systems/","title":"ORCD Systems","text":"<p>ORCD operates and provides support and training for a number of cluster computer systems available to all researchers. These systems all run with a Slurm scheduler and most have a web portal for interactive computing. These are Engaging, SuperCloud, Satori, and OpenMind.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#maintenance-schedule","title":"Maintenance Schedule","text":"<p>With the exception of SuperCloud, the maintenance schedule for all ORCD systems is:</p> <ul> <li>Monthly downtimes on the 3rd Tuesday of the month lasting about a day.</li> <li>Weekly restarts of login nodes Monday mornings starting at 7am for about 15 minutes. If Monday is a holiday this restart will occur on Tuesday.</li> </ul> <p>SuperCloud has monthly downtimes on the 2nd Tuesday of each month.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-cluster","title":"Engaging Cluster","text":"<p>The Engaging cluster is a mixed CPU and GPU computing cluster that is openly available to all  research projects at MIT. It has around 80,000 x86 CPU cores and 300  GPU cards ranging from K80 generation to recent Voltas. Hardware access is through the Slurm  resource scheduler that supports batch and interactive workloads and allows dedicated reservations. The cluster has a large shared file system for working datasets. Additional compute and storage  resources can be purchased by PIs. A wide range of standard software is available and the Docker  compatible Singularity container tool is supported. User-level tools like Anaconda for Python,  R libraries, and Julia packages are all supported. A range of PI group maintained custom software  stacks are also available through the widely adopted environment modules toolkit. A standard,  open-source, web-based portal supporting Jupyter notebooks, R studio, Mathematica, and X graphics  is available at https://engaging-ood.mit.edu. Further information and support is available from orcd-help-engaging@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-engaging","title":"How to Get an Account on Engaging","text":"<p>Accounts on the engaging cluster are connected to your main MIT institutional kerberos id.  Connecting to the cluster for the first time through its web portal automatically activates an account with basic access to resources. See this page for instructions on how to log in.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-quick-links","title":"Engaging Quick Links","text":"<ul> <li>Documentation: https://engaging-web.mit.edu/eofe-wiki/</li> <li>OnDemand web portal: https://engaging-ood.mit.edu</li> <li>Help: Send email to orcd-help-engaging@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori","title":"Satori","text":"<p>Satori is an IBM Power 9 large memory node system. It is open to everyone on campus and has  optimized software stacks for machine learning and for image stack post-processing for  MIT.nano Cryo-EM facilities. The system has 256 NVidia Volta GPU cards attached in groups of  four to 1TB memory nodes and a total of 2560 Power 9 CPU cores. Hardware access is through the  Slurm resource scheduler that supports batch and interactive workloads and allows dedicated  reservations. A wide range of standard software is available and the Docker compatible  Singularity container tool is supported. A standard web based portal  https://satori-portal.mit.edu with Jupyter notebook support is available. Additional compute and storage resources can be purchased by PIs and integrated into the system. Further  information and support is available at orcd-help-satori@mit.edu</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-satori","title":"How to Get an Account on Satori","text":"<p>You can get an account by logging into https://satori-portal.mit.edu with your MIT credentials. This automatically activates an account with basic access to resources. See this page for more information.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori-quick-links","title":"Satori Quick Links","text":"<ul> <li>Documentation: https://mit-satori.github.io/</li> <li>OnDemand web portal: https://satori-portal.mit.edu</li> <li>Help: Send email to orcd-help-satori@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud","title":"SuperCloud","text":"<p>The SuperCloud system is a collaboration with MIT Lincoln Laboratory on a shared facility that  is optimized for streamlining open research collaborations with Lincoln Laboratory. The facility  is open to everyone on campus. The latest SuperCloud system has more than 16,000 x86 CPU cores  and more than 850 NVidia Volta GPUs in total. Hardware access is through the Slurm resource  scheduler that supports batch and interactive workloads and allows dedicated reservations. A wide  range of standard software is available and the Docker compatible Singularity container tool is  supported. User-level tools like Anaconda for Python, R libraries, and Julia packages are all supported. A custom, web-based portal supporting Jupyter notebooks is available at https://txe1-portal.mit.edu/. Further information and support is available at supercloud@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-supercloud","title":"How to Get an Account on SuperCloud","text":"<p>To request a SuperCloud account follow the instructions on SuperCloud's Requesting an Account page.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud-quick-links","title":"SuperCloud Quick Links","text":"<ul> <li>Documentation: https://supercloud.mit.edu/</li> <li>Online Course: https://learn.llx.edly.io/course/practical-hpc/</li> <li>Web portal: https://txe1-portal.mit.edu/</li> <li>Help: Send email to supercloud@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#openmind","title":"OpenMind","text":"<p>The OpenMind system is a collaboration with Department of Brain and Cognitive Sciences (BCS) and McGovern Institute. OpenMind is mainly a GPU computing cluster optimized for artificial intelligence (AI) research and data science. Totally there are around 70 compute nodes, 3500 CPU cores, 48 TB of RAM, and 340 GPUs, including 142 A100-80GB GPUs. It also provides around 2 PB of flash storage supporting fast read/write data speed. Hardware access is through the Slurm resource scheduler that supports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and Docker compatible Apptainer/Singularity container tool is supported. User-level tools like Anaconda for Python, R libraries, and Julia packages are all supported. Further information and support is available at orcd-help-openmind@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-openmind","title":"How to Get an Account on OpenMind","text":"<p>Accounts will be available for MIT users in 2024.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#openmind-quick-links","title":"OpenMind Quick Links","text":"<ul> <li>Documentation: https://github.mit.edu/MGHPCC/OpenMind/wiki</li> <li>Home Page and Online Course: https://openmind.mit.edu/</li> <li>Help: Send email to orcd-help-openmind@mit.edu</li> <li>Slack: https://openmind-46.slack.com</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"filesystems-file-transfer/filesystems/","title":"General Use Filesystems","text":"<p>Large HPC systems often have different filesystems for different purposes. ORCD systems are no different, and each have their own approach. This page documents these.</p>"},{"location":"filesystems-file-transfer/filesystems/#engaging","title":"Engaging","text":"<p>Users each get a small home directory that is backed up and meant for important files. Larger scratch space is not backed up. Additional storage can be purchased. The Lustre scratch space will be faster than NFS for the majority of workloads, however having large numbers of small files will make it slower than NFS and can slow down the filesystem overall, so it is important to follow the Lustre Best Practices. See the Engaging Documentation Page on Storage for more information.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory  NFS <code>/home/&lt;username&gt;</code> 100 GB Backed up Use for important files Lustre <code>/nobackup1/&lt;username&gt;</code> 1 TB Not backed up Scratch space  Faster than NFS NFS <code>/pool001/&lt;username&gt;</code> 1 TB Not backed up Scratch space"},{"location":"filesystems-file-transfer/filesystems/#satori","title":"Satori","text":"Storage Type Path Quota Backed up Purpose/Notes Home Directory <code>/home/&lt;username&gt;</code> 25GB Backed up Use for important files. Quota increase request to 100GB. GPFS <code>/nobackup/users/&lt;username&gt;</code> 500GB Not backed up Scratch space. Quota increase request to 2TB."},{"location":"filesystems-file-transfer/filesystems/#supercloud","title":"SuperCloud","text":"<p>SuperCloud uses Lustre for all central/shared storage (accessible to all nodes in the system). This storage is not backed up. See the SuperCloud Best Practices and Performance Tips page for best practices using the Lustre filesystem. Quotas or limits are set on the storage as guardrails. Individual and group storage use and quotas can been viewed on the User Profile Page on the SuperCloud Web Portal (only accessible if you have an account). Additional storage may be granted on a case by case basis. Local disk spaces will be faster than the Lustre shared filesystem, but all are temporary and can only be accessed on the node where they are created.</p> Storage Type Path Access Backed up Limits Home Directory  Lustre <code>/home/gridsan/&lt;username&gt;</code> User only Not backed up See User Profile Page Group Directories  Lustre <code>/home/gridsan/groups/&lt;groupname&gt;</code> Files shared within a group Not backed up See User Profile Page Job-specific Temporary Storage  Local Disk Access using the <code>$TMPDIR</code> environment variable User or Group Not backed up   Temporary directory created at the start of a job and cleaned up at the end of the job NA Local Disk Space Create the directory <code>/state/partition1/user/$USER</code> as needed User or Group Not backed up  Cleaned up monthly during downtimes NA"},{"location":"filesystems-file-transfer/filesystems/#openmind","title":"OpenMind","text":"<p>OpenMind provides a number of different storage options. See the OpenMind Documentation page on Storage for more information, best practices, and recommendations.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory <code>/home/&lt;username&gt;</code> 20 GB Backed up Use for very important files. Physically located on Flash 2. Flash 1 <code>/om/user/&lt;username&gt;</code> (individual users) and <code>/om/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Flash 1 Scratch <code>/om/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 3 weeks after creation Scratch space Flash 2 <code>/om2/user/&lt;username&gt;</code> (individual users) and <code>/om2/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Flash 2 Scratch <code>/om2/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 2 weeks after creation Scratch space NFS <code>/om3</code>, <code>/om4</code>, <code>/om5</code> Per group Backed up Slow internal long-term storage NESE <code>/nese</code> Per group Backed up Slow internal long-term storage"},{"location":"filesystems-file-transfer/project-filesystems/","title":"Project Specific Filesystems","text":""},{"location":"filesystems-file-transfer/project-filesystems/#purchasing-storage","title":"Purchasing Storage","text":"<p>Additional project and lab storage can be purchased on ORCD shared clusters by individual PI groups. This storage is mounted on the cluster and access to the storage is managed  by the group through MIT Web Moira (see below for details).</p> <p>The current options for storage are:</p> Storage Type Description Encryption at Rest Backup Namespace Notes Compute Very frequent data access Optional No Limited Very fast access, special needs, high IO Data Frequent data access Optional No Limited Day to day research storage, active projects, instrument data buffers, etc. Archival Infrequent data access Yes No Nearly Unlimited Infrequently accessed data, unlimited namespace <p>Please note that all types of storage are not backed up by default.</p> <p>Storage is charged at the start of each fiscal year. The first year is prorated by the number of months left in the current fiscal year. A purchase must be a minimum of 20 TiB and in increments of 20 TiB.</p> <p>If you anticipate needing more than a few 100 TiB let us know when you request your storage. We may suggest purchasing a dedicated server for your lab.</p> <p>For more information, including pricing, and to purchase storage please send an email to orcd-help@mit.edu. If you are purchasing storage please include the following in your request:</p> <ul> <li>The storage type (compute, data, or archival)</li> <li>Amount in TiB (20 TiB increments)</li> <li>Cost object</li> <li>The lab PI</li> </ul>"},{"location":"filesystems-file-transfer/project-filesystems/#managing-access-using-mit-web-moira","title":"Managing access using MIT Web Moira","text":"<p>Individual group storage is configured so that access is limited to a set of accounts belonging to a web moira list that is defined for the group store. The owner and administrators of group storage can manage access themselves, by modifying the membership of an associated moira list under https://groups.mit.edu/webmoira/list/[list_name]. The name of the list corresponds to the UNIX group name associated with the ORCD shared  cluster storage.</p>"},{"location":"filesystems-file-transfer/project-filesystems/#moira-web-interface-example","title":"Moira Web Interface Example","text":"<p>The figure below shows a screenshot of the web moira management page at https://groups.mit.edu/webmoira/list/cnh_research_computing for a hypothetical storage group named <code>cnh_research_computing</code>. The interface provides a  self-service mechanism for controlling access to any storage belonging to this group. MIT account IDs can be added and  removed as needed from this list by the storage access administrators.</p> <p></p>"},{"location":"filesystems-file-transfer/transferring-files/","title":"Transferring Files","text":"<p>There are a few different ways to transfer files depending on your goals, the data you are transferring, and what you are comfortable with. On this page we cover the different methods of transferring files, as well as touch on how to transfer files between systems.</p> <p>For most of these options you will need to know the hostname of the node where you will be transferring files. This is often a login node, but may also be a dedicated data transfer node. Select the system you are using to see options for the hostname here:</p> EngagingSatoriOpenMindSuperCloud <ul> <li><code>eofe4.mit.edu</code></li> <li><code>eofe9.mit.edu</code></li> <li><code>eofe10.mit.edu</code></li> </ul> <ul> <li><code>satori-login-001.mit.edu</code></li> <li><code>satori-login-002.mit.edu</code></li> </ul> <ul> <li><code>openmind-dtn.mit.edu</code></li> </ul> <ul> <li><code>txe1-login.mit.edu</code></li> </ul> <p>For more information specific to the system you are using, you can consult your system's documentation here:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging does not have an additional documentation page.</p> <p>Satori Transferring Files Documentation Page</p> <p>SuperCloud Transferring Files Documentation Page</p> <p>OpenMind Transferring Files Documentation Page</p>"},{"location":"filesystems-file-transfer/transferring-files/#transferring-files-with-the-command-line","title":"Transferring Files with the Command Line","text":"<p>Two of the most common commands used to transfer files are <code>scp</code> and <code>rsync</code>. You will need to run both of these commands from your local computer, before logging into any ORCD system. In order to use these two command you will need:</p> <ul> <li>The hostname of the remote machine you are transferring files or from (usually the login node)</li> <li>The path on the remote machine where you are copying the file to or from</li> <li>To be able to ssh to the remote machine where you are transferring files to or from</li> </ul> <p>Both <code>scp</code> and <code>rsync</code> work similar to <code>cp</code>, in that you specify a source (where the file is coming from) and destination (where the file is going to).</p> <p>Unless you have your paths memorized, the easiest way to do this is to have two terminals open. One logged into the ORCD system you are transferring files to or from, one not logged in. In each navigate to the respective directories where the file exists or you want to transfer it to. In the local tab navigate to where you want to put the transferred file or to to the file you want to transfer. In the ORCD system tab use the <code>pwd</code> command to print out the path to your current location. You can use this to run the <code>scp</code> or <code>rsync</code> command.</p>"},{"location":"filesystems-file-transfer/transferring-files/#scp","title":"scp","text":"<p>First, open a terminal on your computer (not logged into any ORCD system).</p> <p>To transfer a file from your local computer to an ORCD system you would use the command:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp &lt;file-name&gt; USERNAME@eofe9.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; USERNAME@satori-login-001.mit.edu:&lt;path-to-satori-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; USERNAME@txe1-login.mit.edu:&lt;path-to-supercloud-dir&gt;\n</code></pre> <p>For example, let's say you have the local file <code>myscript.py</code> and you want to transfer it to the directory <code>mycode</code> in your home directory. The command would be:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp myscript.py USERNAME@eofe9.mit.edu:/home/USERNAME/mycode/\n</code></pre> <pre><code>scp  myscript.py USERNAME@satori-login-001.mit.edu:/home/USERNAME/mycode/\n</code></pre> <pre><code>scp  myscript.py USERNAME@openmind-dtn.mit.edu:/home/USERNAME/mycode/\n</code></pre> <pre><code>scp myscript.py USERNAME@txe1-login.mit.edu:/home/gridsan/USERNAME/mycode/\n</code></pre> <p>To transfer the other direction (from an ORCD system to your local computer) switch the order:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp USERNAME@eofe9.mit.edu:&lt;path-to-engaging-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <pre><code>scp USERNAME@satori-login-001.mit.edu:&lt;path-to-satori-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <pre><code>scp USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <pre><code>scp USERNAME@txe1-login.mit.edu:&lt;path-to-supercloud-file&gt; &lt;path-to-local-dir&gt;\n</code></pre> <p>If you were to have the file <code>results.csv</code> that you want to copy from the <code>output</code> directory in your home directory to the current directory on your computer the command would be:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp USERNAME@eofe9.mit.edu:/home/USERNAME/output/results.csv .\n</code></pre> <pre><code>scp USERNAME@satori-login-001.mit.edu:/home/USERNAME/output/results.csv .\n</code></pre> <pre><code>scp USERNAME@openmind-dtn.mit.edu:/home/USERNAME/output/results.csv .\n</code></pre> <pre><code>scp USERNAME@txe1-login.mit.edu:/home/gridsan/USERNAME/output/results.csv .\n</code></pre> <p>Note the <code>.</code> in the command above means the current directory.</p> <p>Similar to the <code>cp</code> command, if you want to transfer an entire directory and all of its subdirectories, use the <code>-r</code> (recursive) flag for either direction:</p> EngagingSatoriOpenMindSuperCloud <pre><code>scp -r &lt;file-name&gt; USERNAME@eofe9.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-file-name&gt; USERNAME@satori-login-001.mit.edu:&lt;path-to-satori-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-file-name&gt; USERNAME@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-file-name&gt; USERNAME@txe1-login.mit.edu:&lt;path-to-supercloud-dir&gt;\n</code></pre>"},{"location":"filesystems-file-transfer/transferring-files/#rsync","title":"rsync","text":"<p>The use of <code>rsync</code> is very similar to <code>scp</code>, but the behavior is different. By default <code>rsync</code> will not transfer files that are identical at both the source and destination. There are additional flags you can use to specify what <code>rsync</code> should do when files differ. The <code>rsync</code> command can be very useful when you want to \"sync\" updates to a directory or when transferring large directories. If a transfer fails during <code>rsync</code> you can re-run the command and it will pick up where it left off, rather than re-transfer everything.</p> <p>For general use, the example commands above for <code>scp</code> apply, use the same command but replace <code>scp</code> with <code>rsync</code>.</p> <p>Some useful flags include:</p> <ul> <li><code>-r</code>, <code>--recursive</code> to recursively copy files in all sub-directories</li> <li><code>-l</code>, <code>--links</code> to copy and retain symbolic links</li> <li><code>-u</code>, <code>--update</code> skips any files for which the destination file already exists and has a date later than the source file</li> <li><code>-v</code>, <code>--verbose</code> prints out more information during the file transfer, add more <code>v</code>s for more information</li> <li><code>--partial</code> keeps partially transferred files, useful when transferring large files so rsync can continue where it left off if the transfer fails</li> <li><code>--progress</code> prints information about the progress of the transfer</li> <li><code>-n</code>, <code>--dry-run</code> does not run the transfer but prints out what actions it would be taken, useful to avoid unintended file overwrites</li> </ul> <p>You can run <code>rsync --help</code> to print out a full list of flags that can be used with the <code>rsync</code> command.</p>"},{"location":"filesystems-file-transfer/transferring-files/#moving-files-between-orcd-systems","title":"Moving files between ORCD Systems","text":"<p>If you need to move files between ORCD systems you can do so one of two ways.</p> <ol> <li>ssh to one of the ORCD systems and initiate the transfer from that system to the other. Once you are logged into one system the process is the same as if you were to transfer files to or from your own computer.</li> <li>Run the <code>scp</code> or <code>rsync</code> command on your local system and specify the hostnames and paths for each of the source and destination systems. For example to move a file from Engaging to Satori using <code>scp</code> you would run:</li> </ol> Transferring files from Engaging to Satori<pre><code>scp USERNAME@eofe9.mit.edu:&lt;path-to-engaging-file&gt; USERNAME@satori-login-001.mit.edu:&lt;path-to-satori-dir&gt;\n</code></pre>"},{"location":"filesystems-file-transfer/transferring-files/#graphical-applications-for-file-transfer","title":"Graphical Applications for File Transfer","text":"<p>There are a few applications you can download that will allow you to transfer files with  drag-and-drop, similar to how you would move files around on your own computer.</p> <p>Some of the most common options are:</p> <ul> <li>Cyberduck (Mac and Windows)</li> <li>FileZilla (Mac, Windows, and Linux)</li> <li>WinSCP (Windows only)</li> </ul> <p>To use these you will need to know the hostname of the ORCD system you are accessing, either one of the login nodes or a dedicated data transfer node. See the list of hostnames at the top of this page to see which you should use for the system you are transferring files to.</p>"},{"location":"filesystems-file-transfer/transferring-files/#transferring-files-with-a-web-portal","title":"Transferring Files with a Web Portal","text":"<p>Most ORCD systems have some form of portal that can be accessed through your browser and used to transfer or download files. Engaging and Satori both use OnDemand. SuperCloud has its own custom portal.</p> <ul> <li>Engaging OnDemand Portal</li> <li>Satori OnDemand Portal</li> <li>SuperCloud Web Portal (Documentation)</li> </ul> <p>For documentation on how to download and transfer files on the SuperCloud Web Portal, see the link above.</p> <p>If you are using Engaging or Satori, you can use the file browser by selecting Files -&gt; Home Directory in the menu bar at the top of the page. You can drag and drop files into and out of this page or use the \"Upload\" and \"Download\" buttons. Select multiple files by holding the Control (or Command) key and clicking on the files you'd like to select. Those files can then be downloaded with the \"Download\" button.</p>"},{"location":"filesystems-file-transfer/transferring-files/#globus","title":"Globus","text":"<p>Globus is a tool that helps transfer data between designated endpoints. These transfers can be initiated through the Globus webpage, don't require staying logged in through the entire transfer, and will restart automatically if something fails during the transfer. There are endpoints on a few ORCD systems with basic Globus features. Please note that these basic Globus endpoints will transfer data unencrypted. An MIT Globus subscription with more features is coming soon!</p> <p>To transfer data log into Globus with your MIT credentials. On the \"File Manager\" tab in one of the two \"Collection\" boxes search for the endpoint for the system you want to transfer data to or from. The column on the left should list where you want to transfer from, the column on the right should list where you want to transfer to. Endpoints on ORCD systems are listed below.</p> System Globus Endpoint Engaging mithpc#engaging Satori mithpc#satori OpenMind mithpc#openmind <p>To transfer data to or from your own computer you will need to set up a personal endpoint. Follow the instructions on the page for your system listed here.</p> <p>More documentation on transferring files through Globus can be found on the Globus Documentation Pages. Globus also has an FAQ that is helpful for answering any questions you might have.</p>"},{"location":"images/","title":"Index","text":""},{"location":"images/#directory-of-static-images","title":"Directory of static images","text":""},{"location":"recipes/gromacs/","title":"Installing and Using GROMACS","text":"<p>GROMACS is a free and open-source software suite for high-performance molecular dynamics and output analysis.</p> <p>You can learn about GROMACS here: https://www.gromacs.org/.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#install-gromacs-with-mpi","title":"Install GROMACS with MPI","text":"EngagingSuperCloud <p>Select a version on the GROMACS website, then download and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget --no-check-certificate http://ftp.gromacs.org/pub/gromacs/gromacs-2019.6.tar.gz\ntar xvfz gromacs-2019.6.tar.gz\n</code></pre></p> <p>Load MPI and Cmake modules, <pre><code>module load engaging/openmpi/2.0.3 cmake/3.17.3\n</code></pre></p> <p>Create build and install directories, <pre><code>mkdir -p 2019.6/build\nmkdir 2019.6/install\ncd 2019.6/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2019.6 -DGMX_MPI=ON -DCMAKE_INSTALL_PREFIX=~/gromacs/2019.6/install\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage environment, <pre><code>source ~/gromacs/2019.6/install/bin/GMXRC\n</code></pre></p> <p>Select a version on the GROMACS website, then download and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget http://ftp.gromacs.org/pub/gromacs/gromacs-2023.2.tar.gz\ntar xvfz gromacs-2023.2.tar.gz\n</code></pre></p> <p>Load CUDA, Anaconda and MPI modules, <pre><code>module load cuda/11.8 anaconda/2023a\nmodule load mpi/openmpi-4.1.5\n</code></pre></p> <p>Create build and install directories, <pre><code>mkdir -p 2023.2/build\nmkdir 2023.2/install\ncd 2023.2/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2023.2 -DGMX_MPI=ON -DGMX_GPU=CUDA -DCMAKE_INSTALL_PREFIX=~/gromacs/2023.2-gpu\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage environment before running GROMACS programs, <pre><code>source ~/gromacs/2023.2/install/bin/GMXRC\n</code></pre></p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#run-gromacs","title":"Run GROMACS","text":"<p>Firstly, prepare for an input file. Refer to file formats. Here shows an example with an input file named <code>benchPEP-h.tpr</code> downloaded from this page. In these examples we have saved the input files in the <code>~/gromacs/bench/</code> directory.</p> <p>Secondly, create a batch job script, for example, named <code>job.sh</code>.</p> EngagingSuperCloud <p>This job script requests 2 nodes with a total of 8 CPU cores and 50GB of memory.</p> job.sh<pre><code>#!/bin/bash\n#SBATCH --job-name=\"production run\"\n#SBATCH --partition=sched_mit_hill\n#SBATCH --constraint=centos7\n#SBATCH --mem=50G\n#SBATCH -N 2\n#SBATCH --ntasks 8\n#SBATCH --time=12:00:00\n\n\nmodule purge\nmodule load gromacs/2018.4\n\ngmx_mpi=/home/software/gromacs/2018.4/bin/gmx_mpi\n\nif [ -n \"$SLURM_CPUS_PER_TASK\" ]; then\n    ntomp=\"$SLURM_CPUS_PER_TASK\"\nelse\n    ntomp=\"1\"\nfi\n\n\n# setting OMP_NUM_THREADS to the value used for ntomp to avoid complaints from gromacs\nexport OMP_NUM_THREADS=$ntomp\n\nmpirun -np $SLURM_NTASKS $gmx_mpi mdrun -ntomp $ntomp -deffnm ~/gromacs/bench/benchPEP-h -v\n</code></pre> <p>This job requests 2 nodes with 4 CPU cores and 2 GPUs per node.</p> job.sh<pre><code>#!/bin/bash\n#SBATCH --nodes=2              # 2 nodes\n#SBATCH --ntasks-per-node=2    # 2 MPI tasks per node\n#SBATCH --cpus-per-task=2      # 2 CPU cores per task\n#SBATCH --gres=gpu:volta:2     # 2 GPUs per node\n#SBATCH --time=01:00:00        # 1 hour\n\n\n# Load required modules\nmodule load cuda/11.8 mpi/openmpi-4.1.5\n\n# Enable direct GPU to GPU communications\nexport GMX_ENABLE_DIRECT_GPU_COMM=true\n\n# Activate user install of GROMACS\nsource ~/gromacs/2023.2-gpu/bin/GMXRC\n\n# Check MPI, GPU and GROMACS\nmpirun hostname\nnvidia-smi\nwhich gmx_mpi\n\n# Run GROMACS\nmpirun gmx_mpi mdrun -s ~/gromacs/bench/benchPEP-h.tpr -ntomp ${SLURM_CPUS_PER_TASK} -pme gpu -update gpu -bonded gpu -npme 1\n</code></pre> <p>Finally, submit the job, <pre><code>sbatch job.sh\n</code></pre></p> <p>Refer to GROMACS user guide for more info.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/h100_getting_started/","title":"Getting started on 8-way H100 nodes on Satori","text":"<p>This page provides information on how to run a first job on the IBM Watson AI Lab H100 GPU nodes on Satori. The page describes how to request an access to the Slurm partition associated  with the H100 nodes and how to run a first example pytorch script on the systems. </p> <p>A first set of H100 GPU systems has been added to Satori. These are for priority use by IBM Watson AI Lab research collaborators. They are also available for general opportunistic use when they are idle.</p> <p>Currently ( 2023-06-19 ) there are 4 H100 systems installed.  Each system has 8 H100 GPU cards, two Intel 8468 CPUs each with 48 physical cores and 1TiB of main memory.</p> <p>Below are some instructions for getting started with these systems. </p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#access-to-the-nodes","title":"Access to the nodes","text":"<p>To access the nodes in the priority group you need your satori login id to be listed in the WebMoira  group https://groups.mit.edu/webmoira/list/sched_oliva.  Either Alex Andonian and Vincent Sitzmann are able to add accounts to the <code>sched_oliva</code> Moira list.</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#interactive-access-through-slurm","title":"Interactive access through Slurm","text":"<p>To access an entire node through Slurm, the command below can be used from the satori login node</p> <pre><code>srun -p sched_oliva --gres=gpu:8 -N 1 --mem=0 -c 192 --time 1:00:00 --pty /bin/bash\n</code></pre> <p>this command will launch an interactive shell on one of the nodes (when a full node becomes available).  From this shell the NVidia status command  <pre><code>nvidia-smi\n</code></pre> should list 8 H100 GPUs as available. </p> <p>Single node, multi-gpu training examples (for example https://github.com/artidoro/qlora ) should run  on all 8 GPUs. </p> <p>To use a single GPU interactively the following command can be used <pre><code>srun -p sched_oliva --gres=gpu:1 --mem=128 -c 24 --time 1:00:00 --pty /bin/bash\n</code></pre></p> <p>this will request a single GPU. This request will allow other Slurm sessions to run on other GPUs  simultaneously with this session.</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-nightly-build-pytorch-example-with-a-fresh-miniconda-and-pytorch","title":"Running a nightly build pytorch example with a fresh miniconda and pytorch","text":"<p>A miniconda environment can be used to run the latest nightly build pytorch code on these  systems. To do this, first create a software install directory and install the needed pytorch software</p> <pre><code>mkdir -p /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre> <p>and then switch your shell to that directory. <pre><code>cd /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre></p> <p>now install miniconda and create an environment with the needed software <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \nchmod +x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh -b -p minic\n. ./minic/bin/activate \nconda create -y -n pytorch_test python=3.10\nconda activate pytorch_test                          \nconda install -y -c conda-forge cupy\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n</code></pre></p> <p>Once the software is installed, the following script can be used to test the installation. test.py<pre><code>import torch\ndevice_id = torch.cuda.current_device()\ngpu_properties = torch.cuda.get_device_properties(device_id)\nprint(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n          \"%.1fGb total memory.\\n\" % \n          (torch.cuda.device_count(),\n          device_id,\n          gpu_properties.name,\n          gpu_properties.major,\n          gpu_properties.minor,\n          gpu_properties.total_memory / 1e9))\n</code></pre> Run this script with <pre><code>python test.py\n</code></pre></p> <p>To exit the Slurm srun session enter the command <pre><code>exit\n</code></pre></p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-simple-batch-script-using-an-installed-miniconda-environment","title":"Running a simple batch script using an installed miniconda environment","text":"<p>To run a batch script on one of the H100 nodes in partition <code>sched_oliva</code> first paste the content in the  box below into a slurm script file called, for example, <code>test_script.slurm</code> ( change the <code>RUNDIR</code> setting to assign the  path to a directory where you have already installed a conda environment in a sub-directory called <code>minic</code> ).</p> <p>First create the <code>mytest.py</code> script with the contents above.</p> <pre><code>#!/bin/bash\n#\n#SBATCH --gres=gpu:8\n#SBATCH --partition=sched_oliva\n#SBATCH --time=1:00:00\n#SBATCH --mem=0\n#\n\nnvidia-smi\n\nRUNDIR=/nobackup/users/${USER}/h100-testing/minic\ncd ${RUNDIR}\n\n. ./minic/bin/activate\n\nconda activate pytorch_test\n\npython mytest.py\n</code></pre> <p>This script can then be submitted to Slurm to run in a background batch node using the command.</p> <pre><code>sbatch &lt; test_script.slurm\n</code></pre>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#getting-help","title":"Getting help","text":"<p>As always, please feel welcome to email orcd-help@mit.edu with questions, comments or suggestions. We would be happy to hear from you!</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/mpi/","title":"Message Passing Interface (MPI)","text":"<p>Message Passing Interface (MPI) is a standard designed for data communication in parallel computing. The MPI standard defines useful library functions/routines in C, C++, and Fortran. Python interface is available for MPI.   </p> <p>There are several MPI implementationos, such as <code>OpenMPI</code>, <code>MPICH</code>, <code>MVAPICH</code>, and <code>Intel MPI</code>, which work with Infiniband network for high-bandwidth data communication.</p> Engaging","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#mpi-modules","title":"MPI modules","text":"<p>There are OpenMPI modules available on the cluster. Before building or runrning your MPI programs, load the modules of a <code>gcc</code> compiler and an <code>openmpi</code> libraries to set up environment varialbes.</p> <p>There are two different operations systems (OS) on the cluster: CentOS 7 and Rocky 8. For CentOS 7 nodes, load these modules, <pre><code>module load gcc/6.2.0 openmpi/3.0.4\n</code></pre> or <pre><code>module load gcc/9.3.0 openmpi/4.0.5\n</code></pre> For Rocky 8 nodes, load these modules, <pre><code>module use /orcd/software/community/001/modulefiles/rocky8\nmodule load gcc/12.2.0 openmpi/4.1.4-pmi-ucx-x86_64\n</code></pre> If you need to run MPI programs with GPUs, load these modules instead, <pre><code>module load gcc/12.2.0 openmpi/4.1.4-pmi-cuda-ucx-x86_64\n</code></pre></p> <p>Note</p> <p>Load a <code>gcc</code> module first, then the openmpi modules built with this <code>gcc</code> will be shown in the output of <code>module avail</code> and can be loaded. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#build-mpi-programs","title":"Build MPI programs","text":"<p>This section will be focused on building MPI programs in C or Fortran. Python users can refer to this page for using the <code>mpi4py</code> package.</p> <p>Most MPI software should be built from source codes. First, download the package from the internet. Load one of the OpenMPI modules mentioned above. A typical building process is like this, <pre><code>./configure CC=mpicc CXX=mpicxx --prefix=&lt;/path/to/your/installation&gt;\nmake\nmake install\n</code></pre> Create an install directory and assign its full path to the flag <code>--prefix</code>. This is where the binaries will be saved.</p> <p>Widely-used MPI software includes <code>Gromacs</code>, <code>Lammps</code>, <code>NWchem</code>, <code>OpenFOAM</code> and many others. The building process of every software is different. Refer to its official installation guide for details.</p> Side note: MPI binaries <p>Some MPI software are provided with prebuilt binaries only. In this case, download the binaries compatible with the <code>linux</code> OS and the <code>x86_64</code> CPU architecture. If possible, try to choose an OpenMPI version, that the binary was built with, as close as possible to that of a module on the cluster. This type of MPI software includes <code>ORCA</code>. </p> <p>Spack is a popular tool to build many software packages systematically on clusters. It makes building processes convenient in many cases. If you want to use Spack to build your software package on the cluster, refer to the recipe page for Spack.</p> <p>If you develop your MPI codes, the codes can be compiled and linked like this <pre><code>mpicc -O3 name.c -o my_program\n</code></pre> or <pre><code>mpif90 -O3 name.f90 -o my_program\n</code></pre> This will create an executable file named <code>my_program</code>. Prepare a GNU Makefile to build programs with multiple source files. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#mpi-jobs","title":"MPI jobs","text":"<p>MPI programs are suitable to run on multiple CPU cores of a single node or on multiple nodes. </p> <p>Here is an example script (e.g. named <code>job.sh</code>) to run an MPI job using multiple cores on a single node.  <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 8\n#SBATCH --mem=10GB   \n\nsrun hostname\nmodule load gcc/6.2.0 openmpi/3.0.4\nmpirun -n $SLURM_NTASKS my_program\n</code></pre></p> <p>This job requests 8 cores (with <code>-n</code>) and 10 GB of memory (with <code>--mem</code>) on 1 node (with <code>-N</code>) for 30 minutes (with <code>-t 30</code>). The <code>-n</code> flag is the same as <code>--ntasks</code>. The specified value is saved to the variable <code>SLURM_NTASKS</code>. In this case, the requested number of cores is equal to <code>SLURM_NTASKS</code>. The command <code>mpirun -n $SLURM_NTASKS</code> ensures that each MPI task runs on one core. </p> <p>The command <code>srun hostname</code> is to check if the correct number of cores and nodes are assigned to the job. It is not needed in production runs. </p> Side note: partitions and modules <p>The modules used in the above example is for the CentOS 7 OS, which works for these partitions: <code>sched_mit_hill</code>, <code>newnodes</code>, and <code>sched_any</code>. If using a partition with the Rocky 8 OS, such as <code>sched_mit_orcd</code>, change the modules accrodingly (see the first session). These are public partitions that are avaiable to most users. Many labs have partitions for their purchased nodes. </p> <p>Submit the job with the <code>sbatch</code> command, <pre><code>sbatch job.sh\n</code></pre></p> <p>To run an MPI job on multiple nodes, refer to the following exmaple script. <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=8\n#SBATCH --mem=10GB\n\nsrun hostname\nmodule load gcc/6.2.0 openmpi/3.0.4\nmpirun -n $SLURM_NTASKS my_program\n</code></pre></p> <p>This job requests 2 nodes with 8 cores and 10 GB of memory per node. In this case, the total number of cores (saved to <code>SLURM_NTASKS</code>) is equal to the number of nodes (saved to <code>SLURM_NNODES</code>) times the number of cores per node (saved to <code>SLURM_NTASKS_PER_NODE</code>). The command <code>mpirun -n $SLURM_NTASKS</code> ensures that MPI tasks are distributed to both nodes and each MPI task runs on one core. </p> <p>Alternatively, users can specify the number of cores per node using an OpenMPI option like this <code>mpirun -npernode $SLURM_NTASKS_PER_NODE my_program</code>.</p> <p>If replacing <code>--ntasks-per-node=8</code> with <code>-n 16</code> in the above script, the job will request 16 cores on 2 nodes, but it is not always the case that there are 8 cores per node. For example, there may be 7 cores on one node and 9 cores on another, or 1 core on one node and 15 cores on another, etc, depending on the current available resources on the cluster. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#computing-resources-for-mpi-jobs","title":"Computing resources for MPI jobs","text":"<p>To get a better idea on how many nodes, cores and memory should be requested, users need to consider the following two questions. </p> <p>First, what resources are available on the cluster? Use this command to check node and job info on the cluster, including the constraint associated with OS (<code>%f</code>), the nubmer of CPU cores (<code>%c</code>), the memory size (<code>%m</code>), the wall time limit (<code>%l</code>), and the current usage status (<code>%t</code>).  <pre><code> sinfo -N -p sched_mit_hill,newnodes,sched_any,sched_mit_orcd -o %f,%c,%m,%l,%t |grep -v drain\n</code></pre> On typical nodes of the cluster, the number of cores per node varies from 16 to 128, and the memory per node varies from 63 GB to 515 GB.</p> <p>To obtain a good performance of MPI programs, it is recommended to request all physical CPU cores and memory on each node. For example, request two nodes with 16 physical cores per node and all of the memory like this, <pre><code>#SBATCH -N 2\n#SBATCH --ntasks-per-node=16\n#SBATCH --mem=0\n</code></pre></p> <p>As MPI is a distributed-memory parallelism, sometimes it is good to use the <code>--mem-per-core</code> flag assigning a certain amount of memory to each core. The total memory is increased with the number of cores in this case. Double check that the total memory fits in the maximum memory of a node to avoid failed jobs.</p> <p>Second, what is the speedup of your MPI program? According to Amdahl's law, well-performing MPI programs are usually speeded up almost linearly as the number of cores is increased until it is saturated at some point. In practice, try to run testing cases investigating the speedup of your program, and then decide how many cores are needed to speed it up efficiently. Do not increase the number of cores when speedup is poor. </p> <p>Note</p> <p>After a job starts to run, execute the command <code>squeue -u $USER</code> to check which node the job is running on, and then log in to the node with <code>ssh &lt;hostname&gt;</code> and execute the <code>top</code> command to check how many CPU cores are being used by the program and what the CPU efficiency is. The efficiency may vary with the number of CPU cores. Try to keep your jobs at a high efficiency. </p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi/#hybrid-mpi-and-multithreading-jobs","title":"Hybrid MPI and multithreading jobs","text":"<p>MPI programs are based on distributed-memory parallelism, that says, each MPI task owns a faction of data, such as arrays, matrices, or tensors. In contrast to MPI, the multithreading technique is based on a shared-memory parallelism, in which data is shared by multiple threads. A common implementation of the multithreading technique is OpenMP. For Python users, the <code>numpy</code> package is based on C libraries, such as Openblas, which are usually built with OpenMP. </p> Side note: OpenMP <p>OpenMP is an abbreviation of Open Multi-Processing. It is not related to OpenMPI.</p> <p>Some programs are designed in a hybrid scheme such that MPI and OpenMP are combined to enable two-level parallelization. Set MPI tasks and OpenMP threads in hybrid programs based on the following equation, <pre><code>(Number of MPI Tasks) * (Nubmer of Threads) = Total Number of Cores          (1)\n</code></pre></p> Side note: hyperthreads <p>There are two or multiple hyperthreads on each CPU core in modern CPU architecture. The hyperthread technique is turned off for most nodes of this cluster, but it may be turned on for some nodes as requested by the owner labs. In the case that there are two hyerthreads per physical core, the right side of the equation should be <code>2 * (Total Number of Cores)</code> instead.</p> <p>One way to run hybrid progmrams in Slurm jobs is to use the <code>-n</code> flag for the number of MPI tasks and the <code>-c</code> flag for the number of threads. The follwing is a job script that runs a program with 2 MPI tasks and 8 threads per task on a node with 16 cores. <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 2\n#SBATCH -c 8\n#SBATCH --mem=10GB\n\nmodule load gcc/6.2.0 openmpi/3.0.4\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nmpirun -n $SLURM_NTASKS my_program\n</code></pre> The <code>-c</code> flag is the same as <code>--cpus-per-task</code>. The specified value is saved in the variable <code>SLURM_CPUS_PER_TASK</code>. In this case, the total number of cores equals <code>SLURM_NTASKS * SLURM_CPUS_PER_TASK</code>, that is 16. </p> <p>The built-in environment variable <code>OMP_NUM_THREADS</code> is used to set the number of threads for an OpenMP program. Here it is equal to <code>SLURM_CPUS_PER_TASK</code>. The number of MPI tasks is set to be <code>SLURM_NTASK</code> in the <code>mpirun</code> command line, therefore, the nubmer of MPI tasks times the number of threads equals the total number of CPU cores. </p> <p>Users only need to specify the numbers following Slurm flags <code>-n</code> and <code>-c</code>, for example, <code>-n 4 -c 4</code> or <code>-n 8 -c 2</code>, keeping the product unchanged, then the MPI tasks and threads are all set automatically.  </p> <p>Similarly, here is an exmple script to request two nodes,  <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=2\n#SBATCH -c 8\n#SBATCH --mem=0\n\nmodule load gcc/6.2.0 openmpi/3.0.4\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nmpirun -n $SLURM_NTASKS my_program\n</code></pre> In this case, the total number of cores is equal to <code>SLURM_NNODES * SLURM_NTASKS_PER_NODE * SLURM_CPUS_PER_TASK</code>, that is <code>2 * 2 * 8 = 32</code>. The job will run 4 MPI tasks (i.e. 2 tasks per node) and 8 threads per task. Equation (1) is satisfied as <code>4 * 8 = 32</code>. </p> <p>As mentioned in the previous section, it is recommended to run testing cases to determine the values for the flags <code>-N</code>, <code>-n</code> and <code>-c</code> to obtain a better performance.</p> <p>There is another way to submit jobs for hybrid programs, in which the <code>-c</code> flag is not used at all. For example, it also works like this, <pre><code>#!/bin/bash\n#SBATCH -p sched_mit_hill\n#SBATCH -t 30\n#SBATCH -N 1\n#SBATCH -n 16\n#SBATCH --mem=10GB\n\nmodule load gcc/6.2.0 openmpi/3.0.4\nexport OMP_NUM_THREADS=8\nMPI_NTASKS=$((SLURM_NTASK / $OMP_NUM_THREADS))\nmpirun -n $MPI_NTASKS my_program\n</code></pre> This job requests 16 CPU cores on 1 node and runs 2 MPI tasks with 8 threads per task, so equation (1) is satisfied as <code>2 * 8 = 16</code>. In this case, users need to set the values for the Slurm flag <code>-n</code> and the variable <code>OMP_NUM_THREADS</code>.</p>","tags":["Engaging","MPI","Howto Recipes"]},{"location":"recipes/mpi4py/","title":"MPI for Python","text":"<p>MPI for Python (<code>mpi4py</code>) provides Python bindings for the Message Passing Interface (MPI) standard, allowing Python applications to exploit multiple processors on workstations, clusters and supercomputers.</p> <p>You can learn about <code>mpi4py</code> here: https://mpi4py.readthedocs.io/en/stable/.</p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#installation","title":"Installation","text":"OpenMindEngaging <p>The support team has installed <code>mpi4py</code> in an Anaconda module. You can load the module and do not need to install anything,  <pre><code>module load openmind/anaconda/3-2022.05\n</code></pre></p> <p>If you want to use Anaconda in your directory, refer to section 3 on this page to set it up, then install <code>mpi4py</code>,   <pre><code>conda install -c conda-forge mpi4py\n</code></pre></p> <p>First, load an Anaconda module on a CentOS 7 head node (such as eofe7 or eofe8),  <pre><code>module load anaconda3/2023.07\n</code></pre>  then install <code>mpi4py</code>,   <pre><code>conda create -n mpi\nsource activate mpi\nconda install mpi4py\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#example-codes","title":"Example codes","text":"<p>Prepare your Python codes. Example 1: The following is a code for sending and receiving a dictionary. Save it in a file named <code>p2p-send-recv.py</code>. p2p-send-recv.py<pre><code>from mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\nif rank == 0:\n    data = {'a': 7, 'b': 3.14}\n    comm.send(data, dest=1, tag=11)\n    print(rank,data)\nelif rank == 1:\n    data = comm.recv(source=0, tag=11)\n    print(rank,data)\n</code></pre></p> <p>Example 2: The following is a code for sending and receiving an array. Save it in a file named <code>p2p-array.py</code>. p2p-array.py<pre><code>from mpi4py import MPI\nimport numpy\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\n# passing MPI datatypes explicitly\nif rank == 0:\n    data = numpy.arange(1000, dtype='i')\n    comm.Send([data, MPI.INT], dest=1, tag=77)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(1000, dtype='i')\n    comm.Recv([data, MPI.INT], source=0, tag=77)\n    print(rank,data)\n\n# automatic MPI datatype discovery\nif rank == 0:\n    data = numpy.arange(100, dtype=numpy.float64)\n    comm.Send(data, dest=1, tag=13)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(100, dtype=numpy.float64)\n    comm.Recv(data, source=0, tag=13)\n    print(rank,data)\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#submitting-jobs","title":"Submitting jobs","text":"OpenMindEngaging <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>.  p2p-job.sh<pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n\nmodule load openmind/anaconda/3-2022.05\n\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre>  !!! note       If you use Anaconda in your directory, do not load the Anaconda module. </p> <p>Finally submit the job,  <pre><code>sbatch p2p-job.sh\n</code></pre></p> <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>.  p2p-job.sh<pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n\nmodule load anaconda3/2023.07\n\nsource activate mpi\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\n\nsource activate mpi\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre></p> <p>Finally submit the job,  <pre><code>sbatch p2p-job.sh\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mujoco/","title":"Installing and Using MuJoCo","text":"<p>MuJoCo is a free and open source physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed.</p> <p>You can learn about MuJoCo here: https://mujoco.org.</p> <p>Whether you are installing on Engaging or SuperCloud, you\u2019ll first have to install the MuJoCo binaries. This process is the same on all systems.</p>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#install-the-mujoco-binaries","title":"Install the MuJoCo Binaries","text":"<p>First, install MuJoCo itself somewhere in your home directory. This is as simple as downloading the MuJoCo binaries, which can be found on their web page. For the release that you want, select the file that ends with \u201clinux-x86_64.tar.gz\u201d, for example for 2.3.0 select mujoco-2.3.0-linux-x86_64.tar.gz. Right click, and copy the link address. Then you can download on one of the login nodes with the \u201cwget\u201d command, and untar:</p> <pre><code>wget https://github.com/deepmind/mujoco/releases/download/2.3.0/mujoco-2.3.0-linux-x86_64.tar.gz\ntar -xzf mujoco-2.3.0-linux-x86_64.tar.gz\n</code></pre> <p>In order for mujoco-py to find the MuJoCo binaries, set the following paths:</p> <pre><code>export MUJOCO_PY_MUJOCO_PATH=$HOME/path/to/mujoco230/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MUJOCO_PY_MUJOCO_PATH/bin\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#install-mujoco-py","title":"Install Mujoco-Py","text":"<p>First, make sure the <code>MUJOCO_PY_MUJOCO_PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables are set pointing to your mujoco installation. You can use the \u201cecho\u201d command to do this:</p> <pre><code>echo MUJOCO_PY_MUJOCO_PATH\necho LD_LIBRARY_PATH\n</code></pre> <p>If any of these are not set properly you can set them as described above (see here for MUJOCO_PY_MUJOCO_PATH, LD_LIBRARY_PATH.</p> EngagingSuperCloud <p>Next load either a Python or Anaconda module. In this example I loaded the latest anaconda3 module (run <code>module avail anaconda</code> to see the current list of available anaconda modules):</p> <pre><code>module load anaconda3/2022.10\n</code></pre> <p>From here on you can follow the standard instructions to install mujoco-py, using the <code>--user</code> flag where appropriate to install in your home directory, or install in an anaconda or virtual environment (do not use the <code>--user</code> flag if you want to install in a conda or virtual environment). Here I am installing in my home directory with <code>--user</code>:</p> <pre><code>pip install --user 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Start up python and import mujoco_py to complete the build process:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>MuJoCo, particularly mujoco-py, can be tricky to install on SuperCloud as it uses file locking during the install and whenever the package is loaded. File locking is disabled on the SuperCloud shared filesystem performance reasons, but is available on the local disk of each node. Therefore, one workaround is to install mujoco-py on the local disk of one of the login nodes and then copy the install to your home directory. To load the package, the install then needs to be copied to the local disk.</p> <p>We\u2019ve found the most success by doing this with a python virtual environment. By using a python virtual environment you can install any additional packages you need with mujoco-py, and they can be used along with packages in our anaconda module, unlike conda environments.</p> <p>Create the virtual environment on the local disk of the login node and install mujoco-py (install the version you would like to use):</p> <pre><code>module load anaconda/2023a\nmkdir /state/partition1/user/$USER\npython -m venv /state/partition1/user/$USER/mujoco_env\nsource /state/partition1/user/$USER/mujoco_env/bin/activate\npip install 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Now install any other packages you need to run your MuJoCo jobs. With virtual environments you won\u2019t see any of the packages you\u2019ve previously installed with <code>pip install --user</code> or what you may have installed in another environment. You should still be able to use any of the packages in the anaconda module you\u2019ve loaded, so no need to install any of those.</p> <pre><code>pip install pkgname1\npip install pkgname2\n</code></pre> <p>Since you are installing into virtual environment, do not use the <code>--user</code> flag.</p> <p>Once you\u2019ve installed the packages you need, start Python and import mujoco_py to finish the build:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>Now that your environment is created, copy it to your home directory for permanent storage.</p> <pre><code>cp -r /state/partition1/user/$USER/mujoco_env $/software/mujoco/\n</code></pre> <p>If you\u2019d like you can run the few example lines listed on install section of the mujoco-py github page to verify the install went through properly:</p> <pre><code>import mujoco_py\nimport os\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#using-mujoco-in-a-job","title":"Using MuJoCo in a Job","text":"<p>To use MuJoCo you\u2019ll need to first load the same Python or Anaconda module you used to install mujoco-py. If you installed it into a conda environment or python virtual environment, load that environment as well. We recommend you do this in your job submission script rather than in your .bashrc or at the command line before you submit the job. This way you know your job is configured properly every time it runs.</p> <p>You can use the following test scripts to test your MuJoCo setup in a job environment, and as a starting point for your own job:</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> EngagingSuperCloud submit_test.sh<pre><code>#!/bin/bash\n\n# Load the same python/anaconda module you used to install mujoco-py\nmodule load python/3.8.3\n\n# Run the script\npython mujoco_test.py\n</code></pre> <p>Now whenever you use mujoco-py the installation will need to be on the local disk of the node(s) where you are running. In your job script you can add a few lines of code that will check whether the environment exists on the local disk, and if not copy it. You can run these lines during an interactive job as well.</p> submit_test.sh<pre><code>#!/bin/bash``\n\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\nif [ ! -d \"$MUJOCO_ENV\" ]; then\n    echo \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\n    mkdir -p /state/partition1/user/$USER\n    cp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\n\nmodule load anaconda/2022a\nsource $MUJOCO_ENV/bin/activate\n\npython mujoco_test.py\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/","title":"Example of a minimal program using the nvhpc stack with CUDA aware MPI","text":"","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#about-nvhpc","title":"About NVHPC","text":"<p>NVHPC is an integrated collection of software tools and libraries distributed by NVidia. An overview document describing nvhpc  can be found here. The aim of the NVHPC team is to provide up to date, preconfigured suites of compilers, libraries and tools that are  specifically optimized for NVidia GPU hardware. It supports single and multi-GPU execution.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#basic-usage-example","title":"Basic Usage Example","text":"<p>This example shows steps for using NVHPC to run a simple test MPI program, written in C, that communicates between two GPUs. The detailed steps, that can be executed in an interactive Slurm session, are explained  below.  A complete Slurm job example is shown at the end.</p> <p>Prerequisites</p> <p>This example assumes you have access to a Slurm partition with GPU resources and if using Engaging are working with a Rocky Linux environment.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#1-activate-the-relevant-nvhpc-module","title":"1. Activate the relevant NVHPC module","text":"<p>The NVHPC environment is installed as a module and can be made visible in a session using the command</p> EngagingSatori <pre><code>  module load nvhpc/2023_233/nvhpc/23.3\n</code></pre> <pre><code>module load module load nvhpc/21.5\n</code></pre> <p>this will add a specific version of the nvhpc software (version 23.3 released in 2023 for Engaging and version 21.5 released in 2021 for Satori) to a shell or batch script. The software added includes compilers for C, C++ and Fortran; base GPU optimized numerical libraries for linear algebra, Fourier transforms and others; GPU optimized communication libraries supporting MPI, SHMEM and NCCL APIs.</p> <p>An environment variable, <code>NVHPC_ROOT</code>, is also set. This can be used in scripts to reference the locations of libraries when needed.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#2-set-paths-needed-for-compile-step","title":"2. Set paths needed for compile step","text":"<p>Here we use the module environment variable, <code>NVHPC_ROOT</code>, to set environment variables that have paths needed for compilation and linking of code.</p> <pre><code>culibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#3-create-a-c-program-for-that-executes-some-simple-multi-node-multi-gpu-test-code","title":"3. Create a C program for that executes some simple multi-node, multi-GPU test code.","text":"<p>The next step is to create a file holding C code that uses MPI to send information between two GPUs  running in different processes. Paste the C code below into a file called <code>test.c</code>.</p> test.c<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;cuda_runtime.h&gt;\nint main(int argc, char *argv[])\n{\n  int myrank, mpi_nranks; \n  int LBUF=1000000;\n  float *sBuf_h, *rBuf_h;\n  float *sBuf_d, *rBuf_d;\n  int bSize;\n  int i;\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);                  // my MPI rank\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;mpi_nranks);\n\n  if ( mpi_nranks != 2 ) { printf(\"Program requires exactly 2 ranks\\n\");exit(-1); }\n\n  int deviceCount;\n  cudaGetDeviceCount(&amp;deviceCount);               // How many GPUs?\n  printf(\"Number of GPUs found = %d\\n\",deviceCount);\n  int device_id = myrank % deviceCount;\n  cudaSetDevice(device_id);                       // Map MPI-process to a GPU\n  printf(\"Assigned GPU %d to MPI rank %d of %d.\\n\",device_id, myrank, mpi_nranks);\n\n  // Allocate buffers on each host and device\n  bSize = sizeof(float)*LBUF;\n  sBuf_h = malloc(bSize);\n  rBuf_h = malloc(bSize);\n  for (i=0;i&lt;LBUF;++i){\n    sBuf_h[i]=(float)myrank;\n    rBuf_h[i]=-1.;\n  }\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  cudaMalloc((void **)&amp;sBuf_d,bSize);\n  cudaMalloc((void **)&amp;rBuf_d,bSize);\n\n  cudaMemcpy(sBuf_d,sBuf_h,bSize,cudaMemcpyHostToDevice);\n\n  if ( myrank == 0 ) {\n   MPI_Recv(rBuf_d,LBUF,MPI_REAL,1,0,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n  } \n  else if ( myrank == 1 ) {\n   MPI_Send(sBuf_d,LBUF,MPI_REAL,0,0,MPI_COMM_WORLD);\n  }\n  else\n  {\n   printf(\"Unexpected myrank value %d\\n\",myrank);\n   exit(-1);\n  }\n\n  cudaMemcpy(rBuf_h,rBuf_d,bSize,cudaMemcpyDeviceToHost);\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#4-compile-program","title":"4. Compile program","text":"<p>Here we use nvhpc MPI wrapper to compile. The two environment variables we set earlier (<code>cuincdir</code> and <code>culibdir</code>) are used to let the compile step know where to find the relevant CUDA header and library files. The CUDA runtime library (<code>cudart</code>) is added as a location for finding CUDA functions the code utilizes.</p> <pre><code>mpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#5-execute-program","title":"5. Execute program","text":"<p>Once code has been compiled the <code>mpiexec</code> command that is part of the <code>nvhpc</code> module can be used to run the test program. The <code>nvhpc</code> module defaults to using its builtin version of OpneMPI. The OpenMPI option <code>btl_openib_warn_no_device_params_found</code> is passed into the OpenMPI runtime library. This option suppresses a warning that OpenMPI can generate when it encounters a network device card that is not present in a built-in list that OpenMPI has historically included.</p> EngagingSatori <pre><code>mpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre> <pre><code>salloc -n 2 --gres=gpu:2\nmpiexec -n 2 ./a.out \n</code></pre> <p>Note the <code>salloc</code> command is only needed to run interactively from the login node. If you are running in a batch job or are already in an interactive job on a compute node you will not need to first run <code>salloc</code>.</p> <p>Running this program using the command above should produce the following output.</p> <pre><code>Number of GPUs found = 1\nNumber of GPUs found = 1\nAssigned GPU 0 to MPI rank 0 of 2.\nrBuf_h[0] = -1.000000\nAssigned GPU 0 to MPI rank 1 of 2.\nrBuf_h[0] = 1.000000\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#example-of-slurm-job-file-for-executing-this-example","title":"Example of Slurm job file for executing this example","text":"<p>First create a file called \"test.c\" containing the example C program above. The job script file below will run all the steps described above for \"test.c\". It can be submitted to Slurm using the command <code>sbatch</code> followed by the filename holding the job script.</p> EngagingSatori test_cuda_and_mpi.sbatch<pre><code>#!/bin/bash\n#SBATCH -p sched_system_all\n#SBATCH --constraint=rocky8\n#SBATCH -N 2\n#SBATCH -n 2\n#SBATCH --gres=gpu:2\n#SBATCH --time=00:02:00\n#\n# Basic slurm job that tests GPU aware MPI in the NVHPC tool stack.\n#\n#\n#   To submit through Slurm use:\n#\n#   $ sbatch test_cuda_and_mpi.sbatch\n#  \n#   in terminal.\n\n# Write a little log info\necho \"## Start time \\\"\"`date`\"\\\"\"\necho \"## Slurm job running on nodes \\\"${SLURM_JOB_NODELIST}\\\"\"\necho \"## Slurm submit directory \\\"${SLURM_SUBMIT_DIR}\\\"\"\necho \"## Slurm submit host \\\"${SLURM_SUBMIT_HOST}\\\"\"\necho \" \"\n\n\nmodule load nvhpc/2023_233/nvhpc/23.3\nculibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n\nmpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n\nmpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre> test_cuda_and_mpi.sbatch<pre><code>#!/bin/bash\n#SBATCH -n 2\n#SBATCH --gres=gpu:2\n#SBATCH --time=00:02:00\n#\n# Basic slurm job that tests GPU aware MPI in the NVHPC tool stack.\n#\n#\n#   To submit through Slurm use:\n#\n#   $ sbatch test_cuda_and_mpi.sbatch\n#  \n#   in terminal.\n\n# Write a little log info\necho \"## Start time \\\"\"`date`\"\\\"\"\necho \"## Slurm job running on nodes \\\"${SLURM_JOB_NODELIST}\\\"\"\necho \"## Slurm submit directory \\\"${SLURM_SUBMIT_DIR}\\\"\"\necho \"## Slurm submit host \\\"${SLURM_SUBMIT_HOST}\\\"\"\necho \" \"\n\n\nmodule load nvhpc/21.5\nculibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n\nmpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n\nmpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/relion/","title":"Installing and Using RELION","text":"<p>RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a software package that employs an empirical Bayesian approach for electron cryo-microscopy (cryo-EM) structure determination. </p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/relion/#relion-on-satori","title":"RELION on Satori","text":"<p>This recipe is for building and using RELION on x86 nodes on Satori. It is different from working on IBM power9 nodes on Satori.</p> <p>Note</p> <p>The x86 nodes are available to some labs only. </p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/relion/#install-relion","title":"Install RELION","text":"<p>Go to your directory and download RELION, <pre><code>cd /nobackup/users/$USER\ngit clone https://github.com/3dem/relion.git\n</code></pre></p> <p>Get an interactive session on x86 nodes of Satori, <pre><code>srun -p sched_mit_mbathe -c 2 -t 60 --pty bash\n</code></pre></p> <p>Load modules for the GCC compiler and Openmpi implementation, <pre><code>module use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64  \nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64 \n</code></pre></p> <p>Note: These modules are installed for the x86 nodes only. </p> <p>Build RELION with CUDA and FFTW features, <pre><code>cd ~\nmkdir relion\ncd relion\ngit checkout master \ncd ..\nmkdir 4.0.1\ncd 4.0.1\nmkdir install\nmkdir build\ncd build\n\ncmake -DCMAKE_INSTALL_PREFIX=/home/$USER/relion/4.0.1/install -DFORCE_OWN_FFTW=ON -DAMDFFTW=ON -DCUDA_ARCH=80 ../../relion\nmake\nmake install\n</code></pre></p> <p>It is all set for the installation.</p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/relion/#use-relion","title":"Use RELION","text":"<p>There is a nice Graphical User Interface (GUI) for RELION. To use the GUI, first log in Satori with x-forwardig support, <pre><code>ssh -Y &lt;user&gt;@satori-login-002.mit.edu\n</code></pre></p> <p>Get an interactive session with GPU and x-forwarding support on x86 nodes of Satori, <pre><code>srun --x11 -p sched_mit_mbathe --gres=gpu:1 -c 6 -t 60 --pty bash\n</code></pre></p> <p>Set up environment for compilers, mpi implementation, FFTw, and RELION, <pre><code>module use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64  \nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64 \nmodule load fftw/3.3.10-x86_64\nexport LD_LIBRARY_PATH=/nfs/software001/home/software-r8-x86_64/spack-20230328/opt/spack/linux-rocky8-x86_64/gcc-12.2.0/fftw-3.3.10-qiaruimvw6zu2h4f5eolqom7tixem6vk/lib:$LD_LIBRARY_PATH\nexport RELION_HOME=/home/$USER/relion/4.0.1/install\nexport PATH=${RELION_HOME}/bin:$PATH\nexport LD_LIBRARY_PATH=${RELION_HOME}/lib:$LD_LIBRARY_PATH\n</code></pre></p> <p>then open the RELION GUI,  <pre><code>relion &amp;\n</code></pre></p> <p>Users can use GUI to edit files or submit jobs. Refer to details on this page. </p> <p>Alternatively, users can prepare a batch job script to submit jobs. </p> <p>Download RELION benchmarks for testing,  <pre><code>cd ~/relion\nwget ftp://ftp.mrc-lmb.cam.ac.uk/pub/scheres/relion_benchmark.tar.gz\n</code></pre> then all benchmark files will be saved in a directory named <code>relion_benchmark</code>.</p> <p>Here is an exmaple job script, <pre><code>#!/bin/bash\n#SBATCH --partition=sched_mit_mbathe\n#SBATCH --time=12:00:00\n#SBATCH --nodes=1\n#SBATCH -n 20\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=10000\n#SBATCH --gres=gpu:4\n#SBATCH --chdir='.'\n\nmodule use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64\nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64\nexport LD_LIBRARY_PATH=/nfs/software001/home/software-r8-x86_64/spack-20230328/opt/spack/linux-rocky8-x86_64/gcc-12.2.0/fftw-3.3.10-qiaruimvw6zu2h4f5eolqom7tixem6vk/lib:$LD_LIBRARY_PATH\nexport RELION_HOME=\"/home/$USER/relion/4.0.1/install\"\nexport PATH=${RELION_HOME}/bin:$PATH\nexport LD_LIBRARY_PATH=${RELION_HOME}/lib:$LD_LIBRARY_PATH\n\ncd ~/relion/relion_benchmark\nmkdir output\n\nmpirun -np 20 relion_refine_mpi \\\n  --i Particles/shiny_2sets.star \\\n  --o output \\\n  --ref emd_2660.map:mrc \\\n  --ini_high 60 \\\n  --pool 100 \\\n  --pad 2  \\\n  --ctf \\\n  --iter 25 \\\n  --tau2_fudge 4 \\\n  --particle_diameter 360 \\\n  --K 4 \\\n  --flatten_solvent \\\n  --zero_mask \\\n  --oversampling 1 \\\n  --healpix_order 2 \\\n  --offset_range 5 \\\n  --offset_step 2 \\\n  --sym C1 \\\n  --norm \\\n  --scale \\\n  --j 1   \\\n  --gpu \"\" \\\n --dont_combine_weights_via_disc \\\n  --scratch_dir /tmp\n</code></pre></p> <p>Add the above lines in a file named <code>job.sh</code>, then submit the job, <pre><code>sbatch job.sh\n</code></pre></p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/vscode/","title":"Using VSCode on an ORCD System","text":"<p>VSCode is a convenient IDE for development, and one of its nicest features is its ability to run on a remote system using its RemoteSSH extension. This means you can have the VSCode window on your computer, while the files and anything you run will be on the remote system you are connected to.</p> <p>Once you've installed the RemoteSSH extension this is fairly easy to set up. However, it is also very easy to set up in such a way that it is not only slow for you, but it also puts excess load on the login nodes and in turn slows things down for others on that node. Luckily, with a few extra steps you can run VSCode on a compute node where it can have more resources to run and won't impact others as much.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#setting-up-your-config-file","title":"Setting up your Config File","text":"<p>Click the \"Open a Remote Window\" button in the bottom left corner of your VSCode window (It is a small blue rectangle labeled with <code>&gt;&lt;</code>). In the bar at the top of the page select \"Connect to Host...\", then \"Configure SSH Hosts\", and select first option, which will differ depending on your operating system. This will open your config file in a VSCode tab.</p> <p>To run on a compute node you will need at least 2 entries in this file. The first will be a login node that you'll \"jump\" through and the second will be the compute node that is your final destination.</p> EngagingSatoriOpen Mind config<pre><code>Host eofe-login\n  HostName eofe8.mit.edu\n  User USERNAME\n\nHost eofe-compute\n  User USERNAME\n  HostName nodename\n  ProxyJump eofe-login\n</code></pre> <p>Note</p> <p>If you are using one of the login nodes that requires 2-Factor authentication be ready to receive your default 2-Factor prompt when you connect. If you do not respond right away the connection will time out.</p> <p>Note</p> <p>To use VSCode on a compute node, an SSH key is necessary. If you haven't set up SSH keys yet, refer to this Engaging guide.</p> config<pre><code>Host satori-login\n  HostName satori-login-001.mit.edu\n  User USERNAME\n\nHost satori-compute\n  User USERNAME\n  HostName nodename\n  ProxyJump satori-login\n</code></pre> <p>Note</p> <p>To use VSCode on a compute node, an SSH key is necessary. While Satori documentation is unavailable, you can follow the same steps as outlined on Engaging.</p> config<pre><code>Host om-login\n  HostName openmind7.mit.edu\n  User USERNAME\n\nHost om-compute\n  User USERNAME\n  HostName nodename\n  ProxyJump om-login\n</code></pre> <p>Note</p> <p>To use VSCode on a compute node, an SSH key is necessary. If you haven't set up SSH keys yet, refer to this Open Mind guide.</p> <p>Replace <code>USERNAME</code> with your username on the system you are connecting to. We will fill in \"nodename\" later.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#starting-your-vscode-session-on-a-compute-node","title":"Starting your VSCode Session on a Compute Node","text":"<p>Each time you sit down to do remote work through VSCode you will have three steps:</p> <ol> <li>Start an interactive job on the target system and note the name of the node your job is running on</li> <li>Update your config file with the node name</li> <li>Connect to the compute node using your updated config</li> </ol> <p>We go through these steps in detail below.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#start-an-interactive-job","title":"Start an Interactive Job","text":"<p>Open a terminal window and ssh into the login node. If you are not used to doing this you can open a terminal in VSCode and run:</p> EngagingSatoriOpen Mind <pre><code>ssh eofe-login\n</code></pre> <pre><code>ssh satori-login\n</code></pre> <pre><code>ssh om-login\n</code></pre> <p>Use the name you have used for the login <code>Host</code> in your config file if different than the one above. The example screenshot below shows logging into one of the Engaging login nodes with ssh in a VSCode terminal window.</p> <p></p> <p>Once you are logged in start an interactive session. If you are planning to only edit files a single core may be sufficient, but if you plan to run code or Jupyter Notebooks you may want to allocate more resources accordingly. Refer to the documentation for your system on how to request an interactive job:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging's Documentation for Running Jobs</p> <p>Satori's Documentation for Running Jobs</p> <p>SuperCloud's Documentation for Running Jobs</p> <p>OpenMind's Documentation for Running Jobs</p> <p>Once your job has started you can run the <code>hostname</code> command to get the name of the node your interactive job is running on. You can also run the <code>squeue --me</code> command to list all your running jobs and get the hostname from the last column.</p> <p>The screenshot below shows requesting a single interactive core for 1 hour on Engaging:</p> <p></p> <p>Note that the scheduler will also tell you which node you are allocated in its output. In this screenshot my node name is <code>node020</code>.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#update-your-config-file","title":"Update your Config File","text":"<p>Update the <code>HostName</code> of your compute node entry in your config file. If your config file is not open, follow the instructions above to open it again. Then replace whatever you have for <code>HostName</code> in your config file with the output of the <code>hostname</code> command you ran in your interactive session, or got from <code>squeue --me</code>.</p> <p>If your compute node is <code>node1234</code> then your config file should look something like:</p> EngagingSatoriOpen Mind config<pre><code>Host eofe-login\n  HostName eofe8.mit.edu\n  User USERNAME\n\nHost eofe-compute\n  User USERNAME\n  HostName node1234\n  ProxyJump eofe-login\n</code></pre> config<pre><code>Host satori-login\n  HostName satori-login-001.mit.edu\n  User USERNAME\n\nHost satori-compute\n  User USERNAME\n  HostName node1234\n  ProxyJump satori-login\n</code></pre> config<pre><code>Host om-login\n  HostName openmind7.mit.edu\n  User USERNAME\n\nHost om-compute\n  User USERNAME\n  HostName node1234\n  ProxyJump om-login\n</code></pre> <p>Where <code>USERNAME</code> is replaced by your username.</p> <p>This screenshot shows updating the config file for an interactive job running on Engaging:</p> <p></p> <p>Since the interactive job in my screenshot is running on <code>node020</code>, I have updated <code>HostName</code> to <code>node-020</code> for the <code>eofe-compute</code> entry in my config file.</p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#connect-to-the-compute-node","title":"Connect to the Compute Node","text":"<p>You are ready to connect to the compute node you have allocated through your interactive job from VSCode. Select the \"Open a Remote Window\" button in the bottom left corner of your VSCode window. In the bar at the top of the page select \"Connect to Host...\" and select the Host for the compute node that you have created.</p> EngagingSatoriOpen Mind <p>In the example config file above this would be <code>eofe-compute</code>.</p> <p>In the example config file above this would be <code>satori-compute</code>.</p> <p>In the example config file above this would be <code>om-compute</code>.</p> <p>Here is what this might look like for Engaging:</p> <p></p>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"recipes/vscode/#other-vscode-best-practices-tips-and-tricks","title":"Other VSCode Best Practices, Tips, and Tricks","text":"<ul> <li>Avoid running VSCode through RemoteSSH on the login nodes. If you are only editing files this might be okay, although it is not encouraged. Beyond editing files please use a compute node for VSCode, as described on this page.</li> <li>Add the specific directories you need to your workspace. VSCode constantly scans all the files files and runs git commands on any local git repositories in your workspace, and it does this recursively. For this reason adding high-level directories to your workspace can slow things down quite a bit. For example, avoid adding your entire home directory or group storage to your VSCode session workspace.</li> <li>If you are having trouble authenticating, particularly if you are prompted for a password or 2 Factor authentication options, you can set <code>\"remote.SSH.showLoginTerminal\": true</code> in your settings.json file. See this page for more information.</li> <li>If VSCode is slow to start up on an ORCD System, check to see whether you are activating a conda environment at login. If you are, run the command <code>conda config --set auto_activate_base false</code> to prevent this. You will only have to do this once.</li> </ul>","tags":["vscode","Howto Recipes","Best Practices"]},{"location":"software/compile/","title":"Compiling Source Code and GNU Make","text":"<p>This page covers the basics of building programs from C source code, and automating this process using GNU Make. It is intended for scientists venturing into scientific programming, to help ease the frustrations that typically come up when starting to work in compiled programming languages.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#preparation","title":"Preparation","text":"Engaging <p>A GCC compiler is needed to compile codes. Load a GCC module first,   <pre><code>module load gcc/12.2.0\n</code></pre></p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#building-a-single-file-program","title":"Building a single-file program","text":"<p>Let's start with a simple example: building a \"hello world\" C program with the GCC compiler. The program (hello.c) looks like this: <pre><code>#include &lt;stdio.h&gt;\nint main()\n{\n    printf(\"Hello World\\n\");\n    return (0);\n}\n</code></pre></p> <p>To build a working executable from this file, run: <pre><code>gcc hello.c -o hello\n</code></pre></p> <p>This command creates an executable with a name of hello. Running this command prints the familiar message: <pre><code>$ hello\nHello World\n</code></pre></p> <p>More happened here behind the scene. In fact, this command wraps up 4 steps of the build process: Preprocess, Compile, Assemble, and Link.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#step-1-preprocess","title":"Step 1: Preprocess","text":"<p>In this step, <code>gcc</code> calls preprocessing program <code>cpp</code> to interpret preprocessor directives and modify the source code accordingly.</p> <p>Some common directives are:</p> <ul> <li> <p><code>#include</code>: includes contents of the named file, typically a header file, e.g. <code>#include &lt;stdio.h&gt;</code>.</p> </li> <li> <p><code>#define</code>: macro substitution, e.g. <code>#define PI 3.14159</code>.</p> </li> <li> <p><code>#ifdef ... #end</code>: conditional compilation, the code block is included only if a certain macro is defined, e.g: <pre><code>#ifdef TEST_CASE\n  a=1; b=0; c=0;\n#endif\n</code></pre></p> </li> </ul> <p>We could perform just this step of the build process like so: <pre><code>cpp hello.c hello.i\n</code></pre></p> <p>Examining the output file (<code>vim hello.i</code>) shows that the long and messy stdio.h header has been appended to our simple code. </p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#step-2-compile","title":"Step 2: Compile","text":"<p>In this step, the (modified) source code is translated from the C programming language into assembly code.</p> <p>Assembly code is a low-level programming language with commands that correspond to machine instructions for a particular type of hardware. It is still just plain text, that says you can read assembly and write it too if you so desire.</p> <p>To perform just the compilation step of the build process, we would run: <pre><code>gcc -S -c hello.i -o hello.s\n</code></pre></p> <p>Examining the output file (<code>vim hello.s</code>) shows processor-specific instructions needed to run our program on this specific system. Interestingly, for such a simple program as ours, the assembly code is actually shorter than the preprocesses source code (though not the original source code).</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#step-3-assemble","title":"Step 3: Assemble","text":"<p>Assembly code is then translated into object code. This is a binary representation of the actions your computer needs to take to run your program. It is no longer human-readable, but it can be understood by computers.</p> <p>To perform just this step of the build process, we would run: <pre><code>gcc -c hello.s -o hello.o\n</code></pre></p> <p>You can try to view this object file like we did the other intermediate steps (<code>vim hello.o</code>), but the result will not be useful . Your text editor is trying to interpret binary machine language commands as ASCII characters, and (mostly) failing. Perhaps the most interesting result of doing so is that there are intelligable bits --- these are the few variables, etc, that actually are ASCII characters.</p> <p>Also note that object files are not executables, you can't run them until after the next step.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#step-4-link","title":"Step 4: Link","text":"<p>In the final step, <code>gcc</code> calls the linker program <code>ld</code> to combine the object file with any external functions it needs (e.g. library functions or functions from other source files). In our case, this would include <code>printf</code> from the C standard library.</p> <p>To perform just this step of the build process, we would run: <pre><code>gcc hello.o -o hello\n</code></pre> This produces the executable <code>hello</code> finally. </p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#building-a-multi-file-program","title":"Building a multi-file program","text":"<p>For most projects in the real world, it is convenient to break up the source code into multiple files. Typically, these include a main function in one file, and one or more other files containing functions / subroutines called by <code>main()</code>. In addition, a header file is usually used to share custom data types, function prototypes, preprocessor macros, etc.</p> <p>As an example, we create several source code files in a directory named multi_string, which consists of:</p> <ul> <li>main.c: the main driver function, which calls a subroutine and exits</li> <li>WriteMyString.c: a module containing the subroutine called by main</li> <li>header.h: one function prototype and one macro definition</li> </ul> Source codes for the multi_string program <p>main.c:  <pre><code>#include \"header.h\"\n#include &lt;stdio.h&gt;\nchar    *AnotherString = \"Hello Everyone\";\nmain()\n{\n  printf(\"Running...\\n\");\n  WriteMyString(MY_STRING);\n  printf(\"Finished.\\n\");\n}\n</code></pre></p> <p>WriteMyString.c: <pre><code>#include &lt;stdio.h&gt;\nextern char *AnotherString;\nvoid WriteMyString(char *ThisString)\n{\n  printf(\"%s\\n\", ThisString);\n  printf(\"Global Variable = %s\\n\", AnotherString);\n}\n</code></pre></p> <p>header.h:  <pre><code>#define MY_STRING \"Hello World\"\nvoid WriteMyString();\n</code></pre></p> <p>The easiest way to compile such a program is to include all the required source files at the <code>gcc</code> command line: <pre><code>gcc main.c WriteMyString.c -o my_string\n./my_string\n</code></pre></p> <p>It is also quite common to separate out the process into two steps:</p> <ol> <li> <p>source code -&gt; object code <pre><code>gcc -c WriteMyString.c\ngcc -c main.c\n</code></pre></p> </li> <li> <p>object code -&gt; executable (or library) <pre><code>gcc WriteMyString.o main.o -o my_string\n</code></pre></p> </li> </ol> <p>The reason is that this allows you to reduce compiling time by only recompiling objects that need to be updated. This seems silly for a program with only a few source files, but becomes important when many source files are involved. We will use this approach later when we discuss automating the build process.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#including-header-files","title":"Including header files","text":"<p>In the above process, it is not necessary to include the header file explicitly on the <code>gcc</code> command line. This makes sense since we know that the (bundeled) preprocessing step will append any required headers to the source code before it is compiled.</p> <p>There is one caveat: the preprocessor must be able to find the header files in order to include them. Our example works because the header.h file is in the current directory when we run <code>gcc</code>. We can break it by moving the header to a new subdirectory, like so: <pre><code>mkdir include\nmv header.h include\ngcc main.c WriteMyString.c -o my_string\n</code></pre></p> <p>The above commands give the output error: <pre><code>main.c:4:10: fatal error: header.h: No such file or directory\n    4 | #include \"header.h\"\n      |          ^~~~~~~~~~\ncompilation terminated.\n</code></pre></p> <p>We can fix this by specifically telling gcc where it can find the requisite headers, using the <code>-I</code> flag: <pre><code>gcc -I ./include main.c WriteMyString.c -o my_string\n</code></pre></p> <p>This is most often needed in the case where you wish to use external libraries installed in non-standard locations. We will explore this case in the next section. </p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#linking-external-libraries","title":"Linking external libraries","text":"<p>A library is a collection of pre-compiled object files that can be linked into your programs via the linker. In simpler terms, they are machine code files that contain functions / subroutines that you can use in your programs.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#shared-libraries-vs-static-libraries","title":"Shared libraries vs static libraries","text":"<p>A static library has file extension of .a (meaning archive file). When your program links a static library, the machine code of external functions used in your program is copied into the executable. At runtime, everything your program needs is wrapped up inside the executable.</p> <p>A shared library has file extension of .so (meaning shared objects). When your program is linked against a shared library, only a small table is created in the executable. At runtime, the exectutable must be able to locate the functions listed in this table. This is done by the operating system - a process known as dynamic linking.</p> <p>Static libraries certainly seem simpler, but most programs use shared libraries and dynamic linking. There are several reasons why the added complexity is thought to be worth it:</p> <ul> <li>Makes executable files smaller and saves disk space, because one copy of a library can be shared between multiple programs.</li> <li>Most operating systems allow one copy of a shared library in memory to be used by all running programs, saving memory.</li> <li>If your libraries are updated, programs using shared libraries automatically take advantage of these updates, programs using static libraries would need to be recompiled.</li> </ul> <p>Because of the advantage of dynamic linking, GCC will prefer a shared library to a static library if both are available (by default). We will only use shared libraries in the following. </p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#building-with-libraries-in-default-known-locations","title":"Building with libraries in default (known) locations","text":"<p>Many useful fuctions are provided by libraries in the operating system. These are two widely-used examples:</p> <ul> <li><code>printf()</code> from the libc.so shared library</li> <li><code>sqrt()</code> from the libm.so shared library</li> </ul> <p>In this section, we will introduce how to build a pgoram with shared libraries in the system default locations. Let's start with an example (roots.c) that uses the <code>sqrt()</code> function from the math library: <pre><code>#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\nvoid main()\n{ \n    int i;\n\n    printf(\"\\t Number \\t\\t Square Root of Number\\n\\n\");\n\n    for (i=0; i&lt;=360; ++i)\n        printf(\"\\t %d \\t\\t\\t %d \\n\", i, sqrt((double) i));\n\n}\n</code></pre></p> <p>Notice the function <code>sqrt</code>, which we use, but do not define. The (machine) code for this function is stored in libm.so, and the function definition is stored in the header file math.h.</p> <p>To build successfully, we must:</p> <ol> <li>Include the header file for the external library;</li> <li>Instruct the linker to link to the external library.</li> </ol> <p>We build the program using the two-step scheme: <pre><code>gcc -c roots.c\ngcc roots.o -lm -o roots\n</code></pre></p> <p>The first command preprocesses roots.c, appending the header files, and then translates it to object code. This step does need to find the header file, but it does not yet require the library.</p> <p>The second command links all of the object code into the executable. It does not need to find the header file, which has already been compiled into roots.o, but it does need to find the library file.</p> <p>Library files are linked using the <code>-l</code> flag. Their names are given excluding the lib prefix and exluding the .so suffix, which translates libm.so into <code>m</code> in this case. So we use <code>-lm</code> in the command. </p> <p>Just as we did above, we can combine the two steps into a single command: <pre><code>gcc roots.c -lm -o roots\n</code></pre></p> <p>Finally, we can run the programm: <pre><code>./roots\n</code></pre></p> <p>Note</p> <p>Because we are using shared libraries, the linker must be able to find the linked libraries at runtime, otherwise the program will fail. You can check the libraries required by a program, and whether they are being found correctly or not using the <code>ldd</code> command. For out roots program, we get the following <pre><code>$ ldd roots\nlinux-vdso.so.1 (0x00007ffd2c962000)\nlibm.so.6 =&gt; /lib64/libm.so.6 (0x00007fceadbef000)\nlibc.so.6 =&gt; /lib64/libc.so.6 (0x00007fcead82a000)\n/lib64/ld-linux-x86-64.so.2 (0x00007fceadf71000)\n</code></pre></p> <p>This shows that our executable requires a few basic system libraries such as libc.so as well as the math library <code>libm.so</code> we explicitly included, and that all of these dependencies are found by the linker.</p> Side note: where does the preprocessor look to find header files? <p>The preprocessor will search some default paths for included header files. Before we go down the rabbit hole, it is important to note that you do not have to do this for a typical build, but the commands may prove useful when you are trying to work out why something fails to build.</p> <p>To look for the header, we can run the following command to show the preprocessor search path: <pre><code>cpp -Wp,-v\n</code></pre> The output show the paths where GCC will search for header files by default. </p> Side note: where does the linker look to find libraries? <p>The linker will search some default paths for library files. Again, it is important to note that you do not have to do this for a typical build, but the commands may prove useful when you are trying to work out why something fails to build.</p> <p>To look for the library, we can run the following command to get a list of all library files the linker is aware of,  <pre><code>ldconfig -p \n</code></pre> or search that list for the math library we need: <pre><code>ldconfig -p | grep libm.so\n</code></pre> The latter command gives the output: <pre><code>libm.so.6 (libc6,x86-64, OS ABI: Linux 3.2.0) =&gt; /lib64/libm.so.6\n</code></pre> which shows that the math library is available. </p> <p>We might also want to peek inside a library file (or any object code for that matter) to see what functions and variables are defined within. We can list all the names, then search for the one we care about, like so: <pre><code>nm /lib64/libm.so.6 | grep \" sqrt\"\n</code></pre> The output of this command contains the following line, which shows that it does indeed include something called <code>sqrt</code>. <pre><code>000000000000f7d0 W sqrt\n</code></pre></p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#building-with-libraries-in-non-default-unknown-locations","title":"Building with libraries in non-default (unknown) locations","text":"<p>In many cases, you may need to use external libraries that are not included in the operating system. These libraries can be built by you or other develepers and they are saved in non-default locations. In this section, we will introduce how to build a program with libraries in non-default locations. </p> <p>Let's switch to a new example code. We create a source code named use_ctest.c that reads the following: <pre><code>#include &lt;stdio.h&gt;\n#include \"ctest.h\"\n\nint main(){\n    int x;\n    int y;\n    int z;\n    ctest1(&amp;x);\n    ctest2(&amp;y);\n    z = (x / y);\n    printf(\"%d / %d = %d\\n\", x, y, z);\n    return 0;\n}\n</code></pre> This code calls two functoins <code>ctest1</code> and <code>ctest2</code>, which are included in a custom library named ctest.</p> Side note: building a library <p>In the same level of the main code use_ctest.c, we create a directory named ctest_dir to save all files related to the library ctest.  <pre><code>mkdir ctest_dir\n</code></pre></p> <p>First, create a subdirectory named <code>src</code>,  <pre><code>cd ctest_dir\nmkdir src\ncd src\n</code></pre> and save the following two source code files in there. </p> <p>ctest1.c: <pre><code>void ctest1(int *i){\n  *i=100;\n}\n</code></pre></p> <p>ctest2.c: <pre><code>$ cat ctest2.c \nvoid ctest2(int *i){\n  *i=5;\n}\n</code></pre></p> <p>Each code does nothing but defines an interger.</p> <p>Second, create another subdirectory named <code>include</code>, <pre><code>mkdir include\n</code></pre> and save the following header file in there, ctest.h: <pre><code>#ifndef CTEST_H\n#define CTEST_H\nvoid ctest1(int *);\nvoid ctest2(int *);\n#endif\n</code></pre> This is for the declaration of the funtcions.</p> <p>Third, use the following commands to build the shared library named <code>libctest.so</code>: <pre><code>gcc -Wall -fPIC -c ctest1.c ctest2.c\ngcc -shared -Wl,-soname,libctest.so -o libctest.so ctest1.o ctest2.o\n</code></pre></p> <p>Finally, we move the library to a directory named lib,  <pre><code>cd ..\nmkidr lib\nmv src/libctest.so lib\n</code></pre></p> <p>Assuming that the library ctest has been built (as instructed in the above side note), we will build the program use_ctest and fix possbile errors in the process.</p> <p>First, we start with the simplest command: <pre><code>gcc -c use_ctest.c\n</code></pre> It fails with an error: <pre><code>use_ctest.c:2:10: fatal error: ctest.h: No such file or directory\n</code></pre></p> <p>As the error message indicates, the problem here is that an included header file is not found by the preprocessor. We know that we can use the <code>-I</code> flag to fix this problem: <pre><code>gcc -I ctest_dir/include -c use_ctest.c\n</code></pre> then it creates an object file use_ctest.o.</p> <p>The next step is to use the linker to create an executable. As we have known, we need to explicitly add the library with the <code>-l</code> flag, <pre><code>gcc use_ctest.o -lctest -use_ctest\n</code></pre> but in this case we still get an error: <pre><code>/usr/bin/ld: cannot find -lctest\ncollect2: error: ld returned 1 exit status\n</code></pre> Just like for the header, we need to explicitly specify the path to the library file using <code>-L</code>: <pre><code>gcc -L ctest_dir/lib  use_ctest.o -lctest -o use_ctest\n</code></pre> An executable use_ctest is created successfully! </p> <p>Howerver, what happens when we try to run our shiny new executable? <pre><code>$ ./use_ctest \n./use_ctest: error while loading shared libraries: libctest.so: cannot open shared object file: No such file or directory\n</code></pre></p> <p>Frustrating? No worry. This is a commonly seen error. We can diagnose this problem by checking to see if the dynamic linker is able to gather up all the dependencies at runtime: <pre><code>$ ldd use_ctest\n    linux-vdso.so.1 (0x00007fff56d9d000)\n    libctest.so =&gt; not found\n    libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f7f46df6000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007f7f471bb000)\n</code></pre></p> <p>The output clearly shows that it does not. The problem here is that the dynamic linker will only search the system default paths. There are a few solutions. </p> <ul> <li> <p>Permanently add our custom library to one of the system default paths. This option needs root permissoins, which is not available for HPC users and thus is not recommended here. </p> </li> <li> <p>Specify the location of libraries using the <code>LD_LIBRARY_PATH</code> environment variable. <code>LD_LIBRARY_PATH</code> contains a colon (:) separated list of directories where the dynamic linker should look for shared libraries. The linker will search these directories before the default system paths. You can define the value of <code>LD_LIBRARY_PATH</code> like so: <pre><code>export LD_LIBRARY_PATH=./ctest_dir/lib:$LD_LIBRARY_PATH\n</code></pre> and then run the program: <pre><code>$ ./use_ctest \n100 / 5 = 20\n</code></pre> It works! </p> </li> <li> <p>Hard-code the location of libraries into the executable. Setting (and forgeting to set) <code>LD_LIBRARY_PATH</code> all the time can be tiresome. An alternative approach is to burn the location of the shared libraries into the executable as an <code>RPATH</code> or <code>RUNPATH</code>. This is done by adding some additional flags for the linker, like so: <pre><code>gcc -L ctest_dir/lib use_ctest.o -lctest -Wl,--rpath=ctest_dir/lib -o use_ctest_1\n</code></pre> We can confirm that this works by running the program: <pre><code>$ ./use_ctest_1\n100 / 5 = 20\n</code></pre> or examining the executable to show that it contains the needed library: <pre><code>$ readelf -d use_ctest_1 |grep ctest\n 0x0000000000000001 (NEEDED)             Shared library: [libctest.so]\n 0x000000000000000f (RPATH)              Library rpath: [ctest_dir/lib]\n</code></pre></p> </li> </ul>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#automating-the-build-process-with-gnu-make","title":"Automating the build process with GNU Make","text":"<p>The manual build process we used above can become quite tedious in real world. There are many ways that we might automate this process. The simplest would be to write a shell script that runs the build commands each time we invoke it. Let's take the simple hello.c program as a test case. Here is a bash shell script (named build.sh) to automate the building process, <pre><code>#!/bin/bash\ngcc -c hello.c\ngcc hello.o -o hello\n</code></pre> Run it like so: <pre><code>chmod +X build.sh\n./build.sh\n</code></pre></p> <p>This works fine for small projects, but for large multi-file projects, we would have to compile all the source codes every time we change anything in the codes.</p> <p>The GNU Make utility provides a useful way around this problem. The solution is that we (the programmers) write a special script that defines all the dependencies between source files, edit one or more files in our project, then invoke Make to re-compile only those files that have been changed.</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#how-gnu-make-works","title":"How GNU Make works","text":"<p>GNU Make is a mini-programming language unto itself. To start, we need to create a file named Makefile or makefile to define a set of tasks to be executed. For the simple hello program, a Makefile is like this: <pre><code>hello: hello.o\n    gcc hello.o -o hello\n\nhello.o: hello.c\n    gcc -c hello.c\n\nclean:\n    rm hello hello.o\n\n.PHONY: clean\n</code></pre></p> <p>We can see that each section starts with a line specifyting dependency like so: <code>target: prerequisites</code>. The command block that follows will be executed to generate the target if any of the prerequisites have been modified. </p> <p>Once a Makefile is ready, the program can be built by executing one single command, <pre><code>make\n</code></pre> It looks for the Makefile in the same directory and build the targets. The first (top) target will be built by default, or you can specify a specific target to be built,  <pre><code>make hello\n</code></pre></p> <p>When we run <code>make</code>, the computer will take the following actions:</p> <ol> <li> <p>Find the default target, which is our executable file hello.</p> </li> <li> <p>Check if the target file hello is up-to-date. A target is considered not up-of-date if it does not exist or is older than any of the prerequisites. As hello does not exist, so it will be built.</p> </li> <li> <p>The prerequisite of hello is hello.o, which is also a target, so check if it is up-to-date. As hello.o does not exist, so it will be built.</p> </li> <li> <p>The prerequisite of hello.o is hello.c, which is not a target, so there is nothing left to check. The command <code>gcc -c hello.c</code> will be run to create hello.o.</p> </li> <li> <p>Now hello.o is up-to-date, so the next target hello will be built by running the command <code>gcc hello.o -o hello</code>.</p> </li> </ol> <p>Note that the command under the clean target is not executed by <code>make</code>, because it is neither the first target nor an prerequisite of any other target. To bring this target up, we need to specify the target name: <pre><code>make clean\n</code></pre> It will remove the executable and all of the <code>.o</code> files. Note that if all targets are up-to-date, make does not do anything, so we need to run <code>make clearn</code> every time before rebuilding the program. </p> <p>Let's create a <code>Makefile</code> for the multi-file example program mentioned in previous sections: <pre><code>write: main.o WriteMyString.o\n        gcc main.o WriteMyString.o -o write\n\nmain.o: main.c header.h\n        gcc -c main.c\n\nWriteMyString.o: WriteMyString.c\n        gcc -c WriteMyString.c\n\nclean: \n        rm write *.o\n</code></pre></p> <p>If it is the first build, make builds the targets in the following order: main.o, WriteMyString.o and write. This compiles all source codes and links object files to build the executable. If it is not the first build, make will only build the targets whose prerequisite has been modified since last make. This feature makes it efficient for building a program with many source files. For example, if WriteMyString.c is modified, only WriteMyString.c is recompiled, while main.c is not. If main.c or header.h is modified, only main.c is recompiled, while WriteMyString.c is not. In either case, the write target will be built, since either main.o or WriteMyString.o is updated.</p> <p>By default, make prints on the screen all the commands that it executes. To suppress the print, add an @ before the commands, or turn on the silent mode with the option -s:</p> <p>make -s</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/compile/#writing-a-good-makefile","title":"Writing a good Makefile","text":"<p>A Makefile could be very compilcated in a practical program with many source files. It is important to write a Makefile in good logic. The text in the Makefile should be as simple and clear as possbile. To this end, useful features of Makrefile are introduced in this section.</p> <p>You may have noticed that there are many duplications of the same file name or command name in our previous Makefiles. It is more convinient to use varialbes. Again, take our first multi-file program for example. The Makefile can be rewitten as following,</p> <p>CC=gcc OBJ=main.o WriteMyString.o EXE=write</p> <p>$(EXE): $(OBJ)         $(CC) $(OBJ) -o $(EXE)</p> <p>main.o: main.c header.h         $(CC) -c main.c</p> <p>WriteMyString.o: WriteMyString.c         $(CC) -c WriteMyString.c</p> <p>clean:         rm $(EXE) *.o Here we have defined the varialbes CC for the compiler, OBJ for object files and EXE for the executable file. If we want to change the compiler or the file names, we only modify the corresponding variables at one place, but do not need to modify all related places in the Makefile.</p> <p>Furthermore, we can upgrade the Makefile to a higher automatic level using the so-called \"automatic variables\": $(EXE): $(OBJ)         $(CC) $^ -o $@</p> <p>main.o: main.c header.h         $(CC) -c $&lt;</p> <p>WriteMyString.o: WriteMyString.c         $(CC) -c $&lt;  Here we have used the following automatic variables:</p> <p>$@  --- the name of the current target $^  --- the names of all the prerequisites $&lt;  --- the name of the first prerequisite These automatic variables automatically take the names of current target or prerequisites, no matter what values are assigned to them.</p> <p>We can notice that the main.o and WriteMyString.o targets are built by the same command. Is there a way to combine the two duplicated commands into one so as to compile all source code files by one command line? Yes, it can be done with an implicit pattern rule:</p> <p>%.o: %.c         $(CC) -c $&lt;</p> <p>main.o: header.h  Here % stands for the same thing in the prerequisites as it does in the target. In this example, any .o target has a corresponding .c file as an implied prerequisite. If a target (e.g. main.o) needs additional prerequisites (e.g. header.h), write an actionless rule with those prerequisites. We can imagine that applying this impilict rule should significantly simpify the Makefile when there are a large number of (say hundreds of) source files.</p> <p>If there are many varialbes to be defined, it is convinient to write the definition of all variables in another file, and then include the file in Makefile:</p> <p>include ./variables  The content of the file variables is as following:</p> <p>CC=gcc OBJ=main.o WriteMyString.o EXE=write In most cases, the target name is a file name. But there are exceptions, such as the clean target in this example. The rm command will not create any file named clean. What if there exists a file named clean in this directory? Let's do an experiment.</p> <p>touch clean make clean make: `clean' is up to date. The clean target does not work properly. Since it has no prerequisite, clean will always be considered up-to-date, and thus nothing will be done. To solve this issue, we can declare the target to be phony by making it a prerequisite of the special target .PHONY as follows: .PHONY: clean A phony target is one that is not really the name of a file; rather it is just a name for a recipe to be executed. Finally, we end up with an efficient Makefile:</p> <p>include ./variables .PHONY: clean</p> <p>$(EXE): $(OBJ)         $(CC) $^ -o $@</p> <p>%.o: %.c         $(CC) -c $&lt;</p> <p>main.o: header.h</p> <p>clean:         rm $(EXE) *.o</p>","tags":["Software","C/C++","Engaging","OpenMind"]},{"location":"software/modules/","title":"Modules","text":"<p>Modules are a handy way to set up your environment for particular work, especially in a shared environment. They provide an easy way to load a particular version of a software, language, or compiler.</p> <p>To see what modules are available, type the command:</p> <pre><code>module avail\n</code></pre> <p>Note</p> <p>By default you will only see the modules for core software. To see community modules (available on Engaging) run the command: <pre><code>module use /orcd/software/community/001/modulefiles\n</code></pre> Modules labeled <code>centos7</code> are built for Centos 7 nodes, modules labeled <code>rocky8</code> are built for Rocky 8 nodes, and models labeled <code>linux</code> should work on either.</p> <p>To load a module, use the command:</p> <pre><code>module load moduleName\n</code></pre> <p>Where <code>moduleName</code> can be any of the modules listed by the <code>module avail</code> command.</p> <p>Note</p> <p>We do not recommend including <code>module load</code> commands in your <code>.bashrc</code>, <code>.bash_profile</code>, or any other startup scripts and instead include them in your job scripts. This provides a more predictable and consistent environment for your jobs. It is also very easy to forget that you have modules loaded in your <code>.bashrc</code>, and these can have impact on future workloads.</p> <p>If you want to list the modules you currently have loaded, you can use the <code>module list</code> command:</p> <pre><code>module list\n</code></pre> <p>If you want to change to a different version of the module you have loaded, you can switch the module you have loaded. This is important to do when loading a different version of a module you already have loaded, the module command will not allow you to load two different versions of the same software. To switch modules run:</p> <pre><code>module switch oldModuleName newModuleName\n</code></pre> <p>where <code>oldModuleName</code> is the name of the module you currently have loaded, and <code>newModuleName</code> is the new module that you would like to load.</p> <p>If you would like to unload the module, or remove the changes the module has made to your environment, use the following command:</p> <pre><code>module unload moduleName\n</code></pre> <p>If you would like to unload all modules in your environment, you can use the command:</p> <pre><code>module purge\n</code></pre> <p>This command is helpful when you want to ensure a clean environment. You can include it at the start of your job scripts to make sure your jobs all have a consistent environment. Loaded modules may carry over into your jobs, and sometimes can interfere with the work you are doing in unexpected ways.</p>","tags":["Software","Modules"]},{"location":"software/overview/","title":"Software Overview","text":"<p>Each ORCD system has its own software stack. Many basic and commonly used software and libraries are already installed, so it is good to check before spending the time to install it yourself. This page discusses the general overview of what kinds of software are supported and points you to how to use them and what to do if what you need isn't there.</p>","tags":["Software"]},{"location":"software/overview/#software-landscape","title":"Software Landscape","text":"<p>While the software stack will be different on each system, there are three general classes of software:</p> Category Description Core Commonly used or fundamental software and libraries that are fully supported. Core software is expected to work until it is officially deprecated, and often newer versions are provided to replace them. Community Software that has been built and installed by request, but is not commonly used. Support is on a best-effort basis. Should work when built but not guaranteed to work indefinitely or when replaced with newer versions when deprecated, except by request. Deprecated Software that is no longer supported or expected to work. May be kept for legacy reasons, or will soon be removed. If software you are using is listed as deprecated or soon to be deprecated migrate to the newer version (if available) or request a newer version (if not available). If migrating to a newer version is not an option you may be able to run your application with Singularity. <p>Each individual ORCD system may not label or organize their software in this way. However, this is the support model that will be used going forward. Engaging, in particular, will organize its software in this way.</p> <p>Here are some general notes by system on this for Engaging and SuperCloud. Click on the tab for the system you are interested in:</p> EngagingSuperCloud <p>Engaging nodes are one of two operating systems: Centos 7 and Rocky 8. Each operating system has its own software stack.</p> <p>Centos 7 has been around for longer, so it has more software installed. These nodes have a very large list of modules, older ones that no longer work have not necessarily been removed but can be considered deprecated. Centos 7 nodes will either be retired or migrated to Rocky 8 in the near future, so when given the choice use Rocky 8 nodes.</p> <p>Rocky 8 nodes have a significantly shorter list of modules and are organized into core, community, and deprecated. Core software will be displayed by default, community and deprecated software will require a \"module use\" command to display. See the page on modules for more information.</p> <p>The SuperCloud software stack is managed by the Lincoln Laboratory Supercomputing Center. The modules listed are considered \"core\" software. Deprecated software that was part of the core stack is removed from the system. \"Community\" software is provided in the llgrid_beta directory in the groups location. Anyone can use them but they are not officially supported.</p>","tags":["Software"]},{"location":"software/overview/#steps-for-getting-software","title":"Steps for Getting Software","text":"<p>One of the first steps for getting a workflow running on a new system is to set up any software or packages needed to run it. Here are a few steps to do that on an ORCD system.</p>","tags":["Software"]},{"location":"software/overview/#check-if-the-software-or-package-is-already-installed","title":"Check if the Software or Package is Already Installed","text":"<p>As mentioned above, there is a lot of software already installed. Using the software we've installed saves you time. This software may also perform better or be better configured to use the system. For example, it may be installed in a faster part of the filesystem or configured to use special hardware available on the cluster.</p> <p>For software check the <code>module avail</code> command (see the page on modules for more information). Some software is available without a module, you can check if a particular command is available using the <code>which</code> command at the command line. For example, run <code>which git</code> to see if the <code>git</code> command is available. If it is, the path to the <code>git</code> command will print to the screen.</p> <p>Common languages like Python, Julia, and R are provided through modules as well. Packages for these are sometimes provided along with the installation. A quick way to check if a package is available is to try to import it.</p>","tags":["Software"]},{"location":"software/overview/#install-the-software-or-package","title":"Install the Software or Package","text":"<p>If we don't have the software you need, you can often install it yourself. You will need to install them in your home directory or another directory you have access to. You will not be able to install software in any of the system-wide directories, as changes to these affect everyone using the system (for example you will not be able to install in any location that requires <code>sudo</code>).</p> Why can't I use sudo? <p>The <code>sudo</code> command is used to make system-wide administrator-level changes. On a system where you are the only user this is usually fine, the only person you can affect is yourself. On large shared systems with many users any command that uses <code>sudo</code> has the potential to affect the workflow of other researchers and potentially cause harm, even when it is not intentional. For this reason only trained system administrators have the ability to use <code>sudo</code>.</p> <p>You should not need <code>sudo</code> to install packages in your own space. For software installs, the <code>sudo</code> is only used to put the installation files in the system-wide directory, so it is not needed to install in your own directories. The Installing Software page covers how to specify installation directories for some of the more common build systems. </p> <p>Sometimes you can find pre-built binaries for the software you want. These are the easiest to install. Often you will need to build the software you need. See the page on Installing Software for more information. You may also check the Recipes section of these pages to see if there is an existing recipe for installing the software you are interested in.</p> <p>For Python, Julia, and R packages, each of these have their own package managers for installing packages. See the respective documentation pages linked above for each of these.</p> <p>Note</p> <p>Satori is unique in that its nodes have a different architecture than those in other ORCD systems. They are IBM machines with the ppc64le architecture. There is some software that does not support this architecture. When selecting pre-built software, be sure to select the one for ppc64le.</p>","tags":["Software"]},{"location":"software/overview/#ask-for-help","title":"Ask for Help","text":"<p>If you are having trouble installing software you can reach out to orcd-help@mit.edu or one of the other lists on Getting Help for help. You can also stop by office hours if you prefer. Depending on the software and the system you are using, we may help walk you through installing it for yourself or install it in a community location.</p>","tags":["Software"]},{"location":"software/python/","title":"Installing Python Packages","text":"<p>There are a few different ways to install Python packages. Each ORCD system has its own set of Python modules and naming conventions for those modules, along with a set of recommendations for installing Python packages. This page is meant to give a general overview and link to those pages.</p> EngagingSatoriSuperCloudOpenMind <p>Engaging Installing Python Packages Documentation</p> <p>Satori Installing Python Packages Documentation</p> <p>SuperCloud Installing Python Packages Documentation</p> <p>OpenMind Installing Python Packages Documentation</p> <p>Python packages will need to be installed in your home directory or other directory you have write access to. There are a few different ways to do this, each with its own pros and cons. At a high level, you can:</p> <ul> <li>Install packages in your own Python virtual environment (venv)</li> <li>Install packages in your own conda/mamba environment</li> <li>Install packages to you home directory space using the <code>pip install --user</code> command</li> </ul> <p>Which should you use? That can depend on a lot of things. Our recommendation will usually depend on the system, what you are doing, and which packages your are installing. Python virtual environments tend to be a good all-around option as a starting point. With them you can stay organized with environments, but they don't tend to take up as much space or create as many files as conda or mamba environments. Read through the pros and cons for each, they are meant to help you see when one might be better than another.</p>","tags":["Software","python"]},{"location":"software/python/#modules-for-python","title":"Modules for Python","text":"<p>Python is provided on all ORCD systems through either Python, Anaconda or similar modules. See the documentation for the system your are using below for a description of the python modules available. Some systems have some commonly used Python packages installed with their modules, so it is worth checking to see if these packages will satisfy your use case before installing your own.</p> <p>Refer to the tab below to find out more about the Python modules available on the system you are using.</p> EngagingSatoriSuperCloudOpenMind <p>Some nodes on Engaging have different operating systems (OS). The newest nodes on Engaging are Rocky 8 and older nodes are Centos 7. Each OS has a different software stack, and so has different sets of Python and Anaconda modules. Both will have both Python and Anaconda modules, but will may have different names and versions. Check <code>module avail</code> for this information. Be sure the OS of the login node you are using to launch jobs matches the OS of the compute nodes you are requesting.</p> <p>For Rocky 8 nodes we recommend using the miniforge module, currently available in the community software:</p> <pre><code>module use /orcd/software/community/001/modulefiles/rocky8/\nmodule load miniforge/23.11.0-0\n</code></pre> <p>Satori Python and Anaconda modules are rather old so the advice is to install your own miniforge or miniconda. Home directories on Satori are quite small, so the recommendation is to install these into <code>/nobackup/users/$USER</code> where you have more space.</p> <p>Once it is installed you will need to add channels to your conda configuration:</p> <pre><code>conda config --prepend channels https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda/\nconda config --prepend channels https://opence.mit.edu\nconda config --prepend channels https://ftp.osuosl.org/pub/open-ce/current\n</code></pre> <p>These channels contain packages compiled for Satori's PowerPC architecture. You only need to run these commands once to add these channels to your configuration file.</p> <p>SuperCloud releases two anaconda modules per year, named for the year and the release (anaconda/2024a and anaconda/2024b, for example). The most recent modules may continue to be updated until the next \"release\". There are separate modules for machine learning frameworks (ex: anaconda/Python-ML-2023b). SuperCloud anaconda modules contain a lot of the most commonly used packages and are installed on the local disk of each node, so it is best if you can use packages installed in the modules as much as possible. To enable that, SuperCloud recommends installing packages to you home directory space using the <code>pip install --user</code> command, or creating python virtual environments using the <code>--system-site-packages</code> flag. SuperCloud has a section on their Best Practices page about installing Python packages.</p> <p>OpenMind has both Anaconda and miniconda modules available. They have some of the most commonly used packages already installed.</p>","tags":["Software","python"]},{"location":"software/python/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>Environments allow you to make self-contained \u201cbundles\u201d of packages that can be loaded and unloaded. This helps keep a consistent set of packages and versions for a given project, rather than putting all packages you've ever installed together like they would be when you install with <code>pip install --user</code>. Python virtual environments can be placed anywhere you have write access to and have all their packages in that environment\u2019s directory structure.</p>","tags":["Software","python"]},{"location":"software/python/#creating-python-virtual-environments","title":"Creating Python Virtual Environments","text":"<p>To create a new environment, first load a Python or Anaconda module using the <code>module load</code> command. See the page on Modules for more information on how to load modules. See Modules for Python above for information about specific modules for the system you are using.</p> <p>To create a the environment use the <code>python -m venv</code> command: </p> <pre><code>python -m venv /path/to/virtual/environment\n</code></pre> <p>Note that you specify a path, and not only a name for the environment. This means you can place your environment wherever you'd like. Usually it is placed somewhere in the project directory it is used for. For example, if I want to create an environment for a project called <code>my_project</code> that lives in my home directory, I would run something like:</p> <pre><code>cd ~/my_project\npython3 -m venv my_project_env\n</code></pre> <p>This will create an environment at the path <code>~/my_project/my_project_env</code>.</p> <p>Note</p> <p>Virtual Environment documentation and tutorials will often tell you to run the command <code>python3 -m venv .venv</code> to create an environment. The <code>.</code> at the beginning of <code>.venv</code> creates a hidden directory in the current working directory. You won't see this directory if you run the <code>ls</code> command by itself, run <code>ls -a</code> to see hidden files and directories. If you are new to virtual environments and Linux we recommend using a descriptive name for your environment that is not hidden (doesn't start with <code>.</code>).</p> <p>By default environments will be fully isolated and Python will only see the packages you've installed in the environment. You can signal your virtual environment to \"see\" system installed packages by using the flag <code>--system-site-packages</code> when creating the environment. This is useful when the module you are using has packages already installed. For example:</p> <pre><code>python3 -m venv --system-site-packages my_project_env\n</code></pre> <p>Before installing packages or using an environment we need to activate the environment:</p> <pre><code>source /path/to/virtual/environment/bin/activate\n</code></pre> <p>Using our example above, if we are in the <code>my_project</code> directory already, we can run:</p> <pre><code>source my_project_env/bin/activate\n</code></pre> <p>Finally we can now install packages into the environment. Once activating the environment new packages can be installed with the <code>pip</code> command:</p> <pre><code>pip install pkgname\n</code></pre> <p>Note</p> <p>Do not use <code>--user</code> flag to install, this will install into <code>$HOME/.local</code> instead of the environment.</p> <p>When you are done using or installing packages into an environment you can deactivate it with the command:</p> <pre><code>deactivate\n</code></pre>","tags":["Software","python"]},{"location":"software/python/#using-python-virtual-environments","title":"Using Python Virtual Environments","text":"<p>In order to use an environment you will need to activate it as described above:</p> <pre><code>source /path/to/virtual/environment/bin/activate\n</code></pre> <p>Once it is activated any <code>python</code> commands will run in that environment and have access to the packages in the environment. To use an environment in a job activate the environment in your job script. An environment activated on the login node will not necessarily carry over to your job. Using the same example above, let's look at the script <code>myjob.sh</code> in the <code>my_project</code> directory:</p> myjob.sh<pre><code>#!/bin/bash\n\nsource my_project_env/bin/activate\n\npython myscript.py\n</code></pre> <p>When this job runs it will activate the environment in <code>my_project_env</code> and then run the python script <code>myscript.py</code> using the packages in that environment.</p>","tags":["Software","python"]},{"location":"software/python/#requirementstxt-and-virtual-environments","title":"Requirements.txt and Virtual Environments","text":"<p>Environments can be described by a <code>requirements.txt</code> file, which lists packages and optionally their versions. This file can be created from any existing virtual environment and used to re-create that environment. Version numbers are required to recreate the environment exactly.</p> <p>You can either create a <code>requirements.txt</code> file by hand by creating a file with one package name on each line, or you can create one from a currently active environment with the command:</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>Given a <code>requirements.txt</code> file you can use <code>pip</code> to install the packages in that file into your environment. First activate the environment, then install the packages with:</p> <pre><code>source /path/to/virtual/environment/bin/activate\npip install -r requirements.txt\n</code></pre>","tags":["Software","python"]},{"location":"software/python/#pros-and-cons-for-virtual-environments","title":"Pros and Cons for Virtual Environments","text":"<p>Pros</p> <ul> <li>Can be set to build \u201con top of\u201d the central installation packages (not default behavior, use the <code>--system-site-packages</code> flag when creating)</li> <li>Self-contained environments for each project help stay organized and avoid package dependency conflicts. For software development it allows you to keep better track of your package\u2019s dependencies so others know what they need to install.</li> <li>Virtual environments are self-contained, fairly lightweight and can be put anywhere easily without additional configuration files</li> </ul> <p>Cons</p> <ul> <li>Environments will be the same version of the python that you used to make them</li> <li>Only installs packages available through PyPI, cannot install anaconda/conda-forge distributed packages or libraries</li> </ul>","tags":["Software","python"]},{"location":"software/python/#conda-environments","title":"Conda Environments","text":"<p>Environments allow you to make self-contained \u201cbundles\u201d of packages that can be loaded and unloaded. This helps keep a consistent set of packages and versions for a given project, rather than putting them all together like they would be when you install with <code>pip install --user</code>. Conda environments are a bit different from Python virtual environments, by default they are stored in the <code>.conda</code> directory in your home directory. They also tend to contain a lot more files than Virtual Environments. You can also install some system libraries into conda environments, which can make installing packages with system library dependencies easier.</p> <p>You'll also see mention of mamba environments. Mamba and conda are nearly the same, however mamba has a different dependency solver. Mamba is often better and faster than conda at solving dependencies and picking packages, so we recommend using mamba whenever possible.</p>","tags":["Software","python"]},{"location":"software/python/#creating-conda-environments","title":"Creating Conda Environments","text":"<p>First, load a conda or Anaconda module using the <code>module load</code> command. See the page on Modules for more information on how to load modules. See Modules for Python above for information about specific modules for the system you are using.</p> <p>Note</p> <p>If you are using Satori you will need to add channels to include packages compiled for PowerPC:</p> <pre><code>conda config --prepend channels https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda/\nconda config --prepend channels https://opence.mit.edu\nconda config --prepend channels https://ftp.osuosl.org/pub/open-ce/current\n</code></pre> <p>See Modules for Python above for more information.</p> <p>If there is no anaconda module on the system you are using, or the modules available aren't sufficient for your work, we recommend installing miniforge or miniconda in your home directory. Wwe have had the most  success with miniforge, which is distributed by conda-forge and is packaged with mamba. It is best to avoid installing the full Anaconda as it is very big and can fill up your home directory. One of the most common reasons for slow logins, job startups, and package imports are from a full anaconda installation in the home directory.</p> <p>Note</p> <p>If you are using SuperCloud do not install miniconda or miniforge in your home directory. SuperCloud keeps up to date anaconda modules so installing your own is not necessary and will slow down your applications. See this Best Practices page on SuperCloud-Docs for more information.</p> <p>To create an environment you can use the <code>mamba create</code> or <code>conda create</code> command after loading your conda module:</p> <pre><code>module load conda_module\nmamba create -n my_env python=3.10 pkg1 pkg2 pkg3\n</code></pre> <p>or, using <code>conda</code>:</p> <pre><code>module load conda_module\nconda create -n my_env python=3.10 pkg1 pkg2 pkg3\n</code></pre> <p>where <code>conda_module</code> is the name of the conda or mamba module for the system you are using. In this example I am creating a conda environment named <code>my_env</code> with Python 3.10 and installing packages pkg1, pkg2, pkg3. We have found that conda/mamba creates more robust environments when you include all the packages you need when you create the environment.</p> <p>You can install additional packages by activating the environment and using the <code>mamba install</code> or <code>conda install</code> command. Again you would first load the appropriate module if it isn't already loaded:</p> <pre><code>module load conda_module\nsource activate my_env\nmamba install pkg4\n</code></pre> <p>where <code>conda_module</code> is the name of the conda or mamba module for the system you are using. Here I am showing with <code>mamba</code>, replace with <code>conda</code> if you have a conda environment. Packages that aren't available through conda channels can be installed with the <code>pip</code> command when the environment is activated:</p> <pre><code>module load conda_module\nsource activate my_env\npip install pkg5\n</code></pre> <p>Note</p> <p>To install packages with <code>pip</code> to a conda or mamba environment you should not include the <code>--user</code> flag. Further, if you are using a conda environment and want Python to only use packages in your environment, you can run the following two command:</p> <pre><code>export PYTHONNOUSERSITE=True\n</code></pre> <p>This will make sure your conda environment packages will be chosen before those that may be installed in your home directory.</p> <p>If you would like to use your conda environment in Jupyter, install the \"jupyter\" package into your environment. Once you have done that, you should see your conda environment listed in the available kernels.</p>","tags":["Software","python"]},{"location":"software/python/#using-conda-environments","title":"Using Conda Environments","text":"<p>Then, whenever you want to activate the environment, first load the anaconda module, then activate with <code>source activate my_env</code>. Using <code>source activate</code>\u00a0instead of <code>conda activate</code> allows you to use your conda environment at the command line and in submission scripts without additional steps. See Conda/Mamba Init in the Troubleshooting section below for more information.</p> <pre><code>module load conda_module\nsource activate my_env\n</code></pre> <p>To use a conda environment in a job you can usually add these lines to you job script:</p> myjob.sh<pre><code>#!/bin/bash\n\nmodule load conda_module\n\nsource activate my_env\n\npython myscript.py\n</code></pre> <p>If this isn't working, see the Troubleshooting section below on activating an environment in a job script.</p>","tags":["Software","python"]},{"location":"software/python/#pros-and-cons-for-conda-environments","title":"Pros and Cons for Conda Environments","text":"<p>Pros</p> <ul> <li>Environments can be created with any supported version of Python.</li> <li>Can install packages available through various conda channels as well as PyPI (packages installed with <code>pip</code>).<ul> <li>Conda channels include many system libraries, making packages with complicated dependencies easier to install.</li> </ul> </li> <li>Self-contained environments for each project help stay organized and avoid package dependency conflicts. For software development it allows you to keep better track of your package\u2019s dependencies so others know what they need to install.</li> </ul> <p>Cons</p> <ul> <li>Can not be set to build \u201con top of\u201d the central installation packages, which means basic package will be re-installed in your home directory.</li> <li>Can get very large and take up a lot of space in your home directory.</li> <li>It can sometimes be slower than other options.</li> </ul>","tags":["Software","python"]},{"location":"software/python/#home-directory-install","title":"Home Directory Install","text":"<p>First, load a Python or Anaconda module using the <code>module load</code> command. See the page on Modules for more information on how to load modules.</p> <p>Then, install the package with pip using the <code>--user</code> flag:</p> <pre><code>pip install --user packageName\n</code></pre> <p>Where <code>packageName</code> is the name of the package that you are installing.</p> <p>With <code>pip install --user</code> you are installing a single package and any missing dependencies. Pip will see any packages already installed in the central Python installation and won\u2019t reinstall those as log as they satisfy the dependency requirements. These get installed to:</p> <pre><code>$HOME/.local/lib/pythonV.V/site-packages\n</code></pre> <p>This location is usually first in Python\u2019s package search path, so Python will pick up any libraries installed here before centrally installed ones. The exceptions are:</p> <ul> <li>If you have the <code>PYTHONPATH</code> environment variable set, that location will be searched first</li> <li>You have the <code>PYTHONNOUSERSITE</code> environment variable set to True, this tells Python to remove it from the path</li> </ul>","tags":["Software","python"]},{"location":"software/python/#pros-and-cons-for-local-install","title":"Pros and Cons for .local Install","text":"<p>Pros</p> <ul> <li>The installs are usually pretty easy</li> <li>Only installs what is absolutely needed, allowing Python to use centrally installed packages</li> </ul> <p>Cons</p> <ul> <li>Keeping and tracking a consistent environment is harder, not great for package development or working on different projects with conflicting requirements</li> <li>Everything you\u2019ve installed is always in your environment, which can cause two issues:<ul> <li>The space can eventually get \u201cdirty\u201d or \u201ccorrupted\u201d, the easiest fix is to delete or rename <code>$HOME/.local</code> and start again</li> <li>You can run into package dependency conflict issues, which could be fixed by uninstalling packages no longer needed or by deleting or renaming <code>$HOME/.local</code> and starting again</li> </ul> </li> </ul>","tags":["Software","python"]},{"location":"software/python/#troubleshooting-python-package-issues","title":"Troubleshooting Python Package Issues","text":"","tags":["Software","python"]},{"location":"software/python/#check-your-python-executable","title":"Check your Python Executable","text":"<p>When you use the <code>python</code> command Linux will pick the first <code>python</code> executable it finds in your <code>$PATH</code>. If Python is not finding your installed packages it is possible that the <code>python</code> running is not what you expect. Run the command:</p> <pre><code>which python\n</code></pre> <p>This will print out the path to the <code>python</code> executable that is running. If it doesn't print the right one, check that you've activated your environment or loaded the module you were intending.</p>","tags":["Software","python"]},{"location":"software/python/#check-pythons-path","title":"Check Python's Path","text":"<p>Python has its own path that gets set when it looks for packages. This path depends on a few things, such as whether you have an environment loaded and how that environment is configured. When in doubt you can view this path in Python with the following commands:</p> <pre><code>import sys\nsys.path\n</code></pre> <p>Similar to the <code>PATH</code> environment variable, Python checks the locations on the path in the order they are listed and imports the first of the specified package it finds. If Python is loading the wrong version of a package, checking the path will tell you where to check for the wrong-version package. If Python can't find the package, the path will give you more information about where it is looking.</p>","tags":["Software","python"]},{"location":"software/python/#condamamba-init","title":"Conda/Mamba Init","text":"<p>If you use conda or mamba you may at some point be asked to run <code>conda init</code> or <code>mamba init</code>. These commands will edit your <code>.bashrc</code> file which gets run every time you log in. These additional lines will make permanent changes to your environment that could have unintended effects on your use of the system. This can cause issues, including slowing down your logins and affecting any software builds you try to do.</p> <p>The best thing to do is to never run these commands, or if you have, to remove the lines that they  have added to your <code>.bashrc</code> file. Instead of running <code>conda activate</code> or <code>mamba activate</code>, you can use the <code>source activate</code> command.</p> <p>If this doesn't work for you, as an alternative, you can run the following line:</p> <pre><code>source /path/to/conda/install/etc/profile.d/conda.sh\n</code></pre> <p>whenever you want to use the <code>conda activate</code> command. Be sure to replace <code>/path/to/conda/install</code> with the path to the conda installation, for example if you installed miniforge in your home directory the full path might be <code>$HOME/miniforge3/etc/profile.d/conda.sh</code>.</p> <p>If you would like to use <code>mamba activate</code>, run the following line as well:</p> <pre><code>source /path/to/conda/install/etc/profile.d/mamba.sh\n</code></pre> <p>Both of these lines can be run at the command line, or put in a job script or setup script. Note that environments activated at the command line before launching a job may not carry over to the job itself, so it is always best load these environments in your job script.</p> mycondajob.sh<pre><code>#!/bin/bash\n\nsource /path/to/conda/install/etc/profile.d/conda.sh\nsource /path/to/conda/install/etc/profile.d/mamba.sh\n\nmamba activate myenv # or `conda activate myenv` for conda environments\n\npython myscript.sh\n</code></pre> <p>If you feel strongly that you want to keep the setup that <code>conda init</code> gives you in your <code>.bashrc</code>, first be aware that it could cause issues and it might be something to look into removing when troubleshooting. Second, one of the things these lines do is activate the base environment associated with the conda installation, which is usually the source of most of the issues you can run into. You can set conda to not activate the base environment at login by running the following line:</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> <p>You only need to run this line once. It will edit a configuration file for conda (.condarc). You will still be able to run <code>mamba activate</code> or <code>conda activate</code>.</p>","tags":["Software","python"]},{"location":"software/python/#environment-is-not-activating-in-a-job-script","title":"Environment is not Activating in a Job Script","text":"<p>Getting your python virtual environment or conda/mamba environment to activate in a batch job can sometimes be finicky.</p> <p>Check your log file to be sure you aren't getting any errors when you try to activate the environment. They may give further instruction on what to do.</p> <p>Usually the cause is something in your environment that is interfering, these could be:</p> <ul> <li>The <code>PYTHONPATH</code> environment variable is set- this changes where Python looks for packages first. Best practice is to not use <code>PYTHONPATH</code> when possible.</li> <li>You have another environment activated (see Conda/Mamba Init).</li> <li>Your python script starts with a line like <code>#!/bin/python</code> or similar. This tells the system to run the script with a python executable that isn't part of your environment. Removing the line will fix the issue.</li> </ul> <p>Beyond this, there are a few things you can try. These won't necessarily fix your environment issues, but might work around them. First let's talk about virtual environments, then conda environments.</p>","tags":["Software","python"]},{"location":"software/python/#activating-a-virtual-environment-in-a-script","title":"Activating a Virtual Environment in a Script","text":"<p>Usually activating the environment in your job script before running your Python script is sufficient. </p> <p>If your environment isn't activating, specifying the full path to the python executable in your environment sometimes fixes things. Here is a sample job script:</p> myjob.sh<pre><code>#!/bin/bash\n\nsource /path/to/virtual/environment/bin/activate\n\n/path/to/virtual/environment/bin/python myscript.py\n</code></pre> <p>Be sure to replace <code>/path/to/virtual/environment</code> with the actual path to your virtual environment.</p>","tags":["Software","python"]},{"location":"software/python/#activating-a-condamamba-environment-in-a-script","title":"Activating a Conda/Mamba Environment in a Script","text":"<p>Usually loading an anaconda module and then running <code>source activate myenv</code> will work to activate a conda or mamba environment in a job script (as shown above).</p> <p>One thing to try is suggested above in Conda/Mamba Init:</p> mycondajob.sh<pre><code>#!/bin/bash\n\nsource /path/to/conda/install/etc/profile.d/conda.sh\nsource /path/to/conda/install/etc/profile.d/mamba.sh #optional\n\nconda activate myenv # or `mamba activate myenv` for mamba environments\n\npython myscript.sh\n</code></pre> <p>If this doesn't work sometimes adding <code>eval \"$(conda shell.bash hook)\"</code> in the line before activating the environment will make it work:</p> mycondajob.sh<pre><code>#!/bin/bash\n\nsource /path/to/conda/install/etc/profile.d/conda.sh\nsource /path/to/conda/install/etc/profile.d/mamba.sh #optional\neval \"$(conda shell.bash hook)\"\n\nconda activate myenv # or `mamba activate myenv` for mamba environments\n\npython myscript.sh\n</code></pre>","tags":["Software","python"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#best-practices","title":"Best Practices","text":"<ul> <li>VSCode</li> </ul>"},{"location":"tags/#cc","title":"C/C++","text":"<ul> <li>Compiling Source Code and GNU Make</li> </ul>"},{"location":"tags/#engaging","title":"Engaging","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MPI jobs</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> <li>Compiling Source Code and GNU Make</li> </ul>"},{"location":"tags/#gpu","title":"GPU","text":"<ul> <li>GROMACS</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gromacs","title":"GROMACS","text":"<ul> <li>GROMACS</li> </ul>"},{"location":"tags/#getting-help","title":"Getting Help","text":"<ul> <li>Getting Help</li> </ul>"},{"location":"tags/#getting-started","title":"Getting Started","text":"<ul> <li>Getting Started Tutorial</li> </ul>"},{"location":"tags/#h100","title":"H100","text":"<ul> <li>Getting started on 8-way H100 nodes on Satori</li> </ul>"},{"location":"tags/#howto-recipes","title":"Howto Recipes","text":"<ul> <li>GROMACS</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>MPI jobs</li> <li>MPI for Python</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> <li>RELION</li> <li>VSCode</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>Getting Started Tutorial</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>GROMACS</li> <li>MPI jobs</li> <li>MPI for Python</li> <li>NVHPC with CUDA aware MPI</li> <li>RELION</li> </ul>"},{"location":"tags/#modules","title":"Modules","text":"<ul> <li>Modules</li> </ul>"},{"location":"tags/#mujoco","title":"MuJoCo","text":"<ul> <li>MuJoCo</li> </ul>"},{"location":"tags/#openmind","title":"OpenMind","text":"<ul> <li>MPI for Python</li> <li>Compiling Source Code and GNU Make</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>MPI for Python</li> </ul>"},{"location":"tags/#relion","title":"RELION","text":"<ul> <li>RELION</li> </ul>"},{"location":"tags/#rocky-linux","title":"Rocky Linux","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#satori","title":"Satori","text":"<ul> <li>ORCD Systems</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>NVHPC with CUDA aware MPI</li> <li>RELION</li> </ul>"},{"location":"tags/#software","title":"Software","text":"<ul> <li>Compiling Source Code and GNU Make</li> <li>Modules</li> <li>Overview</li> <li>Python</li> </ul>"},{"location":"tags/#supercloud","title":"SuperCloud","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MuJoCo</li> </ul>"},{"location":"tags/#cuda","title":"cuda","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#cuda-aware-mpi","title":"cuda aware mpi","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#nvhpc","title":"nvhpc","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#python_1","title":"python","text":"<ul> <li>Python</li> </ul>"},{"location":"tags/#vscode","title":"vscode","text":"<ul> <li>VSCode</li> </ul>"}]}