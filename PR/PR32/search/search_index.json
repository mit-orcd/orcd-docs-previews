{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MIT Office of Research Computing and Data Hands On Help Pages","text":"<p>The MIT Office or Research Computing and Data (ORCD) provides access and support for the compute and data needs of a wide range of research activities. These pages provide  help material for hands-on working with ORCD supported services. </p> <p>Help working with ORCD services is also available through email to orcd-help@mit.edu, please  feel free to contact us with questions and suggestions.</p>"},{"location":"acknowledgements/","title":"Acknowledging Us","text":"<p>If have used our systems or consultation services and you would like to acknowledge the MIT Office of Research Computing an Data in your paper, we recommend adding the following to your Acknowledgments section (be sure to select the applicable resource(s) from among those in the brackets below):</p> <p>Acknowledgement Statement</p> <p>The authors acknowledge the MIT Office of Research Computing and Data for providing [high performance computing, consultation, data] resources that have contributed to the research results reported within this paper.</p> <p>Thank you for acknowledging us \u2013 we appreciate it.</p>"},{"location":"code-of-conduct/","title":"Acceptable Use and Code of Conduct","text":"<p>The ORCD systems are operated by the MIT Office of Research Computing and Data and certain appropriate common sense rules apply to working on it.</p>"},{"location":"code-of-conduct/#acceptable-use-guidelines","title":"Acceptable Use Guidelines","text":"<p>ORCD systems are intended for research associated with MIT projects or collaborations around MIT research projects. That can cover a lot of things, but all account holders are expected to use judgement and apply common sense to their use of the system. The system is not to be used to support commercial activities or for non-MIT related activities. It is not to be used for anything that might be construed as illegal or criminal. Datasets on the system must have been obtained legitimately and the system is not to be used for working with unanonymized data or data subject to ITAR or other national security  restrictions. If you are unsure about a planned use, please feel free to contact orcd-help@mit.edu. </p> <p>All systems are covered by MIT Institute wide policies for acceptable use of information technology. For data practices refer to our page on Data Security and Privacy and the links on that page, particularly MIT's page on Information Protection.</p> <p>Account holders should not share accounts and should take reasonable precautions to ensure  that credentials for accessing the system (passwords, ssh keys, etc.) are kept secure.  All account holders agree to respect requests from support staff around how they use  the system. The support staff may, as needed, impose whatever policies are required to ensure the system runs well for all projects on the system. </p>"},{"location":"code-of-conduct/#code-of-conduct","title":"Code of Conduct","text":"<p>ORCD systems are shared resource used by a wide community. All people involved in its use and operations should try their utmost to be courteous and kind at all times. Members of the  ORCD community should be respectful toward one another and endeavor to ensure a  welcoming and collegial environment for all. Account holders are also expected to respect privacy of others activities on the system, and not to try to gain access to parts of the system they are not explicitly authorized to access.</p>"},{"location":"data-security/","title":"Data Security and Privacy","text":""},{"location":"data-security/#what-data-can-be-stored","title":"What data can be stored?","text":"<p>All ORCD current systems are only suitable for storing data with low-level security requirements. This means that they are not to be used to store sensitive data, such as personal information, financial information, or intellectual property. Additionally, they are not to be used to store data that is subject to use agreements that require security controls or audit tracking.</p> <p>The following data categories are appropriate for storing and analyzing on current ORCD systems:</p> <p>Open public data Low-sensitivity private research data Low-sensitivity research data are items such as drafts of papers and analyses derived from public data.</p> <p>The following data types are suitable for ORCD systems:</p> <ul> <li>Anything in the low-risk category described at the MIT IS&amp;T information security pages</li> <li>Drafts of unpublished research papers and results that are based on low-risk data</li> </ul> <p>Anything else in the medium-risk or higher risk levels of the MIT IS&amp;T information security pages should not be stored or analyzed on current ORCD systems.</p> <p>If you have any questions about whether your data is appropriate for storing on ORCD systems please feel free to reach out to us at orcd-help@mit.edu. </p>"},{"location":"data-security/#where-can-more-sensitive-data-be-processed-and-stored","title":"Where can more sensitive data be processed and stored?","text":"<p>The ORCD team is currently developing a system for more sensitive data. If you have sensitive data and would like to learn about our plans please feel free to get in touch at orcd-help@mit.edu.</p>"},{"location":"getting-help/","title":"Getting Help","text":"","tags":["Getting Help"]},{"location":"getting-help/#additional-documentation","title":"Additional Documentation","text":"<p>If you haven't found your answer elsewhere in these pages, your answer may be in the documentation for the system you are using:</p> <ul> <li>Engaging Documentation</li> <li>Satori Documentation</li> <li>SuperCloud Documentation</li> <li>OpenMind Documentation</li> </ul>","tags":["Getting Help"]},{"location":"getting-help/#email","title":"Email","text":"<p>If you can't find your answer in the documentation, please use one of the email lists below to contact us. With the exception of SuperCloud, these lists will create a ticket that our team can assign and track. In all cases these mailing lists includes the entire team, so the best available person to answer your question will respond. Sending email to the entire team will also likely get you the fastest response. Please do not send email directly to individual team members.</p> <ul> <li>General ORCD Questions: orcd-help@mit.edu</li> <li>Engaging: orcd-help-engaging@mit.edu</li> <li>Satori: orcd-help-satori@mit.edu</li> <li>OpenMind: orcd-help-openmind@mit.edu</li> <li>SuperCloud: supercloud@mit.edu</li> </ul> <p>In this email, please provide, where applicable:</p> <ul> <li>Description of your issue or request</li> <li>The command that you used to launch your job</li> <li>Job ID(s)</li> <li>What you tried</li> <li>The full error message you are receiving</li> <li>Any supporting files (code, submission scripts, screenshots, etc)</li> </ul>","tags":["Getting Help"]},{"location":"getting-help/#office-hours","title":"Office Hours","text":"<p>We host weekly office hours. Office Hours are a time when you can drop in and ask us questions. It is a great time to discuss or troubleshoot something that is difficult over email. See the table below for the available Office Hours sessions.</p> Session Time Location In Person Office Hours Thursdays 2-3 pm GIS and Data Lab in the Rotch Library (7-238) Virtual Office Hours First, Third, and Fifth Fridays 2-3pm Zoom, email orcd-help@mit.edu for the link","tags":["Getting Help"]},{"location":"getting-started/","title":"Getting Started Tutorial","text":"<p>This page contains the most common steps for setting up and getting started with your ORCD system account. We provide this page as a convenient reference to get started. Each system has its own in-depth documentation which can be found on the ORCD Systems page.</p> <p>Sections that are system-specific will be shown under a list of tabs. Click on the tab for the system you are using and the rest of the page will show the information for that system.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#getting-an-account","title":"Getting an Account","text":"<p>If you don't already have an account, click on the tab for the system you are interested in and follow the instructions.</p> EngagingSatoriSuperCloudOpenMind <p>Login into the respective OnDemand Portal https://engaging-ood.mit.edu using your MIT kerberos credentials. The system will then be prompted to create your account automatically. Wait couple minutes for the system to create all the pieces for your account before submitting your first job.</p> <p>Login into the respective OnDemand Portal https://satori-portal.mit.edu/ using your MIT kerberos credentials. The system will then be prompted to create your account automatically. Wait couple minutes for the system to create all the pieces for your account before submitting your first job.</p> <p>Follow the instructions on the Account Request Page.</p> <p>OpenMind will be available to the general MIT Community starting 2024. Those currently eligible can follow the instructions on the Getting an Account page.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#logging-in","title":"Logging In","text":"<p>The first thing you should do when you get a new account is verify that you can log in. The different ORCD systems provide multiple ways to log in, including both ssh and web portals. Links to instructions for the different systems are below.</p> EngagingSatoriSuperCloudOpenMind <p>See the Logging into Engaging page for full documentation.</p> <p>See the Logging into Satori page for full documentation.</p> <p>See the Logging into SuperCloud page for full documentation.</p> <p>See the Logging into OpenMind page for full documentation.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#terminal-with-ssh","title":"Terminal with SSH","text":"EngagingSatoriSuperCloudOpenMind <p>Engaging has four login nodes: <code>eofe7</code>, <code>eofe8</code>, <code>eofe9</code>, and <code>eofe10</code>. Replace <code>USERNAME</code> below with your Kerberos username and use the login node you would like to log in with, the example below is using <code>eofe10</code>.</p> <pre><code>ssh USERNAME@eofe10.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password.  You can add an ssh key if you do not want to enter your Kerberos password at login. <code>eofe9</code> and <code>eofe10</code> also require Two-Factor Authentication.</p> <p>Satori has two login nodes: <code>satori-login-001</code> and <code>satori-login-002</code>. Replace <code>USERNAME</code> below with your Kerberos username and use the login node you would like to log in with, the example below is using <code>satori-login-001</code>.</p> <pre><code>ssh USERNAME@satori-login-001.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password. You can add an ssh key if you do not want to enter your Kerberos password at login.</p> <p>In order to log into SuperCloud with ssh you will need to add ssh keys to your account on the Web Portal. Follow the instructions on the SuperCloud Getting Started page to add your keys.</p> <p>Then you can log in with ssh using the following command, where <code>USERNAME</code> is your username on the MIT SuperCloud system:</p> <pre><code>ssh USERNAME@txe1-login.mit.edu\n</code></pre> <p>Log into OpenMind with the following command in a terminal window. Replace <code>USERNAME</code> below with your Kerberos username.</p> <pre><code>ssh USERNAME@openmind.mit.edu\n</code></pre> <p>If you are prompted for a password enter your Kerberos password. You can add an ssh key if you do not want to enter your Kerberos password at login.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#web-portal","title":"Web Portal","text":"EngagingSatoriSuperCloudOpenMind <p>You can log into OnDemand Web Portal with the link: https://engaging-ood.mit.edu. For full detailed instructions please see the Engaging Documentation.</p> <p>You can log into the OnDemand Web Portal with the link: https://satori-portal.mit.edu. For full detailed instructions please see the Satori Documentation.</p> <p>You can log into the SuperCloud Web Portal with the link: https://txe1-portal.mit.edu. For full detailed instructions please see the SuperCloud Documentation.</p> <p>OpenMind does not currently have a web portal, but there are plans to add one in the future. Check back, and in the meantime check out OpenMind's documentation on the FastX Remote Desktop. You may find it provides what you are looking for.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#shared-hpc-clusters","title":"Shared HPC Clusters","text":"<p>Each ORCD system is a shared HPC cluster. You are sharing this resources with a number of other researchers, staff, and students so it is important that you read this page and use the system as intended.</p> <p>Being a cluster, there are several machines connected together with a network. We refer to these as nodes. Most nodes in the cluster are referred to as compute nodes, this is where the computation is done on the system (where you will run your code). When you ssh into the system you are on a special purpose node called the login node. The login node, as its name suggests, is where you log in and is for editing code and files, installing packages and software, downloading data, and starting jobs to run your code on one of the compute nodes.</p> <p>Each job is started using a piece of software called the scheduler, which you can think of as a resource manager. You let it know what resources you need and what you want to run, and the scheduler will find those resources and start your job on them. When your job completes those resources are relinquished. The scheduler is what ensures that no two jobs are using the same resources, so it is very important not to run anything unless it is submitted properly through the scheduler.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#software-and-packages","title":"Software and Packages","text":"<p>The first thing you may want to do is make sure the system has the software and packages you need. We have installed a lot of software and packages on the system already, even though it may not be immediately obvious that it is there. Review the page for the system you are using paying particular attention to the section on modules and installing packages for the language that you use:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging Software Documentation Page</p> <p>Satori Software Documentation Page</p> <p>SuperCloud Software Documentation Page</p> <p>OpenMind Software Documentation Page</p> <p>If you are ever unsure if we have a particular software, and you cannot find it, please send us an email and ask before you spend a lot of time trying to install it. If we have it, we can point you to it, provide advice on how to use it, and if we don't have it we can often give pointers on how to install it. Further, if a lot of people request the same software, we may consider adding it to the system image.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#linux-command-line","title":"Linux Command Line","text":"<p>Every ORCD system runs Linux, so much of what you do on the cluster involves the Linux command line. That doesn't mean you have to be a Linux expert to use the system! However the more you can get comfortable with the Linux command line and a handful of basic commands, the easier using the system will be. If you are already familiar with Linux, feel free to skip this section, or skim as a refresher.</p> <p>Most Linux commands deal with directories and files. A directory, synonymous to a folder, contains files and other directories. The list of directories that lead to a particular directory or file is called its path. In Linux, directories on a path are separated by forward slashes <code>/</code>. It is also important to note that everything in Linux is case sensitive, so a file <code>myScript.sh</code> is not the same as the file <code>myscript.sh</code>. When you first log in you are in you home directory. Your home directory is where you can put all the code and data you need to run your job. Your home directory is not accessible to other users, if you need a space to share files with other users, let us know and we can make a shared group directory for you.</p> EngagingSatoriSuperCloudOpenMind <p>The path to your home directory on Satori is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on Satori is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on SuperCloud is <code>/home/gridsan/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>The path to your home directory on OpenMind is <code>/home/&lt;USERNAME&gt;</code>, where <code>&lt;USERNAME&gt;</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>Anytime after you start typing a Linux command you can press the \"Tab\" button your your keyboard. This called tab-complete, and will try to autocomplete what you are typing. This is particularly helpful when typing out long directory paths and file names. Pressing \"Tab\" once will complete if there is a single completion, pressing it twice will list all potential completions. It is a bit difficult to explain in text, but you can try it out yourself and watch the short demonstration here.</p> <p>Finally, click on the box below for a list of Linux Commands. If you are new to Linux try them out for yourself at the command line.</p> Common Linux Commands <ul> <li>Creating, navigating and viewing directories:<ul> <li><code>pwd</code>: tells you the full path of the directory you are     currently in</li> <li><code>mkdir dirname</code>: creates a directory with the name \"dirname\"</li> <li><code>cd dirname</code>: change directory to directory \"dirname\"<ul> <li><code>cd ../</code>: takes you up one level</li> </ul> </li> <li><code>ls</code>: lists the files in the directory<ul> <li><code>ls -a</code>: lists all files including hidden files</li> <li><code>ls -l</code>: lists files in \"long format\" including ownership     and date of last update</li> <li><code>ls -t</code>: lists files by date stamp, most recently updated     file first</li> <li><code>ls -tr</code>: lists files by dates stamp in reverse order, most     recently updated file is listed last (this is useful if you     have a lot of files, you want to know which file you changed     last and the list of files results in a scrolling window)</li> <li><code>ls dirname</code>: lists the files in the directory \"dirname\"</li> </ul> </li> </ul> </li> <li>Viewing files<ul> <li><code>more filename</code>: shows the first part of a file, hitting the     space bar allows you to scroll through the rest of the file, q     will cause you to exit out of the file.</li> <li><code>less filename</code>: allows you to scroll through the file, forward     and backward, using the arrow keys.</li> <li><code>tail filename</code>: shows the last 10 lines of a file (useful when     you are monitoring a logfile or output file to see that the     values are correct)<ul> <li>t<code>ail &lt;number&gt; filename</code>: show you the last &lt;number&gt;     lines of a file.</li> <li><code>tail -f filename</code>: shows you new lines as they are written     to the end of the file. Press CMD+C or Control+C to exit.     This is helpful to monitor the log file of a batch job.</li> </ul> </li> </ul> </li> <li>Copying, moving, renaming, and deleting files<ul> <li><code>mv filename dirname</code>: moves filename to directory dirname.<ul> <li><code>mv filename1 filename2</code>: moves filename1 to filename2, in     essence renames the file. The date and time are not changed     by the mv command.</li> </ul> </li> <li><code>cp filename dirname</code>: copies to directory dirname.<ul> <li><code>cp filename1 filename2</code>: copies filename1 to filename2. The     date stamp on filename2 will be the date/time that the file     was moved</li> <li><code>cp -r dirname1 dirname2</code>: copies directory dirname1 and its     contents to dirname2.</li> </ul> </li> <li><code>rm filename</code>: removes (deletes) the file</li> </ul> </li> </ul>","tags":["Getting Started","Linux"]},{"location":"getting-started/#transferring-files","title":"Transferring Files","text":"<p>One of the first tasks is to get your code, data, and any other files you need into your home directory on the system. If your code is in github you can use git commands on the system to clone your repository to your home directory. You can also transfer your files to your home directory from your computer by using the commands <code>scp</code> or <code>rsync</code>. Read the page on Transferring Files for the system you are using to learn how to use these commands and transfer what you need to your home directory.</p> <p>You can use <code>scp</code> or <code>rsync</code> from the command line on your local computer for any ORCD system. Both commands work similarly to the <code>cp</code> command, following the pattern <code>&lt;command&gt; &lt;source&gt; &lt;destination&gt;</code>, the only difference being that you will need to include the hostname of the system you are transferring to or from. For this reason you must run this command from the terminal on your computer before you've logged in.</p> <p>To transfer a file from your computer to the ORCD system:</p> EngagingSatoriSuperCloudOpenMind <pre><code>scp &lt;file-name&gt; USERNAME@eofe8.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp &lt;local-file-name&gt; USERNAME@satori-login-001:&lt;path-to-satori-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp &lt;local-file-name&gt; USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;\n</code></pre> <pre><code>scp &lt;local-file-name&gt; &lt;user&gt;@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <p>To transfer a file from an ORCD system to your computer:</p> EngagingSatoriSuperCloudOpenMind <pre><code>scp USERNAME@eofe8.mit.edu:&lt;path-to-engaging-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp USERNAME@satori-login-001:&lt;path-to-satori-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <pre><code>scp &lt;user&gt;@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;/&lt;file-name&gt; &lt;path-to-local-dest&gt;\n</code></pre> <p>Similar to <code>cp</code>, use the <code>-r</code> flag to copy over an entire directory and its contents. </p> EngagingSatoriSuperCloudOpenMind <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@eofe8.mit.edu:&lt;path-to-engaging-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@satori-login-001:&lt;path-to-satori-dir&gt;\n</code></pre> <p>(You can use any of the login nodes listed above)</p> <pre><code>scp -r &lt;local-dir-name&gt; USERNAME@txe1-login:&lt;path-to-supercloud-dir&gt;\n</code></pre> <pre><code>scp -r &lt;local-dir-name&gt; &lt;user&gt;@openmind-dtn.mit.edu:&lt;path-to-openmind-dir&gt;\n</code></pre> <p>The <code>rsync</code> command can be used similarly and has some additional flags you can use. It also can be used to transfer only new or modified files to the destination, which makes it easy to keep a directory in \"sync\".</p> <p>For more information on transferring files and additional methods please see the documentation page for your system:</p> EngagingSatoriSuperCloudOpenMind <p>Engaging Transferring Files Documentation Page Coming Soon</p> <p>Satori Transferring Files Documentation Page</p> <p>SuperCloud Transferring Files Documentation Page</p> <p>OpenMind Transferring Files Documentation Page</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#running-your-first-job","title":"Running your First Job","text":"<p>At this point you may want to do a test-run of your code. You always want to start small in your test runs, so you should choose a small example that tests the functionality of what you would ultimately like to run on the system. If your test code is serial and runs okay on a moderate personal laptop or desktop you can request an interactive session to run your code in by executing the command:</p> EngagingSatoriSuperCloudOpenMind <pre><code># Requesting a single core for an interactive job for 1 hour\nsrun -n 1  -t 01:00:00 --pty /bin/bash\n</code></pre> <pre><code>#Requesting a single core for an interactive job for 1 hour\nsrun -n 1  -t 01:00:00 --pty /bin/bash\n</code></pre> <pre><code># Requesting a single core for an interactive job\nLLsub -i\n</code></pre> <pre><code># Requesting a single core for an interactive job for 1 hour\nsrun -n 1 -t 01:00:00  --pty bash  \n</code></pre> <p>After you run this command you will be on a compute node and you can do a test-run of your code. This command will allocate one core to your job. If your test code is multithreaded or parallel, uses a lot of memory, or requires a GPU you should request additional resources as needed. Not requesting the resources you will be using can negatively impact others on the system.</p> <p>Please see your system's documentation pages for more information on requesting more resources for running interactive jobs, and how to run batch jobs.</p> EngagingSatoriSuperCloudOpenMind <p>Engaging's Documentation for Running Jobs</p> <p>Satori's Documentation for Running Jobs</p> <p>SuperCloud's Documentation for Running Jobs</p> <p>OpenMind's Documentation for Running Jobs</p>","tags":["Getting Started","Linux"]},{"location":"orcd-systems/","title":"ORCD Systems","text":"<p>ORCD operates and provides support and training for a number of cluster computer systems available to all researchers. These systems all run with a Slurm scheduler and have a web portal for interactive computing. These are Engaging, SuperCloud, and Satori.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-cluster","title":"Engaging Cluster","text":"<p>The engaging cluster is a mixed CPU and GPU computng cluster that is openly available to all  research projects at MIT. It has around 80,000 x86 CPU cores and 300  GPU cards ranging from K80 generation to recent Voltas. Hardware access is through the Slurm  resource scheduler that supports batch and interactive workloads and allows dedicated reservations. The cluster has a large shared file system for working datasets. Additional compute and storage  resources can be purchased by PIs. A wide range of standard software is available and the Docker  compatible Singularity container tool is supported. User-level tools like Anaconda for Python,  R libraries and Julia packages are all supported. A range of PI group maintained custom software  stacks are also available through the widely adopted environment modules toolkit. A standard,  open-source, web-based portal supporting Jupyter notebooks, R studio, Mathematica and X graphics  is available at https://engaging-ood.mit.edu. Further information and support is available from orcd-help-engaging@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-engaging","title":"How to Get an Account on Engaging","text":"<p>Accounts on the engaging cluster are connected to your main MIT institutional kerberos id.  Connecting to the cluster for the first time through its web portal automatically activates an account with basic access to resources. See this page for instructions on how to log in.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#engaging-quick-links","title":"Engaging Quick Links","text":"<ul> <li>Documentation: https://engaging-web.mit.edu/eofe-wiki/</li> <li>OnDemand web portal: https://engaging-ood.mit.edu</li> <li>Help: Send email to orcd-help-engaging@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori","title":"Satori","text":"<p>Satori is an IBM Power 9 large memory node system. It is open to everyone on campus and has  optimized software stacks for machine learning and for image stack post-processing for  MIT.nano Cryo-EM facilities. The system has 256 NVidia Volta GPU cards attached in groups of  four to 1TB memory nodes and a total of 2560 Power 9 CPU cores. Hardware access is through the  Slurm resource scheduler that supports batch and interactive workload and allows dedicated  reservations. A wide range of standard software is available and the Docker compatible  Singularity container tool is supported. A standard web based portal  https://satori-portal.mit.edu with Jupyter notebook support is available. Additional compute  and storage resources can be purchased by PIs and integrated into the system. Further  information and support is available at orcd-help-satori@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-satori","title":"How to Get an Account on Satori","text":"<p>You can get an account by logging into https://satori-portal.mit.edu with your MIT credentials.  See this page for instructions: https://mit-satori.github.io/satori-basics.html#how-can-i-get-an-account.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#satori-quick-links","title":"Satori Quick Links","text":"<ul> <li>Documentation: (https://mit-satori.github.io/</li> <li>OnDemand web portal: https://satori-portal.mit.edu</li> <li>Help: Send email to orcd-help-satori@mit.edu</li> <li>Slack: https://mit-satori.slack.com/</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud","title":"SuperCloud","text":"<p>The SuperCloud system is a collaboration with MIT Lincoln Laboratory on a shared facility that  is optimized for streamlining open research collaborations with Lincoln Laboratory. The facility  is open to everyone on campus. The latest SuperCloud system has more than 16,000 x86 CPU cores  and more than 850 NVidia Volta GPUs in total. Hardware access is through the Slurm resource  scheduler that supports batch and interactive workload and allows dedicated reservations. A wide  range of standard software is available and the Docker compatible Singularity container tool is  supported. A custom, web-based portal supporting Jupyter notebooks is available at https://txe1-portal.mit.edu/. Further information and support is available at  supercloud@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-supercloud","title":"How to Get an Account on SuperCloud","text":"<p>To request a SuperCloud account follow the instructions on this page:  https://supercloud.mit.edu/requesting-account.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#supercloud-quick-links","title":"SuperCloud Quick Links","text":"<ul> <li>Documentation: https://supercloud.mit.edu/</li> <li>Online Course: https://learn.llx.edly.io/course/practical-hpc/</li> <li>Web portal: https://txe1-portal.mit.edu/</li> <li>Help: Send email to supercloud@mit.edu</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#openmind","title":"OpenMind","text":"<p>The OpenMind system is a collaboration with Department of Brain and Cognitive Sciences (BCS) and McGovern Institute. OpenMind is mainly a GPU computing cluster optimized for artificial intelligence (AI) research and data science. Totally there are around 70 compute nodes, 3500 CPU cores, 48 TB of RAM, and 340 GPUs, including 142 A100-80GB GPUs. It also provides around 2 PB of flash storage supporting fast read/write data speed. Hardware access is through the Slurm resource scheduler that supports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and Docker compatible Apptainer/Singularity container tool is supported. User-level tools like Anaconda for Python, R libraries and Julia packages are all supported. Further information and support is available at orcd-help-openmind@mit.edu.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#how-to-get-an-account-on-openmind","title":"How to Get an Account on OpenMind","text":"<p>Accounts will be available for MIT users in 2024.</p>","tags":["Engaging","Satori","SuperCloud"]},{"location":"orcd-systems/#openmind-quick-links","title":"OpenMind Quick Links","text":"<ul> <li>Documentation: https://github.mit.edu/MGHPCC/OpenMind/wiki</li> <li>Home Page and Online Course: https://openmind.mit.edu/</li> <li>Help: Send email to orcd-help-openmind@mit.edu</li> <li>Slack: https://openmind-46.slack.com</li> </ul>","tags":["Engaging","Satori","SuperCloud"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#engaging","title":"Engaging","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gpu","title":"GPU","text":"<ul> <li>GROMACS</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gromacs","title":"GROMACS","text":"<ul> <li>GROMACS</li> </ul>"},{"location":"tags/#getting-help","title":"Getting Help","text":"<ul> <li>Getting Help</li> </ul>"},{"location":"tags/#getting-started","title":"Getting Started","text":"<ul> <li>Getting Started Tutorial</li> </ul>"},{"location":"tags/#h100","title":"H100","text":"<ul> <li>Getting started on 8-way H100 nodes on Satori</li> </ul>"},{"location":"tags/#howto-recipes","title":"Howto Recipes","text":"<ul> <li>GROMACS</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>MPI for Python</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> <li>Installing and Using RELION</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>Getting Started Tutorial</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>GROMACS</li> <li>MPI for Python</li> <li>NVHPC with CUDA aware MPI</li> <li>Installing and Using RELION</li> </ul>"},{"location":"tags/#mujoco","title":"MuJoCo","text":"<ul> <li>MuJoCo</li> </ul>"},{"location":"tags/#openmind","title":"OpenMind","text":"<ul> <li>MPI for Python</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>MPI for Python</li> </ul>"},{"location":"tags/#relion","title":"RELION","text":"<ul> <li>Installing and Using RELION</li> </ul>"},{"location":"tags/#rocky-linux","title":"Rocky Linux","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#satori","title":"Satori","text":"<ul> <li>ORCD Systems</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>NVHPC with CUDA aware MPI</li> <li>Installing and Using RELION</li> </ul>"},{"location":"tags/#supercloud","title":"SuperCloud","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MuJoCo</li> </ul>"},{"location":"tags/#cuda","title":"cuda","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#cuda-aware-mpi","title":"cuda aware mpi","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#nvhpc","title":"nvhpc","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"filesystems-file-transfer/filesystems/","title":"General Use Filesystems","text":"<p>Large HPC systems often have different filesystems for different purposes. ORCD systems are no different, and each have their own approach. This page documents these.</p>"},{"location":"filesystems-file-transfer/filesystems/#supercloud","title":"SuperCloud","text":"<p>SuperCloud uses Lustre for all central/shared storage (accessible to all nodes in the system). This storage is not backed up. See the SuperCloud Best Practices and Performance Tips page for best practices using the Lustre filesystem. Quotas or limits are set on the storage as guardrails. Additional storage may be granted on a case by case basis. Local disk spaces will be faster than the Lustre shared filesystem, but all are temporary and can only be accessed on the node where they are created.</p> Storage Type Path Access Backed up Limits Home Directory  Lustre <code>/home/gridsan/&lt;username&gt;</code> User only Not backed up See User Profile Page Group Directories  Lustre <code>/home/gridsan/groups/&lt;groupname&gt;</code> Files shared within a group Not backed up See User Profile Page Job-specific Temporary Storage  Local Disk Access using the <code>$TMPDIR</code> environment variable User or Group Not backed up   Temporary directory created at the start of a job and cleaned up at the end of the job NA Local Disk Space Create the directory <code>/state/partition1/user/$USER</code> as needed User or Group Not backed up  Cleaned up monthly during downtimes NA"},{"location":"filesystems-file-transfer/filesystems/#engaging","title":"Engaging","text":"<p>Users each get a small home directory that is backed up and meant for important files. Larger scratch space is not backed up. Additional storage can be purchased. The Lustre scratch space will be faster than NFS for the majority of workloads, however having large numbers of small files will make it slower than NFS and can slow down the filesystem overall, so it is important to follow the Lustre Best Practices. See the Engaging Documentation Page on Storage for more information.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory  NFS <code>/home/&lt;username&gt;</code> 100 GB Backed up Use for important files Lustre <code>/nobackup1/&lt;username&gt;</code> 1 TB Not backed up Scratch space  Faster than NFS NFS <code>/pool001/&lt;username&gt;</code> 1 TB Not backed up Scratch space"},{"location":"filesystems-file-transfer/filesystems/#satori","title":"Satori","text":"Storage Type Path Quota Backed up Purpose/Notes Home Directory <code>/home/&lt;username&gt;</code> 25GB Backed up Use for important files. Quota increase request to 100GB. GPFS <code>/nobackup/users/&lt;username&gt;</code> 500GB Not backed up Scratch space. Quota increase request to 2TB."},{"location":"filesystems-file-transfer/filesystems/#openmind","title":"OpenMind","text":"<p>OpenMind provides a number of different storage options. See the OpenMind Documentation page on Storage for more information, best practices, and recommendations.</p> Storage Type Path Quota Backed up Purpose/Notes Home Directory <code>/home/&lt;username&gt;</code> 20 GB Backed up Use for very important files. Physically located on Vast. Weka <code>/om/user/&lt;username&gt;</code> (individual users) and <code>/om/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Weka Scratch <code>/om/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 3 weeks after creation Scratch space Vast <code>/om2/user/&lt;username&gt;</code> (individual users) and <code>/om2/group/&lt;groupname&gt;</code> (groups) Per group Backed up Fast internal storage Vast Scratch <code>/om2/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 2 weeks after creation Scratch space Lustre Scratch <code>/nobackup/scratch/&lt;week-day&gt;</code> N/A Not backed up  purged 3 weeks after creation Scratch space   See Lustre Best Practices page NFS <code>/om3</code>, <code>/om4</code>, <code>/om5</code> Per group Backed up Slow internal long-term storage NESE <code>/nese</code> Per group Backed up Slow internal long-term storage"},{"location":"filesystems-file-transfer/project-filesystems/","title":"Project Specific Filesystems","text":""},{"location":"filesystems-file-transfer/project-filesystems/#purchasing-storage","title":"Purchasing Storage","text":"<p>Additional project and lab storage can be purchased on ORCD shared clusters by individual PI groups. This storage is mounted on the cluster and access to the storage is managed  by the group through MIT Web Moira, https://groups.mit.edu/webmoira/ (see below for details).</p> <p>Current pricing for storage is</p> Storage Type Pricing Duration Backup NESE encrypted at rest   disk. $2.50/month  50TB minimum,    10TB increments. 12 month  minimum. No automated backup. <p>The NESE encrypted at rest disk uses a large centrally managed storage cloud at the MGHPCC facility. Any shared ORCD cluster at the MGHPCC can access this storage. Data on NESE disk is transparently encrypted at rest.</p> <p>To purchase storage please send an email to orcd-help@mit.edu.</p>"},{"location":"filesystems-file-transfer/project-filesystems/#managing-access-using-mit-web-moira","title":"Managing access using MIT Web Moira","text":"<p>Individual group storage is configured so that access is limited to a set of accounts belonging to a web moira list that is defined for the group store. The owner and administrators of group storage can manage access themselves, by modifying the membership of an associated moira list under https://groups.mit.edu/webmoira/list/. The name of the list corresponds to the UNIX group name associated with the ORCD shared  cluster storage.</p>"},{"location":"filesystems-file-transfer/project-filesystems/#moira-web-interface-example","title":"Moira Web Interface Example","text":"<p>The figure below shows a screenshot of the web moira management page at https://groups.mit.edu/webmoira/list/cnh_research_computing for a hypothetical storage group named <code>cnh_research_computing</code>. The interface provides a  self-service mechanism for controlling access to any storage belonging to this group. MIT account ids can be added and  removed as needed from this list by the storage access administrators.</p> <p></p>"},{"location":"images/","title":"Index","text":""},{"location":"images/#directory-of-static-images","title":"Directory of static images","text":""},{"location":"recipes/gromacs/","title":"Installing and Using GROMACS","text":"<p>GROMACS is a free and open-source software suite for high-performance molecular dynamics and output analysis.</p> <p>You can learn about GROMACS here: https://www.gromacs.org/.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#gromacs-on-engaging","title":"GROMACS on Engaging","text":"","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#install-gromacs-with-mpi","title":"Install GROMACS with MPI","text":"<p>Select a version on the GROMACS website, then dowload and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget --no-check-certificate http://ftp.gromacs.org/pub/gromacs/gromacs-2019.6.tar.gz\ntar xvfz gromacs-2019.6.tar.gz\n</code></pre></p> <p>Load MPI and Cmake modules, <pre><code>module load engaging/openmpi/2.0.3 cmake/3.17.3\n</code></pre></p> <p>Create build and isntall directories, <pre><code>mkdir -p 2019.6/build\nmkdir 2019.6/install\ncd 2019.6/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2019.6 -DGMX_MPI=ON -DCMAKE_INSTALL_PREFIX=~/gromacs/2019.6/install\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage enviroenment, <pre><code>source ~/gromacs/2019.6/install/bin/GMXRC\n</code></pre></p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#gromacs-on-supercloud","title":"GROMACS on SuperCloud","text":"","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#install-gromacs-with-mpi-and-cuda","title":"Install GROMACS with MPI and CUDA","text":"<p>Select a version on the GROMACS website, then dowload and extract the tar ball. <pre><code>cd ~\nmkdir gromacs\ncd gromacs\nwget http://ftp.gromacs.org/pub/gromacs/gromacs-2023.2.tar.gz\ntar xvfz gromacs-2023.2.tar.gz\n</code></pre></p> <p>Load CUDA, Anaconda and MPI modules, <pre><code>module load cuda/11.8 anaconda/2023a\nmodule load mpi/openmpi-4.1.5\n</code></pre></p> <p>Create build and isntall directories, <pre><code>mkdir -p 2023.2/build\nmkdir 2023.2/install\ncd 2023.2/build\n</code></pre></p> <p>Use <code>cmake</code> to configure compiling options, <pre><code>cmake ~/gromacs/gromacs-2023.2 -DGMX_MPI=ON -DGMX_GPU=CUDA -DCMAKE_INSTALL_PREFIX=~/gromacs/2023.2-gpu\n</code></pre></p> <p>Compile, check and install, <pre><code>make\nmake check\nmake install\n</code></pre></p> <p>Set up usage enviroenment before running GROMACS programs, <pre><code>source ~/gromacs/2023.2/install/bin/GMXRC\n</code></pre></p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/gromacs/#run-gromacs","title":"Run GROMACS","text":"<p>Firstly, prepare for an input file. Refer to file formats. Here shows an example with an input file named <code>benchPEP-h.tpr</code> dowloaded from this page.</p> <p>Secondly, edit a batch job script, for example, named <code>job.sh</code>, requesting 2 node with 4 CPU cores and 2 GPUs per node. <pre><code>#!/bin/bash\n#SBATCH --nodes=2              # 2 nodes\n#SBATCH --ntasks-per-node=2    # 2 MPI tasks per node\n#SBATCH --cpus-per-task=2      # 2 CPU cores per task\n#SBATCH --gres=gpu:volta:2     # 2 GPUs per node\n#SBATCH --time=01:00:00        # 1 hour\n\n\n# Load required modules\nmodule load cuda/11.8 mpi/openmpi-4.1.5\n\n# Allow GROMACS to see GPUs\nexport CUDA_VISIBLE_DEVICES=0,1\n\n# Enable direct GPU to GPU communications\nexport GMX_ENABLE_DIRECT_GPU_COMM=true\n\n# Activate user install of GROMACS\nsource ~/gromacs/2023.2-gpu/bin/GMXRC\n\n# Check MPI, GPU and GROMACS\nmpirun hostname\nnvidia-smi\nwhich gmx_mpi\n\n# Run GROMACS\nmpirun gmx_mpi mdrun -s ~/gromacs/bench/benchPEP-h.tpr -ntomp ${SLURM_CPUS_PER_TASK} -pme gpu -update gpu -bonded gpu -npme 1\n</code></pre></p> <p>Finally, submit the job, <pre><code>sbatch job.sh\n</code></pre></p> <p>Refer to GROMACS user guide for more info.</p>","tags":["Engaging","SuperCloud","MPI","GPU","GROMACS","Howto Recipes"]},{"location":"recipes/h100_getting_started/","title":"Getting started on 8-way H100 nodes on Satori","text":"<p>This page provides information on how to run a first job on the IBM Watson AI Lab H100 GPU nodes on Satori. The page describes how to request an access to the Slurm partition associated  with the H100 nodes and how to run a first example pytorch script on the systems. </p> <p>A first set of H100 GPU systems has been added to Satori. These are for priority use by IBM Watson AI Lab research collaborators. They are also available for general opportunistic use when they are idle.</p> <p>Currently ( 2023-06-19 ) there are 4 H100 systems installed.  Each system has 8 H100 GPU cards, two Intel 8468 CPUs each with 48 physical cores and 1TiB of main memory.</p> <p>Below are some instructions for getting started with these systems. </p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#access-to-the-nodes","title":"Access to the nodes","text":"<p>To access the nodes in the priority group you need your satori login id to be listed in the Webmoira  group https://groups.mit.edu/webmoira/list/sched_oliva.  Either Alex Andonian and Vincent Sitzmann are able to add accounts to the <code>sched_oliva</code> moira list.</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#interactive-access-through-slurm","title":"Interactive access through Slurm","text":"<p>To access an entire node through Slurm, the command below can be used from the satori login node</p> <pre><code>srun -p sched_oliva --gres=gpu:8 -N 1 --mem=0 -c 192 --time 1:00:00 --pty /bin/bash\n</code></pre> <p>this command will launch an interactive shell on one of the nodes (when a full node becomes available).  From this shell the NVidia status command  <pre><code>nvidia-smi\n</code></pre> should list 8 H100 GPUs as available. </p> <p>Single node, multi-gpu training examples (for example https://github.com/artidoro/qlora ) should run  on all 8 GPUs. </p> <p>To use a single GPU interactively the following command can be used <pre><code>srun -p sched_oliva --gres=gpu:1 --mem=128 -c 24 --time 1:00:00 --pty /bin/bash\n</code></pre></p> <p>this will request a single GPU. This request will allow other Slurm sessions to run on other GPUs  simultaneously with this session.</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-nightly-build-pytorch-example-with-a-freash-miniconda-and-pytorch","title":"Running a nightly build pytorch example with a freash miniconda and pytorch","text":"<p>A miniconda environment can be used to run the latest nightly build pytorch code on these  systems. To do this, first create a software install directory and install the needed pytorch software</p> <pre><code>mkdir -p /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre> <p>and then switch your shell to that directory. <pre><code>cd /nobackup/users/${USER}/pytorch_h100_testing/conda_setup\n</code></pre></p> <p>now install miniconda and create an environment with the needed software <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \nchmod +x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh -b -p minic\n. ./minic/bin/activate \nconda create -y -n pytorch_test python=3.10\nconda activate pytorch_test                          \nconda install -y -c conda-forge cupy\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n</code></pre></p> <p>Once the software is installed, the following script can be used to test the installation. <pre><code>cat &gt; test.py &lt;&lt;'EOF'\nimport torch\ndevice_id = torch.cuda.current_device()\ngpu_properties = torch.cuda.get_device_properties(device_id)\nprint(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n          \"%.1fGb total memory.\\n\" % \n          (torch.cuda.device_count(),\n          device_id,\n          gpu_properties.name,\n          gpu_properties.major,\n          gpu_properties.minor,\n          gpu_properties.total_memory / 1e9))\nEOF\n\npython test.py\n</code></pre></p> <p>To exit the Slurm srun session enter the command <pre><code>exit\n</code></pre></p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#running-a-simple-batch-script-using-an-installed-miniconda-environment","title":"Running a simple batch script using an installed miniconda environment","text":"<p>To run a batch script on one of the H100 nodes in partition sched_oliva first paste the content in the  box below into a slurm script file called, for example, <code>test_script.slurm</code> ( change the RUNDIR setting to assign the  path to a directory where you have already installed a conda environment in a sub-directory called <code>minic</code> ).</p> <pre><code>#!/bin/bash\n#\n#SBATCH --gres=gpu:8\n#SBATCH --partition=sched_oliva\n#SBATCH --time=1:00:00\n#SBATCH --mem=0\n#\n\nnvidia-smi\n\nRUNDIR=/nobackup/users/${USER}/h100-testing/minic\ncd ${RUNDIR}\n\n. ./minic/bin/activate\n\nconda activate pytorch_test\n\ncat &gt; mytest.py &lt;&lt;'EOF'\nimport torch\ndevice_id = torch.cuda.current_device()\ngpu_properties = torch.cuda.get_device_properties(device_id)\nprint(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n          \"%.1fGb total memory.\\n\" %\n          (torch.cuda.device_count(),\n          device_id,\n          gpu_properties.name,\n          gpu_properties.major,\n          gpu_properties.minor,\n          gpu_properties.total_memory / 1e9))\nEOF\n\npython mytest.py\n</code></pre> <p>This script can then be submitted to Slurm to run in a background batch node using the command.</p> <pre><code>sbatch &lt; test_script.slurm\n</code></pre>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/h100_getting_started/#getting-help","title":"Getting help","text":"<p>As always, please feel welcome to email orcd-help@mit.edu with questions, comments or suggestions. We would be happy to hear from you!</p>","tags":["Satori","Howto Recipes","H100"]},{"location":"recipes/mpi4py/","title":"Installing and Using MPI for Python","text":"<p>MPI for Python (<code>mpi4py</code>) provides Python bindings for the Message Passing Interface (MPI) standard, allowing Python applications to exploit multiple processors on workstations, clusters and supercomputers.</p> <p>You can learn about <code>mpi4py</code> here: https://mpi4py.readthedocs.io/en/stable/.</p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#mpi4py-on-openmind","title":"Mpi4py on OpenMind","text":"","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#install","title":"Install","text":"<p>If you use an Anaconda module, no installation is required.</p> <p>If you want to use Anaconda in your directory, refer to section 3 on this page to set it up, then intall <code>mpi4py</code>,  <pre><code>conda install -c conda-forge mpi4py\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mpi4py/#run-mpi4py","title":"Run Mpi4py","text":"<p>Prepare your Python codes. Example 1: The following is a code for sending and receiving a dictionary. Save it in a file named <code>p2p-send-recv.py</code>. <pre><code>from mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\nif rank == 0:\n    data = {'a': 7, 'b': 3.14}\n    comm.send(data, dest=1, tag=11)\n    print(rank,data)\nelif rank == 1:\n    data = comm.recv(source=0, tag=11)\n    print(rank,data)\n</code></pre></p> <p>Example 2: The following is a code for sending and receiving an array. Save it in a file named <code>p2p-array.py</code>. <pre><code>from mpi4py import MPI\nimport numpy\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\n# passing MPI datatypes explicitly\nif rank == 0:\n    data = numpy.arange(1000, dtype='i')\n    comm.Send([data, MPI.INT], dest=1, tag=77)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(1000, dtype='i')\n    comm.Recv([data, MPI.INT], source=0, tag=77)\n    print(rank,data)\n\n# automatic MPI datatype discovery\nif rank == 0:\n    data = numpy.arange(100, dtype=numpy.float64)\n    comm.Send(data, dest=1, tag=13)\n    print(rank,data)\nelif rank == 1:\n    data = numpy.empty(100, dtype=numpy.float64)\n    comm.Recv(data, source=0, tag=13)\n    print(rank,data)\n</code></pre></p> <p>Prepare a job script. The following is a job script for running <code>mpi4py</code> codes on 8 CPU cores of one node. Save it in a file named <code>p2p-job.sh</code>. <pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 8\n\nmodule load openmpi/gcc/64/1.8.1\nmodule load openmind/anaconda/3-2022.05\n\nmpirun -np $SLURM_NTASKS python p2p-send-recv.py\nmpirun -np $SLURM_NTASKS python p2p-array.py\n</code></pre></p> <p>An Openmpi module is needed. If you use Anaconda in your directory, do not load the Anaconda module. </p> <p>Finally submit the job, <pre><code>sbatch p2p-job.sh\n</code></pre></p>","tags":["OpenMind","MPI","Python","Howto Recipes"]},{"location":"recipes/mujoco/","title":"Installing and Using MuJoCo","text":"<p>MuJoCo is a free and open source physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed.</p> <p>You can learn about MuJoCo here: https://mujoco.org.</p> <p>Whether you are installing on Engaging or SuperCloud, you\u2019ll first have to install the MuJoCo binaries. This process is the same on all systems.</p>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#install-the-mujoco-binaries","title":"Install the MuJoCo Binaries","text":"<p>First, install MuJoCo itself somewhere in your home directory. This is as simple as downloading the MuJoCo binaries, which can be found on their web page. For the release that you want, select the file that ends with \u201clinux-x86_64.tar.gz\u201d, for example for 2.3.0 select mujoco-2.3.0-linux-x86_64.tar.gz. Right click, and copy the link address. Then you can download on one of the login nodes with the \u201cwget\u201d command, and untar:</p> <pre><code>wget https://github.com/deepmind/mujoco/releases/download/2.3.0/mujoco-2.3.0-linux-x86_64.tar.gz\ntar -xzf mujoco-2.3.0-linux-x86_64.tar.gz\n</code></pre> <p>In order for mujoco-py to find the MuJoCo binaries, set the following paths:</p> <pre><code>export MUJOCO_PY_MUJOCO_PATH=$HOME/path/to/mujoco230/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MUJOCO_PY_MUJOCO_PATH/bin\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#mujoco-on-engaging","title":"MuJoCo on Engaging","text":"","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#install-mujoco-py","title":"Install Mujoco-Py","text":"<p>First, make sure the <code>MUJOCO_PY_MUJOCO_PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables are set pointing to your mujoco installation. You can use the \u201cecho\u201d command to do this:</p> <pre><code>echo MUJOCO_PY_MUJOCO_PATH\necho LD_LIBRARY_PATH\n</code></pre> <p>If any of these are not set properly you can set them as described above (see here for MUJOCO_PY_MUJOCO_PATH, LD_LIBRARY_PATH.</p> <p>Next load either a Python or Anaconda module. In this example I loaded the latest anaconda3 module (run <code>module avail anaconda</code> to see the current list of available anaconda modules):</p> <pre><code>module load anaconda3/2022.10\n</code></pre> <p>From here on you can follow the standard instructions to install mujoco-py, using the <code>--user</code> flag where appropriate to install in your home directory, or install in an anaconda or virtual environment (do not use the <code>--user</code> flag if you want to install in a conda or virtual environment). Here I am installing in my home directory with <code>--user</code>:</p> <pre><code>pip install --user 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Start up python and import mujoco_py to complete the build process:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>If you\u2019d like you can run the few example lines listed on install section of the mujoco-py github page to verify the install went through properly:</p> <pre><code>import mujoco_py\nimport os\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#using-mujoco-in-a-job","title":"Using MuJoCo in a Job","text":"<p>To use MuJoCo you\u2019ll need to first load the same Python or Anaconda module you used to install mujoco-py. If you installed it into a conda environment or python virtual environment, load that environment as well. We recommend you do this in your job submission script rather than in your .bashrc or at the command line before you submit the job. This way you know your job is configured properly every time it runs. You can use the following test scripts to test your MuJoCo setup in a job environment, and as a starting point for your own job:</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> submit_test.sh<pre><code>#!/bin/bash\n\n# Load the same python/anaconda module you used to install mujoco-py\nmodule load python/3.8.3\n\n# Run the script\npython mujoco_test.py\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#mujoco-on-supercloud","title":"MuJoCo on SuperCloud","text":"<p>MuJoCo, particularly mujoco-py, can be tricky to install on SuperCloud as it uses file locking during the install and whenever the package is loaded. File locking is disabled on the SuperCloud shared filesystem performance reasons, but is available on the local disk of each node. Therefore, one workaround is to install mujoco-py on the local disk of one of the login nodes and then copy the install to your home directory. To load the package, the install then needs to be copied to the local disk.</p> <p>We\u2019ve found the most success by doing this with a python virtual environment. By using a python virtual environment you can install any additional packages you need with mujoco-py, and they can be used along with packages in our anaconda module, unlike conda environments.</p> <p>If you haven't already, first follow the instructions above to install the MuJoCo binaries.</p>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#create-the-virtual-environment","title":"Create the Virtual Environment","text":"<p>Next create the virtual environment on the local disk of the login node and install mujoco-py (install the version you would like to use):</p> <pre><code>module load anaconda/2023a\nmkdir /state/partition1/user/$USER\npython -m venv /state/partition1/user/$USER/mujoco_env\nsource /state/partition1/user/$USER/mujoco_env/bin/activate\npip install 'mujoco-py&lt;2.2,&gt;=2.1'\n</code></pre> <p>Now install any other packages you need to run your MuJoCo jobs. With virtual environments you won\u2019t see any of the packages you\u2019ve previously installed with <code>pip install --user</code> or what you may have installed in another environment. You should still be able to use any of the packages in the anaconda module you\u2019ve loaded, so no need to install any of those.</p> <pre><code>pip install pkgname1\npip install pkgname2\n</code></pre> <p>Since you are installing into virtual environment, do not use the <code>--user</code> flag.</p> <p>Once you\u2019ve installed the packages you need, start Python and import mujoco_py to finish the build:</p> <pre><code>python\nimport mujoco_py\n</code></pre> <p>Now that your environment is created, copy it to your home directory for permanent storage.</p> <pre><code>cp -r /state/partition1/user/$USER/mujoco_env $/software/mujoco/\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#running-a-job","title":"Running a Job","text":"<p>Now whenever you use mujoco-py the installation will need to be on the local disk of the node(s) where you are running. In your job script you can add a few lines of code that will check whether the environment exists on the local disk, and if not copy it. You can run these lines during an interactive job as well.</p> <pre><code># Set some useful environment variables\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\n# Check if the environment exists on the local disk. If not copy it over from the home directory.\nif [ ! -d \"$MUJOCO_ENV\" ]; then\n    echo \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\n    mkdir -p /state/partition1/user/$USER\n    cp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\n\n# Load an anaconda module, then activate your mujoco environment\nmodule load anaconda/2023a\nsource $MUJOCO_ENV/bin/activate\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/mujoco/#supercloud-test-scripts","title":"SuperCloud Test Scripts","text":"<p>The following are some test scripts you can use to check that your configuration worked.</p> mujoco_test.py<pre><code>import mujoco_py\nimport os\n\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\nsim.step()\nprint(sim.data.qpos)\n</code></pre> submit_test.sh<pre><code>#!/bin/bash\n\nexport MUJOCO_ENV_HOME=$HOME/software/mujoco/mujoco_env\nexport MUJOCO_ENV=/state/partition1/user/$USER/mujoco_env\n\nif [ ! -d \"$MUJOCO_ENV\" ]; then\n    echo \"Copying $MUJOCO_ENV_HOME to $MUJOCO_ENV\"\n    mkdir -p /state/partition1/user/$USER\n    cp -r $MUJOCO_ENV_HOME $MUJOCO_ENV\nfi\n\nmodule load anaconda/2022a\nsource $MUJOCO_ENV/bin/activate\n\npython mujoco_test.py\n</code></pre>","tags":["Engaging","SuperCloud","Howto Recipes","MuJoCo"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/","title":"Example of a minimal program using the nvhpc stack with CUDA aware MPI","text":"","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#about-nvhpc","title":"About NVHPC","text":"<p>NVHPC is an integrated collection of software tools and libraries distributed by NVidia. An overview document describing nvhpc  can be found here. The aim of the nvhpc team is to provide up to date, preconfigured suites of compilers, libraries and tools that are  specifically optimized for NVidia GPU hardware. It supports single and multi-GPU execution.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#basic-usage-example","title":"Basic Usage Example","text":"<p>This example shows steps for using NVHPC to run a simple test MPI program, written in C, that communicates between two GPUs. The detailed steps, that can be executed in an interactive Slurm session, are explained  below.  A complete Slurm job example is shown at the end.</p> <p>Prerequisites</p> <p>This example assumes you have access to a Slurm partition with GPU resources and are working with a Rocky Linux environment.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#1-activate-the-relevant-nvhpc-module","title":"1. Activate the relevant NVHPC module","text":"<p>The NVHPC environment is installed as a module and can be made visible in a session using the command</p> <pre><code>module load nvhpc/2023_233/nvhpc/23.3\n</code></pre> <p>this will add a specific version of the nvhpc software (version 23.3 released in 2023) to a shell or batch script. The software added includes compilers for C, C++ and Fortran; base GPU optimized numerical libraries for linear algebra, Fourier transforms and others; GPU optimized communication libraries supporting MPI, SHMEM and NCCL APIs.</p> <p>An environment variable, <code>NVHPC_ROOT</code>, is also set. This can be used in scripts to reference the locations of libraries when needed.</p>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#2-set-paths-needed-for-compile-step","title":"2. Set paths needed for compile step","text":"<p>Here we use the module environment variable, <code>NVHPC_ROOT</code>, to set environment variables that have paths needed for compilation and linking of code.</p> <pre><code>culibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#3-create-a-c-program-for-that-excutes-some-simple-multi-node-multi-gpu-test-code","title":"3. Create a C program for that excutes some simple multi-node, multi-GPU test code.","text":"<p>The next step is to create a file holding C code that uses MPI to send information between two GPUs  running in different processes. Paste the C code below into a file called <code>test.c</code>.</p> test.c<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;cuda_runtime.h&gt;\nint main(int argc, char *argv[])\n{\n  int myrank, mpi_nranks; \n  int LBUF=1000000;\n  float *sBuf_h, *rBuf_h;\n  float *sBuf_d, *rBuf_d;\n  int bSize;\n  int i;\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);                  // my MPI rank\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;mpi_nranks);\n\n  if ( mpi_nranks != 2 ) { printf(\"Program requires exactly 2 ranks\\n\");exit(-1); }\n\n  int deviceCount;\n  cudaGetDeviceCount(&amp;deviceCount);               // How many GPUs?\n  printf(\"Number of GPUs found = %d\\n\",deviceCount);\n  int device_id = myrank % deviceCount;\n  cudaSetDevice(device_id);                       // Map MPI-process to a GPU\n  printf(\"Assigned GPU %d to MPI rank %d of %d.\\n\",device_id, myrank, mpi_nranks);\n\n  // Allocate buffers on each host and device\n  bSize = sizeof(float)*LBUF;\n  sBuf_h = malloc(bSize);\n  rBuf_h = malloc(bSize);\n  for (i=0;i&lt;LBUF;++i){\n    sBuf_h[i]=(float)myrank;\n    rBuf_h[i]=-1.;\n  }\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  cudaMalloc((void **)&amp;sBuf_d,bSize);\n  cudaMalloc((void **)&amp;rBuf_d,bSize);\n\n  cudaMemcpy(sBuf_d,sBuf_h,bSize,cudaMemcpyHostToDevice);\n\n  if ( myrank == 0 ) {\n   MPI_Recv(rBuf_d,LBUF,MPI_REAL,1,0,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n  } \n  else if ( myrank == 1 ) {\n   MPI_Send(sBuf_d,LBUF,MPI_REAL,0,0,MPI_COMM_WORLD);\n  }\n  else\n  {\n   printf(\"Unexpected myrank value %d\\n\",myrank);\n   exit(-1);\n  }\n\n  cudaMemcpy(rBuf_h,rBuf_d,bSize,cudaMemcpyDeviceToHost);\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#4-compile-program","title":"4. Compile program","text":"<p>Here we use nvhpc MPI wrapper to compile. The two environment variables we set earlier (<code>cuincdir</code> and <code>culibdir</code>) are used to let the compile step know where to find the relevant CUDA header and library files. The CUDA runtime library (<code>cudart</code>) is added as a location for finding CUDA functions the code utilizes.</p> <pre><code>mpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#5-execute-program","title":"5. Execute program","text":"<p>Once code has been compiled the <code>mpiexec</code> command that is part of the <code>nvhpc</code> module can be used to run the test program. The <code>nvhpc</code> module defaults to using its builtin version of OpneMPI. The OpenMPI option <code>btl_openib_warn_no_device_params_found</code> is passed into the OpenMPI runtime library. This option supresses a warning that OpenMPI can generate when it encounters a network device card that is not present in a built-in list that OpenMPI has historically included.</p> <pre><code>mpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre> <p>Running this program using the command above should produce the following output.</p> <pre><code>Number of GPUs found = 1\nNumber of GPUs found = 1\nAssigned GPU 0 to MPI rank 0 of 2.\nrBuf_h[0] = -1.000000\nAssigned GPU 0 to MPI rank 1 of 2.\nrBuf_h[0] = 1.000000\n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/nvhpc-a100-with-cuda-and-mpi-example/#example-of-slurm-job-file-for-excuting-this-example","title":"Example of Slurm job file for excuting this example","text":"<p>The job script file below will run all the steps described above. It can  be submitted to Slurm using the command <code>sbatch</code> followed by the filename holding the job script.</p> <pre><code>#!/bin/bash\n#SBATCH -p sched_system_all\n#SBATCH --constraint=rocky8\n#SBATCH -N 2\n#SBATCH -n 2\n#SBATCH --gres=gpu:2\n#SBATCH --time=00:02:00\n#\n# Basic slurm job that tests GPU aware MPI in the NVHPC tool stack.\n#\n#\n#   To submit through Slurm use:\n#\n#   $ sbatch test_cuda_and_mpi.sbatch\n#  \n#   in terminal.\n\n# Write a little log info\necho \"## Start time \\\"\"`date`\"\\\"\"\necho \"## Slurm job running on nodes \\\"${SLURM_JOB_NODELIST}\\\"\"\necho \"## Slurm submit directory \\\"${SLURM_SUBMIT_DIR}\\\"\"\necho \"## Slurm submit host \\\"${SLURM_SUBMIT_HOST}\\\"\"\necho \" \"\n\n\nmodule load nvhpc/2023_233/nvhpc/23.3\nculibdir=$NVHPC_ROOT/cuda/lib64\ncuincdir=$NVHPC_ROOT/cuda/include\n\ncat &gt; test.c &lt;&lt;'EOFA'\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;cuda_runtime.h&gt;\nint main(int argc, char *argv[])\n{\n  int myrank, mpi_nranks; \n  int LBUF=1000000;\n  float *sBuf_h, *rBuf_h;\n  float *sBuf_d, *rBuf_d;\n  int bSize;\n  int i;\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);                  // my MPI rank\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;mpi_nranks);\n\n  if ( mpi_nranks != 2 ) { printf(\"Program requires exactly 2 ranks\\n\");exit(-1); }\n\n  int deviceCount;\n  cudaGetDeviceCount(&amp;deviceCount);               // How many GPUs?\n  printf(\"Number of GPUs found = %d\\n\",deviceCount);\n  int device_id = myrank % deviceCount;\n  cudaSetDevice(device_id);                       // Map MPI-process to a GPU\n  printf(\"Assigned GPU %d to MPI rank %d of %d.\\n\",device_id, myrank, mpi_nranks);\n\n  // Allocate buffers on each host and device\n  bSize = sizeof(float)*LBUF;\n  sBuf_h = malloc(bSize);\n  rBuf_h = malloc(bSize);\n  for (i=0;i&lt;LBUF;++i){\n    sBuf_h[i]=(float)myrank;\n    rBuf_h[i]=-1.;\n  }\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  cudaMalloc((void **)&amp;sBuf_d,bSize);\n  cudaMalloc((void **)&amp;rBuf_d,bSize);\n\n  cudaMemcpy(sBuf_d,sBuf_h,bSize,cudaMemcpyHostToDevice);\n\n  if ( myrank == 0 ) {\n   MPI_Recv(rBuf_d,LBUF,MPI_REAL,1,0,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n  } \n  else if ( myrank == 1 ) {\n   MPI_Send(sBuf_d,LBUF,MPI_REAL,0,0,MPI_COMM_WORLD);\n  }\n  else\n  {\n   printf(\"Unexpected myrank value %d\\n\",myrank);\n   exit(-1);\n  }\n\n  cudaMemcpy(rBuf_h,rBuf_d,bSize,cudaMemcpyDeviceToHost);\n  if ( myrank == 0 ) {\n   printf(\"rBuf_h[0] = %f\\n\",rBuf_h[0]);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}\nEOFA\n\nmpicc test.c -I${cuincdir} -L${culibdir} -lcudart\n\nmpiexec --mca btl_openib_warn_no_device_params_found 0 -n 2 ./a.out \n</code></pre>","tags":["Engaging","Satori","Howto Recipes","nvhpc","MPI","cuda","cuda aware mpi","GPU","Rocky Linux"]},{"location":"recipes/relion/","title":"Installing and Using RELION","text":"<p>RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a software package that employs an empirical Bayesian approach for electron cryo-microscopy (cryo-EM) structure determination. </p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/relion/#relion-on-satori","title":"RELION on Satori","text":"<p>This recipe is for building and using RELION on x86 nodes on Satori. It is different from working on IBM power9 nodes on Satori. </p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/relion/#install","title":"Install","text":"<p>Go to your directory and download RELION, <pre><code>cd /nobackup/users/$USER\ngit clone https://github.com/3dem/relion.git\n</code></pre></p> <p>Get an interactive session on x86 nodes of Satori, <pre><code>srun -p sched_mit_mbathe -c 2 -t 60 --pty bash\n</code></pre></p> <p>Note: The x86 nodes are available to some labs only. </p> <p>Load necessary modules <pre><code>module use /software/spack/share/spack/lmod/linux-rocky8-x86_64/Core\nmodule load gcc/12.2.0-x86_64  \nmodule load openmpi/4.1.4-pmi-cuda-ucx-x86_64 \nmodule load fftw/3.3.10-x86_64\n</code></pre></p> <p>Note: These modules are installed for the x86 nodes only. </p> <p>Build RELION <pre><code>cd relion\ngit checkout master \ncd ..\nmkdir 4.0.1\ncd 4.0.1\nmkdir install\nmkdir build\ncd build\n\ncmake -DCMAKE_INSTALL_PREFIX=/home/$USER/relion/4.0.1/install ../../relion\nmake\nmake install\n</code></pre></p> <p>It is all set for the installation.</p>","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"recipes/relion/#run-relion","title":"Run RELION","text":"","tags":["Satori","MPI","RELION","Howto Recipes"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#engaging","title":"Engaging","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gpu","title":"GPU","text":"<ul> <li>GROMACS</li> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#gromacs","title":"GROMACS","text":"<ul> <li>GROMACS</li> </ul>"},{"location":"tags/#getting-help","title":"Getting Help","text":"<ul> <li>Getting Help</li> </ul>"},{"location":"tags/#getting-started","title":"Getting Started","text":"<ul> <li>Getting Started Tutorial</li> </ul>"},{"location":"tags/#h100","title":"H100","text":"<ul> <li>Getting started on 8-way H100 nodes on Satori</li> </ul>"},{"location":"tags/#howto-recipes","title":"Howto Recipes","text":"<ul> <li>GROMACS</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>MPI for Python</li> <li>MuJoCo</li> <li>NVHPC with CUDA aware MPI</li> <li>Installing and Using RELION</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>Getting Started Tutorial</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>GROMACS</li> <li>MPI for Python</li> <li>NVHPC with CUDA aware MPI</li> <li>Installing and Using RELION</li> </ul>"},{"location":"tags/#mujoco","title":"MuJoCo","text":"<ul> <li>MuJoCo</li> </ul>"},{"location":"tags/#openmind","title":"OpenMind","text":"<ul> <li>MPI for Python</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>MPI for Python</li> </ul>"},{"location":"tags/#relion","title":"RELION","text":"<ul> <li>Installing and Using RELION</li> </ul>"},{"location":"tags/#rocky-linux","title":"Rocky Linux","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#satori","title":"Satori","text":"<ul> <li>ORCD Systems</li> <li>Getting started on 8-way H100 nodes on Satori</li> <li>NVHPC with CUDA aware MPI</li> <li>Installing and Using RELION</li> </ul>"},{"location":"tags/#supercloud","title":"SuperCloud","text":"<ul> <li>ORCD Systems</li> <li>GROMACS</li> <li>MuJoCo</li> </ul>"},{"location":"tags/#cuda","title":"cuda","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#cuda-aware-mpi","title":"cuda aware mpi","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"},{"location":"tags/#nvhpc","title":"nvhpc","text":"<ul> <li>NVHPC with CUDA aware MPI</li> </ul>"}]}