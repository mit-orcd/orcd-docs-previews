
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://orcd.mit.edu/recipes/torch-gpu-intermediate/">
      
      
        <link rel="prev" href="../torch-gpu/">
      
      
        <link rel="next" href="../rag/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>PyTorch on GPUs - II - ORCD Docs</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#intermediate-distributed-deep-learning-with-pytorch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ORCD Docs" class="md-header__button md-logo" aria-label="ORCD Docs" data-md-component="logo">
      
  <img src="../../images/ORCD_logo_icononly.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ORCD Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PyTorch on GPUs - II
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ORCD Docs" class="md-nav__button md-logo" aria-label="ORCD Docs" data-md-component="logo">
      
  <img src="../../images/ORCD_logo_icononly.png" alt="logo">

    </a>
    ORCD Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../orcd-systems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ORCD Systems
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Started Tutorial
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Accessing ORCD Systems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Accessing ORCD Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../accessing-orcd/ondemand-login/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logging in with OnDemand
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../accessing-orcd/ssh-login/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logging in with SSH via Terminal
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../accessing-orcd/ssh-setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SSH Key Setup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vscode/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VSCode Remote SSH
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Filesystems and File Transfer
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Filesystems and File Transfer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../filesystems-file-transfer/filesystems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    General Use Filesystems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../filesystems-file-transfer/project-filesystems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Project Specific Filesystems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../filesystems-file-transfer/transferring-files/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transferring Files
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Software
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Software
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/compile/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Compiling Codes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/apptainer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Containers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/julia/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Julia
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/modules/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Modules
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/R/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    R
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/licensed/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Licensed Software
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/building-software/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Other Software
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Running Jobs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Running Jobs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/requesting-resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Requesting Resources
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/job-arrays/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Job Arrays
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/available-resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Available Resources
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../running-jobs/application-analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Application Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" checked>
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ORCD Recipes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            ORCD Recipes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../af3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AlphaFold 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gromacs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GROMACS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../intel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Intel compiler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jupyter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jupyter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpi4py/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MPI for Python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MPI Jobs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nvhpc-a100-with-cuda-and-mpi-example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    NVHPC with CUDA aware MPI
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cuda-q/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CUDA-Q
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../orca/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ORCA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pycharm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PyCharm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../torch-gpu/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PyTorch on GPUs - I
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    PyTorch on GPUs - II
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    PyTorch on GPUs - II
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installing-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Installing PyTorch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fully-sharded-data-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Fully Sharded Data Parallel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Fully Sharded Data Parallel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#an-example-with-a-single-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      An example with a single GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#single-node-multi-gpu-fsdp" class="md-nav__link">
    <span class="md-ellipsis">
      Single-node multi-GPU FSDP
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hybrid-fully-sharded-data-parallel-and-tensor-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid Fully Sharded Data Parallel and Tensor Parallel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Hybrid Fully Sharded Data Parallel and Tensor Parallel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#single-node-multi-gpu-fsdp-tp" class="md-nav__link">
    <span class="md-ellipsis">
      Single-node multi-GPU FSDP + TP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-node-multi-gpu-fsdp-tp" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-node multi-GPU FSDP + TP
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../build-vasp-gcc-cpu/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VASP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vscode/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VSCode Remote SSH
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../X11/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    X11
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../acknowledgements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Acknowledging Us
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data-security/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Security and Privacy
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code-of-conduct/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Acceptable Use and Code of Conduct
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-help/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Help
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faqs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAQs
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../glossary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Glossary
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tags/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Index
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installing-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Installing PyTorch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fully-sharded-data-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Fully Sharded Data Parallel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Fully Sharded Data Parallel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#an-example-with-a-single-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      An example with a single GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#single-node-multi-gpu-fsdp" class="md-nav__link">
    <span class="md-ellipsis">
      Single-node multi-GPU FSDP
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hybrid-fully-sharded-data-parallel-and-tensor-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid Fully Sharded Data Parallel and Tensor Parallel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Hybrid Fully Sharded Data Parallel and Tensor Parallel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#single-node-multi-gpu-fsdp-tp" class="md-nav__link">
    <span class="md-ellipsis">
      Single-node multi-GPU FSDP + TP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-node-multi-gpu-fsdp-tp" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-node multi-GPU FSDP + TP
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <nav class="md-tags" >
    
      
      
      
        <a href="../../tags/#tag:engaging" class="md-tag">Engaging</a>
      
    
      
      
      
        <a href="../../tags/#tag:gpu" class="md-tag">GPU</a>
      
    
      
      
      
        <a href="../../tags/#tag:howto-recipes" class="md-tag">Howto Recipes</a>
      
    
      
      
      
        <a href="../../tags/#tag:pytorch" class="md-tag">PyTorch</a>
      
    
  </nav>



<h1 id="intermediate-distributed-deep-learning-with-pytorch">Intermediate Distributed Deep Learning with PyTorch<a class="headerlink" href="#intermediate-distributed-deep-learning-with-pytorch" title="Permanent link">&para;</a></h1>
<p>Deep learning is the foundation of artificial intelligence nowadays. Deep learning programs can be accelerated substantially on GPUs.  </p>
<p>There are various parallelisms to enable distributed deep learning on multiple GPUs, including data parallel and model parallel. </p>
<p>We have introduced <a href="../torch-gpu/">basic recipes of data parallel with PyTorch</a>, which is a popular Python package for working on deep learning projects.</p>
<p>In data parallel, the model has to fit into the GPU memory. However, large model sizes are required for large language models (LLMs) based on the transformer architecture. When the model does not fit into the memory of a single GPU, the normal data parallelism does not work. </p>
<p>On this page, we will introduce intermediate recipes to train large models on multiple GPUs with PyTorch. </p>
<p>First, there is a <a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">Fully Sharded Data Parallel (FSDP)</a> approach to split the model into multiple GPUs so that the memory requirement fits. A shard of the model is stored in each GPU, and communication between GPUs happens during the training process. We will introduce FSDP recipes in the first section. </p>
<p>However, FSDP does not gain additional speedup beyond the data parallel framework. Better approaches are based on model parallel, which not only splits the model into multiple GPUs but also accelerates the training process with parallel sharded computations. There are various schemes of model parallel, such as pipeline parallel (PP) and tensor parallel (TP). Usually, model parallel is applied on top of data parallel to gain further speedup. In the second section, we will focus on recipes of hybrid Fully Sharded Data Parallel and Tensor Parallel (referred to as FSDP + TP) . </p>
<h2 id="installing-pytorch">Installing PyTorch<a class="headerlink" href="#installing-pytorch" title="Permanent link">&para;</a></h2>
<div class="tabbed-set tabbed-alternate" data-tabs="1:1"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="__tabbed_1_1">Engaging</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>First, load a Miniforge module to provide a Python platform with PyTorch and CUDA support preinstalled, 
 <div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>module load miniforge/24.3.0-0
</span></code></pre></div></p>
</div>
</div>
</div>
<h2 id="fully-sharded-data-parallel">Fully Sharded Data Parallel<a class="headerlink" href="#fully-sharded-data-parallel" title="Permanent link">&para;</a></h2>
<p>We use <a href="https://github.com/pytorch/examples/tree/main/mnist">an example code</a> to train a convolutional neural network (CNN) with the MNIST data set.</p>
<p>We will first run the example on a single GPU and then extend it to <a href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">multiple GPUs with FSDP</a>.</p>
<p>Download the codes <a href="../scripts/torch-gpu-2/mnist_gpu.py">mnist_gpu.py</a> and <a href="../scripts/torch-gpu-2/FSDP_mnist.py">FSDP_mnist.py</a> for these two cases respectively. </p>
<h3 id="an-example-with-a-single-gpu">An example with a single GPU<a class="headerlink" href="#an-example-with-a-single-gpu" title="Permanent link">&para;</a></h3>
<div class="tabbed-set tabbed-alternate" data-tabs="2:1"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="__tabbed_2_1">Engaging</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>To run the example on a single GPU, prepare a job script named <code>job.sh</code> like this,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>#!/bin/bash
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>#SBATCH -p mit_normal_gpu
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>#SBATCH --job-name=mnist-gpu
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>#SBATCH -N 1
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>#SBATCH -n 1
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>#SBATCH --mem=20GB
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>#SBATCH --gres=gpu:h200:1  
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>module load miniforge/24.3.0-0
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>python mnist_gpu.py
</span></code></pre></div></p>
<p>Here we sepecify the GPU type of H200 with <code>--gres=gpu:h200:1</code>. If one is not particular about the type of GPU, <code>--gres=gpu:1</code> can be used instead. </p>
<p>Submit the job script,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>sbatch job.sh
</span></code></pre></div></p>
</div>
</div>
</div>
<p>While the job is running, you can check if the program runs on a GPU. First, check the hostname that it runs on,
<div class="language-text highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>squeue --me
</span></code></pre></div>
and then log in to the node,
<div class="language-text highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>ssh &lt;nodeXXX&gt;
</span></code></pre></div>
and check the GPU usage with the <a href="https://orcd-docs.mit.edu/running-jobs/application-analysis/#nvtop"><code>nvtop</code></a> command.</p>
<h3 id="single-node-multi-gpu-fsdp">Single-node multi-GPU FSDP<a class="headerlink" href="#single-node-multi-gpu-fsdp" title="Permanent link">&para;</a></h3>
<p>Now we extend this example to multiple GPUs on a single node with FSDP. </p>
<div class="tabbed-set tabbed-alternate" data-tabs="3:1"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio" /><div class="tabbed-labels"><label for="__tabbed_3_1">Engaging</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>Prepare a job script named <code>job.sh</code> like this,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>#!/bin/bash
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>#SBATCH -p mit_normal_gpu 
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>#SBATCH --job-name=fsdp
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>#SBATCH -N 1
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>#SBATCH -n 2
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>#SBATCH --mem=20GB
</span><span id="__span-5-7"><a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a>#SBATCH --gres=gpu:h200:2
</span><span id="__span-5-8"><a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>
</span><span id="__span-5-9"><a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>module load miniforge/24.3.0-0
</span><span id="__span-5-10"><a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a>
</span><span id="__span-5-11"><a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a>python FSDP_mnist.py
</span></code></pre></div>
 then submit it,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>sbatch job.sh
</span></code></pre></div></p>
</div>
</div>
</div>
<p>As set up in the program <code>FSDP_mnist.py</code>, it will run on all GPUs requested in Slurm, that is 2 in this case. That says the model is split into 2 shards, each stored on a GPU, and the training process happens on 2 batches of data simultaneously. Communication between GPUs happens under the hood. </p>
<h2 id="hybrid-fully-sharded-data-parallel-and-tensor-parallel">Hybrid Fully Sharded Data Parallel and Tensor Parallel<a class="headerlink" href="#hybrid-fully-sharded-data-parallel-and-tensor-parallel" title="Permanent link">&para;</a></h2>
<p>Tensor parallel can be applied on top of data parallel to gain further speedup. In this section, we introduce recipes of hybrid FSDP and TP.</p>
<p>We use an example that implements FSDP + TP on LLAMA2 (Large Language Model Meta AI 2). Refer to <a href="https://pytorch.org/tutorials/intermediate/TP_tutorial.html">the description of this example</a>. Download the codes: <a href="../scripts/torch-gpu-2/fsdp_tp_example.py">fsdp_tp_example.py</a>, <a href="../scripts/torch-gpu-2/llama2_model.py">llama2_model.py</a>, and <a href="../scripts/torch-gpu-2/log_utils.py">log_utils.py</a>.</p>
<h3 id="single-node-multi-gpu-fsdp-tp">Single-node multi-GPU FSDP + TP<a class="headerlink" href="#single-node-multi-gpu-fsdp-tp" title="Permanent link">&para;</a></h3>
<p>First, let's run the example on multiple GPUs within a single node. </p>
<p>The code <code>fsdp_tp_example.py</code> is set up for this purpose. The TP size is set to be 2 in the code. The total number of GPUs should be equal to a multiple of the TP size, then the FSDP size is equal to the number of GPUs divided by the TP size.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="4:1"><input checked="checked" id="__tabbed_4_1" name="__tabbed_4" type="radio" /><div class="tabbed-labels"><label for="__tabbed_4_1">Engaging</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>To run this example on multiple GPUs, prepare a job script like this,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>#!/bin/bash                                                                                          
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>#SBATCH -p mit_preemptable                                                                           
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>#SBATCH -t 60                                                                                        
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a>#SBATCH -N 1                                                                                         
</span><span id="__span-7-5"><a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a>#SBATCH -n 4                                                                                         
</span><span id="__span-7-6"><a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>#SBATCH --mem=30GB                                                                                   
</span><span id="__span-7-7"><a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a>#SBATCH --gres=gpu:h200:4                                                                            
</span><span id="__span-7-8"><a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a>
</span><span id="__span-7-9"><a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a>module load miniforge/24.3.0-0
</span><span id="__span-7-10"><a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a>
</span><span id="__span-7-11"><a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a>torchrun --nnodes=1 --nproc_per_node=4 \
</span><span id="__span-7-12"><a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a>     --rdzv_id=$SLURM_JOB_ID \
</span><span id="__span-7-13"><a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a>     --rdzv_endpoint=&quot;localhost:1234&quot; \
</span><span id="__span-7-14"><a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a>     fsdp_tp_example.py
</span></code></pre></div>
 then submit it,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>sbatch job.sh
</span></code></pre></div></p>
</div>
</div>
</div>
<p>With the flags <code>--nnodes=1 --nproc-per-node=4</code>, the <code>torchrun</code> command will run the program on 4 GPUs within one node. The training process happens on 2 batches of data with FSDP, and the model is trained with TP sharded computation on 2 GPUs for each batch of data.</p>
<p>The flags with <code>rdzv</code> (meaning the Rendezvous protocol) are required by <code>torchrun</code> to coordinate multiple processes. The flag <code>--rdzv-id=$SLURM_JOB_ID</code> sets to the <code>rdzv</code> ID to be the job ID, but it can be any random number. The flag <code>--rdzv-endpoint=localhost:1234</code> is to set the host and the port. Use <code>localhost</code> when there is only one node. The port can be any 4- or 5-digit number larger than 1024. </p>
<h3 id="multi-node-multi-gpu-fsdp-tp">Multi-node multi-GPU FSDP + TP<a class="headerlink" href="#multi-node-multi-gpu-fsdp-tp" title="Permanent link">&para;</a></h3>
<p>Finally, we run this example on multiple GPUs across multiple nodes. Note that this example requires 8 GPUs which is more than the standard user has access to on Engaging. If you have access to more GPUs, run this example on the corresponding partition.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="5:1"><input checked="checked" id="__tabbed_5_1" name="__tabbed_5" type="radio" /><div class="tabbed-labels"><label for="__tabbed_5_1">Engaging</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>Prepare a job script like this,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>#!/bin/bash
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>#SBATCH -p  mit_normal_gpu
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>#SBATCH -N 2
</span><span id="__span-9-4"><a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a>#SBATCH --ntasks-per-node=1
</span><span id="__span-9-5"><a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>#SBATCH --cpus-per-task=4
</span><span id="__span-9-6"><a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a>#SBATCH --gpus-per-node=h200:4 
</span><span id="__span-9-7"><a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>#SBATCH --mem=30GB
</span><span id="__span-9-8"><a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a>
</span><span id="__span-9-9"><a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a>module load miniforge/24.3.0-0
</span><span id="__span-9-10"><a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a>
</span><span id="__span-9-11"><a id="__codelineno-9-11" name="__codelineno-9-11" href="#__codelineno-9-11"></a># Get IP address of the master node
</span><span id="__span-9-12"><a id="__codelineno-9-12" name="__codelineno-9-12" href="#__codelineno-9-12"></a>nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
</span><span id="__span-9-13"><a id="__codelineno-9-13" name="__codelineno-9-13" href="#__codelineno-9-13"></a>nodes_array=($nodes)
</span><span id="__span-9-14"><a id="__codelineno-9-14" name="__codelineno-9-14" href="#__codelineno-9-14"></a>master_node=${nodes_array[0]}
</span><span id="__span-9-15"><a id="__codelineno-9-15" name="__codelineno-9-15" href="#__codelineno-9-15"></a>master_node_ip=$(srun --nodes=1 --ntasks=1 -w &quot;$master_node&quot; hostname --ip-address)
</span><span id="__span-9-16"><a id="__codelineno-9-16" name="__codelineno-9-16" href="#__codelineno-9-16"></a>
</span><span id="__span-9-17"><a id="__codelineno-9-17" name="__codelineno-9-17" href="#__codelineno-9-17"></a>srun torchrun --nnodes=$SLURM_NNODES \
</span><span id="__span-9-18"><a id="__codelineno-9-18" name="__codelineno-9-18" href="#__codelineno-9-18"></a>          --nproc-per-node=$SLURM_CPUS_PER_TASK \
</span><span id="__span-9-19"><a id="__codelineno-9-19" name="__codelineno-9-19" href="#__codelineno-9-19"></a>          --rdzv-id=$SLURM_JOB_ID   \
</span><span id="__span-9-20"><a id="__codelineno-9-20" name="__codelineno-9-20" href="#__codelineno-9-20"></a>          --rdzv-backend=c10d \
</span><span id="__span-9-21"><a id="__codelineno-9-21" name="__codelineno-9-21" href="#__codelineno-9-21"></a>          --rdzv-endpoint=$master_node_ip:1234 \
</span><span id="__span-9-22"><a id="__codelineno-9-22" name="__codelineno-9-22" href="#__codelineno-9-22"></a>          fsdp_tp_example.py
</span></code></pre></div>
 then submit it,
 <div class="language-text highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>sbatch job.sh
</span></code></pre></div></p>
</div>
</div>
</div>
<p>The configuration of the <code>#SBATCH</code> and <code>torchrun</code> flags is similar to that in <a href="../torch-gpu/">the basic recipe of data parallel</a>. </p>
<p>The program runs on 8 GPUs with 4 per node. As is set up in the code <code>fsdp_tp_example.py</code>, the training process happens on 4 batches of data with FSDP,  and the model is trained with TP sharded computation on 2 GPUs for each batch of data.</p>
<details>
<summary>Topology of GPU Communication</summary>
<p>The NVIDIA Collective Communications Library (NCCL) is set as the backend in all of the PyTorch programs here, so that the communication between GPUs within one node benefits from the high bandwidth of NVLinks, and the communication between GPUs across nodes benefits from the bandwidth of the Infiniband network. </p>
<p>The intra-node GPU-GPU communication speed is much faster than the inter-node. The communication overhead of TP is much larger than that of FSDP. The topology of GPU communication is set up (in the code <code>fsdp_tp_example.py</code>) in a way that TP communication is intra-node and FSDP communication is inter-node, so that the usage of network bandwidth is optimized. </p>
</details>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="August 10, 2025 20:49:34 UTC">August 10, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      <a href="https://accessibility.mit.edu/">Accessibility</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../js/open_in_new_tab.js"></script>
      
    
  </body>
</html>